{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# auto reload libraries (you do need to re-import libraries if you make changes)\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# base \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "\n",
    "# preprocessing \n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.experimental import enable_iterative_imputer \n",
    "from sklearn.impute import SimpleImputer, IterativeImputer,KNNImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, RobustScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "\n",
    "# models\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# own defined functions/classes \n",
    "from preprocessing.preprocess_pipe import MakeLowerCase\n",
    "from preprocessing.preprocess_pipe import HdResolutionCategorizer\n",
    "from preprocessing.preprocess_pipe import StorageCategorizer\n",
    "from preprocessing.preprocess_pipe import SsdCategorizer\n",
    "\n",
    "from preprocessing.preprocess_pipe import print_missing\n",
    "from preprocessing.preprocess_pipe import calculate_perf\n",
    "from preprocessing.preprocess_pipe import custom_scoring_func\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global parameters\n",
    "pd.set_option('display.max_columns', 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO\n",
    "- Add custum scoring metric: **DONE**\n",
    "- further integrate preprocessing steps from Arnaud **DONE**\n",
    "- Integerate steps from Victor **Partly DONE**\n",
    "- Integrate steps from Bram \n",
    "- Integrate custum scoring metric (also when performing the cross validation/grid search) **DONE**\n",
    "- Make function that checkts whether prediction of the maximum price are >= predictions of the minimum price\n",
    "- Investigate why there is such a huge difference between the performance on 'our test set' and the performance of submission test set\n",
    "- Implement better missing values imputations methods\n",
    "- Add some post processing visualizations:\n",
    "    - feature importance plots\n",
    "    - look at our predictions visually (do the make since?)\n",
    "    - look at the residuals\n",
    "    - Have a look at this especially section 5 for model interpretability ideas\n",
    "    https://christophm.github.io/interpretable-ml-book/pdp.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: import data and split in train and validation set\n",
    "The validation set is more our own kind of test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions of all training data (510, 22)\n",
      "Dimension test data (222, 20)\n"
     ]
    }
   ],
   "source": [
    "# read in trainig and validation data\n",
    "# use the same data split as we did in R\n",
    "df_all_train = pd.read_csv(\"../data/train.csv\", sep=',')\n",
    "df_test = pd.read_csv(\"../data/test.csv\", sep=',')\n",
    "\n",
    "print(f'Dimensions of all training data {df_all_train.shape}')\n",
    "print(f'Dimension test data {df_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split in training and validation set\n",
    "df_train = df_all_train.sample(frac=0.75, random_state=0, replace=False)\n",
    "df_val = df_all_train.drop(df_train.index)\n",
    "\n",
    "# reset index, if you don't resit missing rows get inserted in the pipeline\n",
    "# see: https://stackoverflow.com/questions/31323499/sklearn-error-valueerror-input-contains-nan-infinity-or-a-value-too-large-for\n",
    "df_train = df_train.reset_index().drop('index',axis=1)\n",
    "df_val = df_val.reset_index().drop('index',axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: define pipeline\n",
    "\n",
    "The pipeline should handle all steps performed on the data from data cleaning till the acutal model prediction. \n",
    "It should also be fairly easily to apply this to the test data. Therefore we make use of the scikit learn pipeline which allows all of this functionality. You can also implement your own `custom` transformations.\n",
    "\n",
    "For more information on how to use and why using pipelines see:\n",
    " - https://medium.com/vickdata/a-simple-guide-to-scikit-learn-pipelines-4ac0d974bdcf \n",
    " - scikit learn api on pipelines: https://scikit-learn.org/stable/modules/compose.html\n",
    "\n",
    "We make a difference between the pre processing steps for\n",
    "- numerical features\n",
    "- categorical features\n",
    "\n",
    "It's important to note that if a feature is in the numerical feature pipeline it can't be in the categorical feature pipeline. Also if some preprocessing steps in the numerical feature pipeline need features from the categorical pipeline you will get an error. All preprocessing steps in each pipeline should be able to perform the transformation only with the categorical or numerical features. \n",
    "\n",
    "You can implement some custom transformation steps like 'MakeLowerCase' ==> see preprocessing.preprocess_pipe for more details or check this article  these two article (also gives some explanation on how to implement pipelines)\n",
    "\n",
    "- https://gist.github.com/amberjrivera/8c5c145516f5a2e894681e16a8095b5c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Numerical features to pass down the numerical pipeline \n",
    "numerical_features = ['screen_size' ,'pixels_x','pixels_y',\n",
    "                      'ram', 'weight','ssd','storage']\n",
    "\n",
    "#Categrical features to pass down the categorical pipeline \n",
    "categorical_features = ['brand','base_name', 'screen_surface','touchscreen',\n",
    "                        'cpu','cpu_details','detachable_keyboard',\n",
    "                        'discrete_gpu','gpu', 'os','os_details']\n",
    "\n",
    "# define all unique features\n",
    "features = np.unique(numerical_features).tolist() + np.unique(categorical_features).tolist()\n",
    "\n",
    "# target variables\n",
    "target = ['min_price','max_price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "X_train = df_train[features]\n",
    "y_train = df_train[target]\n",
    "\n",
    "# validation (this is kind our own test set)\n",
    "X_val  = df_val[features]\n",
    "y_val = df_val[target]\n",
    "\n",
    "# train_validation (this is all training data we have) for fitting the model\n",
    "X_all_train = df_all_train[features]\n",
    "y_all_train = df_all_train[target]\n",
    "\n",
    "# test\n",
    "X_test = df_test[features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can add many more and \n",
    "# you can even define custom preprocessing steps like 'MakeLowerCase()'\n",
    "\n",
    "# pipeline  uses only numerical features,\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    #('imputer', SimpleImputer(missing_values=np.nan, strategy='mean')),\n",
    "    #('imputer', KNNImputer(n_neighbors=5, weights='uniform',metric='nan_euclidean')),\n",
    "    ('imputer', IterativeImputer(missing_values=np.nan, random_state=0, n_nearest_features=None, sample_posterior=True)),\n",
    "    #('transformation', PowerTransformer(method='yeo-johnson',standardize=False)),\n",
    "     #Scale features using statistics that are robust to outliers.\n",
    "    ('scaler', RobustScaler(with_centering=True, with_scaling=True, quantile_range=(10.0, 90.0)))]) \n",
    "\n",
    "# pipeline use only categorical features\n",
    "categorical_transformer = Pipeline(steps=[ \n",
    "    ('lowercase', MakeLowerCase()), # lower cases all columns containing strings\n",
    "    #('sd_category' ,SsdCategorizer(drop_original_feature=True)),\n",
    "    #('storage_category', StorageCategorizer(drop_original_feature=True)),\n",
    "    ('imputer', SimpleImputer(missing_values=np.nan, strategy=\"most_frequent\")),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "# add both preprocessing pipelines in one pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numerical_features),\n",
    "        ('cat', categorical_transformer, categorical_features)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(382, 1)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is how the preporcessed pipeline looks like (just to have an idea)\n",
    "pd.DataFrame(preprocessor.fit_transform(X_train)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: add models to pipeline\n",
    "## 1) Random Forest\n",
    "\n",
    "### A) Training and parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters currently in use:\n",
      "\n",
      "{'bootstrap': True,\n",
      " 'ccp_alpha': 0.0,\n",
      " 'criterion': 'mae',\n",
      " 'max_depth': None,\n",
      " 'max_features': 'auto',\n",
      " 'max_leaf_nodes': None,\n",
      " 'max_samples': None,\n",
      " 'min_impurity_decrease': 0.0,\n",
      " 'min_impurity_split': None,\n",
      " 'min_samples_leaf': 1,\n",
      " 'min_samples_split': 2,\n",
      " 'min_weight_fraction_leaf': 0.0,\n",
      " 'n_estimators': 100,\n",
      " 'n_jobs': None,\n",
      " 'oob_score': False,\n",
      " 'random_state': 1,\n",
      " 'verbose': 0,\n",
      " 'warm_start': False}\n"
     ]
    }
   ],
   "source": [
    "# define model: I just add some default parameters but you could\n",
    "# also just write: RandomForestRegressor() since we will perform a grid search \n",
    "# to find good hyperparameter values\n",
    "model_rf = RandomForestRegressor(random_state=1, criterion='mae', bootstrap=True)\n",
    "\n",
    "# Look at parameters used by our current forest\n",
    "print('Parameters currently in use:\\n')\n",
    "pprint(model_rf.get_params())\n",
    "\n",
    "# add to pipeline\n",
    "pipeline_rf = Pipeline(memory=None,\n",
    "              steps=[('preprocessor', preprocessor),\n",
    "                     ('regressor', model_rf)])\n",
    "\n",
    "\n",
    "# add transformation on the target variable, by default power transformation \n",
    "# also performs standardization after performing the power transformation\n",
    "# and back transform to the original space when outputting predictions \n",
    "transformer_target = PowerTransformer(method='yeo-johnson',standardize=True)\n",
    "pipeline_rf_update = TransformedTargetRegressor(regressor=pipeline_rf, \n",
    "                                         transformer=transformer_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 500, stop = 4000, num = 20)]\n",
    "\n",
    "# The number of features to consider when looking for the best split:\n",
    "# - If “auto”, then max_features=n_features.\n",
    "# - If “sqrt”, then max_features=sqrt(n_features).\n",
    "# - If “log2”, then max_features=log2(n_features).\n",
    "# - If None, then max_features=n_features\n",
    "max_features = ['auto', 'sqrt','log2',None]\n",
    "\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(20, 200, num = 20)]\n",
    "max_depth.append(None)\n",
    "\n",
    "# The function to measure the quality of a split\n",
    "criterion = ['mse','mae']\n",
    "\n",
    "# Minimum number of samples required to split a node\n",
    "# - If int, then consider min_samples_split as the minimum number.\n",
    "# - If float, then min_samples_split is a fraction and ceil(min_samples_split * n_samples) are the minimum number of samples for each split.\n",
    "min_samples_split = [0.001, 0.01, 0.025, 0.05]\n",
    "# Minimum number of samples required at each leaf node\n",
    "\n",
    "# - If int, then consider min_samples_leaf as the minimum number.\n",
    "# - If float, then min_samples_leaf is a fraction and ceil(min_samples_leaf * n_samples) are the minimum number of samples for each node.\n",
    "min_samples_leaf = [0.001, 0.01, 0.025, 0.05,]\n",
    "\n",
    "# Create the random grid\n",
    "random_grid_rf = {\n",
    "   'regressor__regressor__n_estimators': n_estimators,\n",
    "   'regressor__regressor__max_features': max_features,\n",
    "   'regressor__regressor__max_depth': max_depth,\n",
    "   'regressor__regressor__criterion': criterion,\n",
    "   'regressor__regressor__min_samples_split': min_samples_split,\n",
    "   'regressor__regressor__min_samples_leaf': min_samples_leaf}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helpful articles:\n",
    " - custom scoring metric: https://stackoverflow.com/questions/48468115/how-to-create-a-customized-scoring-function-in-scikit-learn-for-scoring-a-set-of\n",
    " - random parameter search: https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74\n",
    "you can also use the traditional grid search in pyhton, but I prefer the randomizedSearch\n",
    " - Once the optimal parameters are found for the model, you don't need to run this again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Invalid parameter n_estimators for estimator TransformedTargetRegressor(check_inverse=True, func=None, inverse_func=None,\n                           regressor=Pipeline(memory=None,\n                                              steps=[('preprocessor',\n                                                      ColumnTransformer(n_jobs=None,\n                                                                        remainder='drop',\n                                                                        sparse_threshold=0.3,\n                                                                        transformer_weights=None,\n                                                                        transformers=[('num',\n                                                                                       Pipeline(memory=None,\n                                                                                                steps=[('imputer',\n                                                                                                        IterativeImputer(add_indicator=False,\n                                                                                                                         estimator=None,\n                                                                                                                         imputation_order=...\n                                                                            max_features='auto',\n                                                                            max_leaf_nodes=None,\n                                                                            max_samples=None,\n                                                                            min_impurity_decrease=0.0,\n                                                                            min_impurity_split=None,\n                                                                            min_samples_leaf=1,\n                                                                            min_samples_split=2,\n                                                                            min_weight_fraction_leaf=0.0,\n                                                                            n_estimators=100,\n                                                                            n_jobs=None,\n                                                                            oob_score=False,\n                                                                            random_state=1,\n                                                                            verbose=0,\n                                                                            warm_start=False))],\n                                              verbose=False),\n                           transformer=PowerTransformer(copy=True,\n                                                        method='yeo-johnson',\n                                                        standardize=True)). Check the list of available parameters with `estimator.get_params().keys()`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"C:\\Users\\Pieter-Jan\\Anaconda3\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 418, in _process_worker\n    r = call_item()\n  File \"C:\\Users\\Pieter-Jan\\Anaconda3\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 272, in __call__\n    return self.fn(*self.args, **self.kwargs)\n  File \"C:\\Users\\Pieter-Jan\\Anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 608, in __call__\n    return self.func(*args, **kwargs)\n  File \"C:\\Users\\Pieter-Jan\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 256, in __call__\n    for func, args, kwargs in self.items]\n  File \"C:\\Users\\Pieter-Jan\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 256, in <listcomp>\n    for func, args, kwargs in self.items]\n  File \"C:\\Users\\Pieter-Jan\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 504, in _fit_and_score\n    estimator = estimator.set_params(**cloned_parameters)\n  File \"C:\\Users\\Pieter-Jan\\Anaconda3\\lib\\site-packages\\sklearn\\base.py\", line 236, in set_params\n    (key, self))\nValueError: Invalid parameter n_estimators for estimator TransformedTargetRegressor(check_inverse=True, func=None, inverse_func=None,\n                           regressor=Pipeline(memory=None,\n                                              steps=[('preprocessor',\n                                                      ColumnTransformer(n_jobs=None,\n                                                                        remainder='drop',\n                                                                        sparse_threshold=0.3,\n                                                                        transformer_weights=None,\n                                                                        transformers=[('num',\n                                                                                       Pipeline(memory=None,\n                                                                                                steps=[('imputer',\n                                                                                                        IterativeImputer(add_indicator=False,\n                                                                                                                         estimator=None,\n                                                                                                                         imputation_order=...\n                                                                            max_features='auto',\n                                                                            max_leaf_nodes=None,\n                                                                            max_samples=None,\n                                                                            min_impurity_decrease=0.0,\n                                                                            min_impurity_split=None,\n                                                                            min_samples_leaf=1,\n                                                                            min_samples_split=2,\n                                                                            min_weight_fraction_leaf=0.0,\n                                                                            n_estimators=100,\n                                                                            n_jobs=None,\n                                                                            oob_score=False,\n                                                                            random_state=1,\n                                                                            verbose=0,\n                                                                            warm_start=False))],\n                                              verbose=False),\n                           transformer=PowerTransformer(copy=True,\n                                                        method='yeo-johnson',\n                                                        standardize=True)). Check the list of available parameters with `estimator.get_params().keys()`.\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-ffdb07513915>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;31m# run grid search and refit with best hyper parameters\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mrf_random_search\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrf_random_search\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrf_random_search\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_score_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    708\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    709\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 710\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    711\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    712\u001b[0m         \u001b[1;31m# For multi-metric evaluation, store the best_index_, best_params_ and\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1482\u001b[0m         evaluate_candidates(ParameterSampler(\n\u001b[0;32m   1483\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparam_distributions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1484\u001b[1;33m             random_state=self.random_state))\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[1;34m(candidate_params)\u001b[0m\n\u001b[0;32m    687\u001b[0m                                \u001b[1;32mfor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    688\u001b[0m                                in product(candidate_params,\n\u001b[1;32m--> 689\u001b[1;33m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[0;32m    690\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    691\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1015\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1016\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1017\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1018\u001b[0m             \u001b[1;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1019\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    907\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    908\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'supports_timeout'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 909\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    910\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    911\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[1;34m(future, timeout)\u001b[0m\n\u001b[0;32m    560\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[0;32m    561\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 562\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    563\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mLokyTimeoutError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    564\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\concurrent\\futures\\_base.py\u001b[0m in \u001b[0;36mresult\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    430\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mCancelledError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    431\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mFINISHED\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 432\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    433\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    434\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\concurrent\\futures\\_base.py\u001b[0m in \u001b[0;36m__get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    382\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__get_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    383\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 384\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    385\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    386\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Invalid parameter n_estimators for estimator TransformedTargetRegressor(check_inverse=True, func=None, inverse_func=None,\n                           regressor=Pipeline(memory=None,\n                                              steps=[('preprocessor',\n                                                      ColumnTransformer(n_jobs=None,\n                                                                        remainder='drop',\n                                                                        sparse_threshold=0.3,\n                                                                        transformer_weights=None,\n                                                                        transformers=[('num',\n                                                                                       Pipeline(memory=None,\n                                                                                                steps=[('imputer',\n                                                                                                        IterativeImputer(add_indicator=False,\n                                                                                                                         estimator=None,\n                                                                                                                         imputation_order=...\n                                                                            max_features='auto',\n                                                                            max_leaf_nodes=None,\n                                                                            max_samples=None,\n                                                                            min_impurity_decrease=0.0,\n                                                                            min_impurity_split=None,\n                                                                            min_samples_leaf=1,\n                                                                            min_samples_split=2,\n                                                                            min_weight_fraction_leaf=0.0,\n                                                                            n_estimators=100,\n                                                                            n_jobs=None,\n                                                                            oob_score=False,\n                                                                            random_state=1,\n                                                                            verbose=0,\n                                                                            warm_start=False))],\n                                              verbose=False),\n                           transformer=PowerTransformer(copy=True,\n                                                        method='yeo-johnson',\n                                                        standardize=True)). Check the list of available parameters with `estimator.get_params().keys()`."
     ]
    }
   ],
   "source": [
    "# define random search (and narrow down time grid search)\n",
    "rf_random_search = RandomizedSearchCV(\n",
    "   estimator = pipeline_rf_update, \n",
    "   param_distributions = random_grid_rf, n_iter = 20,  \n",
    "   cv = 10, verbose=2, random_state=1, n_jobs = -1, refit=True,\n",
    "   scoring=make_scorer(custom_scoring_func, greater_is_better=False)\n",
    ")\n",
    "\n",
    "\n",
    "# run grid search and refit with best hyper parameters\n",
    "rf_random_search.fit(X_train, y_train)  \n",
    "print(rf_random_search.best_params_)    \n",
    "print(rf_random_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **TODO make a plot that visualizes hyperparameters (maybe kind of heatmap)**\n",
    "- Once we have found good tuning parameters write them down so we don't need to redo this step over and over"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_regressor__regressor__n_estimators</th>\n",
       "      <th>param_regressor__regressor__min_samples_split</th>\n",
       "      <th>param_regressor__regressor__min_samples_leaf</th>\n",
       "      <th>param_regressor__regressor__max_features</th>\n",
       "      <th>param_regressor__regressor__max_depth</th>\n",
       "      <th>param_regressor__regressor__criterion</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>split5_test_score</th>\n",
       "      <th>split6_test_score</th>\n",
       "      <th>split7_test_score</th>\n",
       "      <th>split8_test_score</th>\n",
       "      <th>split9_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>31.905521</td>\n",
       "      <td>2.291826</td>\n",
       "      <td>0.673621</td>\n",
       "      <td>0.081532</td>\n",
       "      <td>2710</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>None</td>\n",
       "      <td>29</td>\n",
       "      <td>mse</td>\n",
       "      <td>{'regressor__regressor__n_estimators': 2710, '...</td>\n",
       "      <td>-271.807497</td>\n",
       "      <td>-491.544084</td>\n",
       "      <td>-300.191495</td>\n",
       "      <td>-259.885802</td>\n",
       "      <td>-346.331515</td>\n",
       "      <td>-437.885004</td>\n",
       "      <td>-310.700210</td>\n",
       "      <td>-404.912016</td>\n",
       "      <td>-326.861040</td>\n",
       "      <td>-224.964401</td>\n",
       "      <td>-337.508306</td>\n",
       "      <td>79.784820</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>160.234444</td>\n",
       "      <td>2.397188</td>\n",
       "      <td>0.498756</td>\n",
       "      <td>0.125669</td>\n",
       "      <td>1789</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.01</td>\n",
       "      <td>auto</td>\n",
       "      <td>143</td>\n",
       "      <td>mae</td>\n",
       "      <td>{'regressor__regressor__n_estimators': 1789, '...</td>\n",
       "      <td>-265.614472</td>\n",
       "      <td>-514.263427</td>\n",
       "      <td>-297.168201</td>\n",
       "      <td>-285.780397</td>\n",
       "      <td>-344.010039</td>\n",
       "      <td>-450.697282</td>\n",
       "      <td>-312.054909</td>\n",
       "      <td>-403.668284</td>\n",
       "      <td>-334.922721</td>\n",
       "      <td>-212.709695</td>\n",
       "      <td>-342.088943</td>\n",
       "      <td>86.002784</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>59.629014</td>\n",
       "      <td>2.309821</td>\n",
       "      <td>0.264329</td>\n",
       "      <td>0.039237</td>\n",
       "      <td>1052</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.01</td>\n",
       "      <td>None</td>\n",
       "      <td>200</td>\n",
       "      <td>mae</td>\n",
       "      <td>{'regressor__regressor__n_estimators': 1052, '...</td>\n",
       "      <td>-347.097384</td>\n",
       "      <td>-601.049926</td>\n",
       "      <td>-326.204480</td>\n",
       "      <td>-377.856443</td>\n",
       "      <td>-394.213070</td>\n",
       "      <td>-501.337968</td>\n",
       "      <td>-397.639881</td>\n",
       "      <td>-435.187274</td>\n",
       "      <td>-380.328694</td>\n",
       "      <td>-281.407223</td>\n",
       "      <td>-404.232234</td>\n",
       "      <td>86.587846</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>80.657950</td>\n",
       "      <td>4.263683</td>\n",
       "      <td>1.195949</td>\n",
       "      <td>0.426722</td>\n",
       "      <td>4000</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.1</td>\n",
       "      <td>None</td>\n",
       "      <td>171</td>\n",
       "      <td>mae</td>\n",
       "      <td>{'regressor__regressor__n_estimators': 4000, '...</td>\n",
       "      <td>-404.774388</td>\n",
       "      <td>-614.720666</td>\n",
       "      <td>-378.318576</td>\n",
       "      <td>-454.197086</td>\n",
       "      <td>-485.439579</td>\n",
       "      <td>-511.058218</td>\n",
       "      <td>-371.932770</td>\n",
       "      <td>-492.081397</td>\n",
       "      <td>-439.081769</td>\n",
       "      <td>-361.057415</td>\n",
       "      <td>-451.266186</td>\n",
       "      <td>74.294994</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>43.493094</td>\n",
       "      <td>3.449362</td>\n",
       "      <td>0.639099</td>\n",
       "      <td>0.147347</td>\n",
       "      <td>1973</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.1</td>\n",
       "      <td>auto</td>\n",
       "      <td>143</td>\n",
       "      <td>mae</td>\n",
       "      <td>{'regressor__regressor__n_estimators': 1973, '...</td>\n",
       "      <td>-405.087960</td>\n",
       "      <td>-615.673623</td>\n",
       "      <td>-378.203569</td>\n",
       "      <td>-453.525028</td>\n",
       "      <td>-484.163255</td>\n",
       "      <td>-510.721119</td>\n",
       "      <td>-372.306905</td>\n",
       "      <td>-492.854036</td>\n",
       "      <td>-441.099973</td>\n",
       "      <td>-362.349818</td>\n",
       "      <td>-451.598529</td>\n",
       "      <td>74.226056</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "2      31.905521      2.291826         0.673621        0.081532   \n",
       "3     160.234444      2.397188         0.498756        0.125669   \n",
       "7      59.629014      2.309821         0.264329        0.039237   \n",
       "5      80.657950      4.263683         1.195949        0.426722   \n",
       "9      43.493094      3.449362         0.639099        0.147347   \n",
       "\n",
       "  param_regressor__regressor__n_estimators  \\\n",
       "2                                     2710   \n",
       "3                                     1789   \n",
       "7                                     1052   \n",
       "5                                     4000   \n",
       "9                                     1973   \n",
       "\n",
       "  param_regressor__regressor__min_samples_split  \\\n",
       "2                                          0.01   \n",
       "3                                         0.001   \n",
       "7                                           0.2   \n",
       "5                                         0.025   \n",
       "9                                         0.025   \n",
       "\n",
       "  param_regressor__regressor__min_samples_leaf  \\\n",
       "2                                         0.01   \n",
       "3                                         0.01   \n",
       "7                                         0.01   \n",
       "5                                          0.1   \n",
       "9                                          0.1   \n",
       "\n",
       "  param_regressor__regressor__max_features  \\\n",
       "2                                     None   \n",
       "3                                     auto   \n",
       "7                                     None   \n",
       "5                                     None   \n",
       "9                                     auto   \n",
       "\n",
       "  param_regressor__regressor__max_depth param_regressor__regressor__criterion  \\\n",
       "2                                    29                                   mse   \n",
       "3                                   143                                   mae   \n",
       "7                                   200                                   mae   \n",
       "5                                   171                                   mae   \n",
       "9                                   143                                   mae   \n",
       "\n",
       "                                              params  split0_test_score  \\\n",
       "2  {'regressor__regressor__n_estimators': 2710, '...        -271.807497   \n",
       "3  {'regressor__regressor__n_estimators': 1789, '...        -265.614472   \n",
       "7  {'regressor__regressor__n_estimators': 1052, '...        -347.097384   \n",
       "5  {'regressor__regressor__n_estimators': 4000, '...        -404.774388   \n",
       "9  {'regressor__regressor__n_estimators': 1973, '...        -405.087960   \n",
       "\n",
       "   split1_test_score  split2_test_score  split3_test_score  split4_test_score  \\\n",
       "2        -491.544084        -300.191495        -259.885802        -346.331515   \n",
       "3        -514.263427        -297.168201        -285.780397        -344.010039   \n",
       "7        -601.049926        -326.204480        -377.856443        -394.213070   \n",
       "5        -614.720666        -378.318576        -454.197086        -485.439579   \n",
       "9        -615.673623        -378.203569        -453.525028        -484.163255   \n",
       "\n",
       "   split5_test_score  split6_test_score  split7_test_score  split8_test_score  \\\n",
       "2        -437.885004        -310.700210        -404.912016        -326.861040   \n",
       "3        -450.697282        -312.054909        -403.668284        -334.922721   \n",
       "7        -501.337968        -397.639881        -435.187274        -380.328694   \n",
       "5        -511.058218        -371.932770        -492.081397        -439.081769   \n",
       "9        -510.721119        -372.306905        -492.854036        -441.099973   \n",
       "\n",
       "   split9_test_score  mean_test_score  std_test_score  rank_test_score  \n",
       "2        -224.964401      -337.508306       79.784820                1  \n",
       "3        -212.709695      -342.088943       86.002784                2  \n",
       "7        -281.407223      -404.232234       86.587846                3  \n",
       "5        -361.057415      -451.266186       74.294994                4  \n",
       "9        -362.349818      -451.598529       74.226056                5  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# have look at the best hyperparameters and their respective performance (maybe also look at the sd)\n",
    "pd.DataFrame(rf_random_search.cv_results_).sort_values(\n",
    "    by=['mean_test_score'],ascending=False).head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B) Performance on validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'minimum price': 122.62542422255532,\n",
       " 'maximum price': 125.07706505859072,\n",
       " 'total error': 247.70248928114603}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_perf(y_val, rf_random_search.predict(X_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C) Post processing\n",
    "\n",
    " - inspect predictions/residuals (make visualisations) (See Bram)\n",
    " - feature importance (see Bram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D) Predictions test data\n",
    "\n",
    "Refit on all training data (using the parameters found on the random search) and submit prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'regressor__regressor__n_estimators'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-069a28ffd01f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m      \u001b[1;33m**\u001b[0m\u001b[0mrf_random_search\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m      \u001b[0mbootstrap\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m      \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m )\n\u001b[0;32m      7\u001b[0m \u001b[1;31m# add to pipeline\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'regressor__regressor__n_estimators'"
     ]
    }
   ],
   "source": [
    "# train your final model on all data with best parameters \n",
    "model_rf_final = RandomForestRegressor(\n",
    "     **rf_random_search.best_params_,\n",
    "     bootstrap=True,\n",
    "     n_jobs=-1\n",
    ")\n",
    "# add to pipeline\n",
    "pipeline_rf_final = Pipeline(memory=None,\n",
    "              steps=[('preprocessor', preprocessor),\n",
    "                     ('regressor', model_rf_final)],\n",
    "              verbose=True)\n",
    "\n",
    "# again add transformer for target variable\n",
    "pipeline_rf_final = TransformedTargetRegressor(regressor=pipeline_rf_final, \n",
    "                                         transformer=transformer_target)\n",
    "\n",
    "# fit final model on all training data we have at hand\n",
    "pipeline_rf_final = pipeline_rf_final.fit(X_all_train, y_all_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'minimum price': 4.037629680842831,\n",
       " 'maximum price': 4.20369025192588,\n",
       " 'total error': 8.241319932768711}"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# performance on all data where the model was fit one (should be very low)\n",
    "calculate_perf(pipeline_rf_final.predict(X_all_train), y_all_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions on test data\n",
    "rf_pred_test = pipeline_rf_final.predict(X_test)\n",
    "\n",
    "# submission format\n",
    "rf_submission_format = pd.DataFrame.from_dict(\n",
    " {'ID':df_test['id'].values,\n",
    " 'MIN':rf_pred_test[:,0],\n",
    " 'MAX':rf_pred_test[:,1]}).set_index('ID')\n",
    "\n",
    "# write to csv\n",
    "rf_submission_format.to_csv('../output/predictions_test/rf_python.csv' ,\n",
    "                            header=True, index=True, sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Support Vector Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Penalized Linear regression (Lasso, Ridge, Elastic net)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
