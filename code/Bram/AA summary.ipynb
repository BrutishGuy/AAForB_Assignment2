{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "This notebook is to compare the results of predicting min price and max price of the different techniques: random forest, gradient boosting, random forest with log transforming target variable, and gradient boosting with log transforming target variable. Random forest with log transforming target variable turns out to be the best.\n",
    "\n",
    "To better understand this notebook take a look at the other notebooks to understand how EDA, feature engineering, missing data and hyperparameter tuning is done. Other notebook contain plots and explanation.\n",
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "df =pd.read_csv('train.csv')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering\n",
    "Do all manipulations on the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-2-53f173d18cfa>:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['brand'][mask] = 'Other1'\n",
      "<ipython-input-2-53f173d18cfa>:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['cpu'][mask] = 'Other1'\n",
      "<ipython-input-2-53f173d18cfa>:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['cpu'][mask] = 'Other1'\n"
     ]
    }
   ],
   "source": [
    "# Drop rows where ''screen_surface', 'weight' and 'detachable_keyboard' has a missing value\n",
    "df.dropna(subset=['screen_surface'], how='any', inplace=True)\n",
    "df.dropna(subset=['detachable_keyboard'], how='any', inplace=True)\n",
    "df.dropna(subset=['weight'], how='any', inplace=True)\n",
    "\n",
    "\n",
    "df=df.drop(columns=['pixels_y'])\n",
    "df=df.drop(columns=['name'])\n",
    "df=df.drop(columns=[ 'os_details'])\n",
    "df=df.drop(columns=[ 'cpu_details'])\n",
    "df=df.drop(columns=[ 'base_name'])\n",
    "df=df.drop(columns=[ 'gpu'])\n",
    "df=df.drop(columns=[ 'id'])\n",
    "\n",
    "\n",
    "countries = df['brand']\n",
    "country_counts = countries.value_counts()\n",
    "mask = df['brand'].isin(country_counts[country_counts < 9].index)\n",
    "# Label all other categories as Other\n",
    "df['brand'][mask] = 'Other1'\n",
    "\n",
    "countries = df['cpu']\n",
    "country_counts = countries.value_counts()\n",
    "mask = df['cpu'].isin(country_counts[country_counts < 9].index.drop('Intel Core i9'))\n",
    "# Label all other categories as Other1\n",
    "df['cpu'][mask] = 'Other1'\n",
    "mask=df['cpu'].isin(['Intel Pentium','AMD A6'])\n",
    "df['cpu'][mask] = 'Other1'\n",
    "\n",
    "\n",
    "df['screen_surface']=df['screen_surface'].str.lower()\n",
    "df=pd.get_dummies(df, columns=['screen_surface'], drop_first=True, prefix='DM')\n",
    "df=pd.get_dummies(df, columns=['os'], drop_first=True, prefix='DM')\n",
    "df=pd.get_dummies(df, columns=['brand'], drop_first=True, prefix='DM')\n",
    "df=pd.get_dummies(df, columns=['cpu'], drop_first=True, prefix='DM')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Max price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=df.drop(columns=['min_price', 'max_price'])\n",
    "y_log=df['max_price'].apply(np.log) ##take logtransformation of max price\n",
    "y=df['max_price']\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "n=5\n",
    "score=np.zeros((8, n))\n",
    "for i in range(n):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=37+i)\n",
    "    \n",
    "    \n",
    "    ##Random forest\n",
    "    \n",
    "    # Setup the hyperparameter grid\n",
    "    grid = {'n_estimators': [200], 'max_depth': [3,5,9], 'max_features': [4,6,8,12], 'random_state': [42]} \n",
    "    # Instantiate rfr\n",
    "    rfr = RandomForestRegressor()\n",
    "    # Instantiate the RandomizedSearchCV object: gbr_cv\n",
    "    rfr_cv = RandomizedSearchCV(rfr, grid,n_iter=10, cv=5, scoring='neg_mean_absolute_error')\n",
    "    # Fit it to the data\n",
    "    rfr_cv.fit(X_train,y_train)\n",
    "\n",
    "    rfr.set_params(**rfr_cv.best_params_)\n",
    "    rfr.fit(X_train,y_train)\n",
    "    train_predictions = rfr.predict(X_train)\n",
    "    test_predictions = rfr.predict(X_test)\n",
    "    \n",
    "    score[0][i]=mean_absolute_error(y_train,  train_predictions)\n",
    "    score[1][i]=mean_absolute_error(y_test,  test_predictions)\n",
    "    \n",
    "    ## Gradient boost\n",
    "\n",
    "    # Setup the hyperparameter grid\n",
    "    grid = {'n_estimators': [200], 'learning_rate': [0.01,0.05,0.1], 'max_features': [4,6,8,12], 'subsample':[0.3,0.6,0.8], 'random_state': [42]}\n",
    "    # Instantiate gbr\n",
    "    gbr = GradientBoostingRegressor()\n",
    "    # Instantiate the RandomizedSearchCV object: gbr_cv\n",
    "    gbr_cv = RandomizedSearchCV(gbr, grid,n_iter=25, cv=5, scoring='neg_mean_absolute_error')\n",
    "    # Fit it to the data\n",
    "    gbr_cv.fit(X_train,y_train)\n",
    "\n",
    "    gbr.set_params(**gbr_cv.best_params_)\n",
    "    gbr.fit(X_train,y_train)\n",
    "    train_predictions = gbr.predict(X_train)\n",
    "    test_predictions = gbr.predict(X_test)\n",
    "\n",
    "    score[2][i]=mean_absolute_error(y_train,  train_predictions)\n",
    "    score[3][i]=mean_absolute_error(y_test,  test_predictions)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #with log(target)------------------------------------------------------------------------------------\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y_log, test_size=0.20, random_state=37+i)\n",
    "    ##Random forrest\n",
    "    \n",
    "    # Setup the hyperparameter grid\n",
    "    grid = {'n_estimators': [200], 'max_depth': [3,5,9], 'max_features': [4,6,8,12], 'random_state': [42]} \n",
    "    # Instantiate rfr\n",
    "    rfr = RandomForestRegressor()\n",
    "    # Instantiate the RandomizedSearchCV object: gbr_cv\n",
    "    rfr_cv = RandomizedSearchCV(rfr, grid, n_iter=10, cv=5, scoring='neg_mean_absolute_error')\n",
    "    # Fit it to the data\n",
    "    rfr_cv.fit(X_train,y_train)\n",
    "\n",
    "    rfr.set_params(**rfr_cv.best_params_)\n",
    "    rfr.fit(X_train,y_train)\n",
    "    train_predictions = np.exp(rfr.predict(X_train))\n",
    "    test_predictions = np.exp(rfr.predict(X_test))\n",
    "    \n",
    "    score[4][i]=mean_absolute_error(np.exp(y_train),  train_predictions)\n",
    "    score[5][i]=mean_absolute_error(np.exp(y_test),  test_predictions)\n",
    "    \n",
    "    ## Gradient boost\n",
    "\n",
    "    # Setup the hyperparameter grid\n",
    "    grid = {'n_estimators': [200], 'learning_rate': [0.01,0.05,0.1], 'max_features': [12,6,8,4], 'subsample':[0.3,0.6,0.8], 'random_state': [42]}\n",
    "    # Instantiate gbr\n",
    "    gbr = GradientBoostingRegressor()\n",
    "    # Instantiate the RandomizedSearchCV object: gbr_cv\n",
    "    gbr_cv = RandomizedSearchCV(gbr, grid, n_iter=25, cv=5, scoring='neg_mean_absolute_error')\n",
    "    # Fit it to the data\n",
    "    gbr_cv.fit(X_train,y_train)\n",
    "\n",
    "    gbr.set_params(**gbr_cv.best_params_)\n",
    "    gbr.fit(X_train,y_train)\n",
    "    train_predictions = np.exp(gbr.predict(X_train))\n",
    "    test_predictions = np.exp(gbr.predict(X_test))\n",
    "\n",
    "    score[6][i]=mean_absolute_error(np.exp(y_train),  train_predictions)\n",
    "    score[7][i]=mean_absolute_error(np.exp(y_test),  test_predictions)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 96.21530715,  96.54981008,  99.35355761,  96.67432904,\n",
       "         97.19023736],\n",
       "       [159.41551309, 155.15677492, 128.39322473, 160.77959469,\n",
       "        156.10788101],\n",
       "       [135.73071167, 134.03144477, 136.71586033, 135.1800637 ,\n",
       "        117.24037706],\n",
       "       [171.66411048, 172.55832311, 149.7940902 , 172.06260003,\n",
       "        163.63791514],\n",
       "       [ 99.50115818, 100.3992163 , 100.98304484,  97.48573268,\n",
       "         97.45134849],\n",
       "       [161.58549722, 147.79263493, 124.50559616, 153.85525307,\n",
       "        154.60177412],\n",
       "       [114.22098295, 118.83154195, 122.13275252, 102.36168284,\n",
       "        116.26530484],\n",
       "       [163.61769414, 159.47443002, 135.70153059, 165.06251604,\n",
       "        159.55901825]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoresdf=pd.DataFrame(data=score, index=['rfr_train', 'rfr_test',\"gbr_train\", \"gbr_test\",'rfr_train_log',\\\n",
    "                                'rfr_test_log',\"gbr_train_log\", \"gbr_test_log\"], columns=[\"rs37\", \"rs38\",\"rs39\",\"rs40\",\"rs41\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rfr_train         98.961388\n",
       "rfr_test         152.893670\n",
       "gbr_train        124.878027\n",
       "gbr_test         163.494866\n",
       "rfr_train_log     95.925332\n",
       "rfr_test_log     149.321338\n",
       "gbr_train_log    112.395985\n",
       "gbr_test_log     158.718170\n",
       "dtype: float64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scoresdf.mean(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only have to look on how well the regressors perform on the test set. So Random forest with first log transform max price performs best: the mean MAE over 5 different random splits in test and train data is 149."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Min price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['min_price', 'max_price'])\n",
    "y_log=df['min_price'].apply(np.log) ##take logtransformation of max price\n",
    "y=df['min_price']\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "n=3\n",
    "score=np.zeros((8, n))\n",
    "for i in range(n):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=37+i)\n",
    "    \n",
    "    \n",
    "    ##Random forest\n",
    "    \n",
    "    # Setup the hyperparameter grid\n",
    "    grid = {'n_estimators': [200], 'max_depth': [3,5,9], 'max_features': [4,6,8,12], 'random_state': [42]} \n",
    "    # Instantiate rfr\n",
    "    rfr = RandomForestRegressor()\n",
    "    # Instantiate the RandomizedSearchCV object: gbr_cv\n",
    "    rfr_cv = RandomizedSearchCV(rfr, grid,n_iter=10, cv=5, scoring='neg_mean_absolute_error')\n",
    "    # Fit it to the data\n",
    "    rfr_cv.fit(X_train,y_train)\n",
    "\n",
    "    rfr.set_params(**rfr_cv.best_params_)\n",
    "    rfr.fit(X_train,y_train)\n",
    "    train_predictions = rfr.predict(X_train)\n",
    "    test_predictions = rfr.predict(X_test)\n",
    "    \n",
    "    score[0][i]=mean_absolute_error(y_train,  train_predictions)\n",
    "    score[1][i]=mean_absolute_error(y_test,  test_predictions)\n",
    "    \n",
    "    ## Gradient boost\n",
    "\n",
    "    # Setup the hyperparameter grid\n",
    "    grid = {'n_estimators': [200], 'learning_rate': [0.01,0.05,0.1], 'max_features': [4,6,8,12], 'subsample':[0.3,0.6,0.8], 'random_state': [42]}\n",
    "    # Instantiate gbr\n",
    "    gbr = GradientBoostingRegressor()\n",
    "    # Instantiate the RandomizedSearchCV object: gbr_cv\n",
    "    gbr_cv = RandomizedSearchCV(gbr, grid,n_iter=20, cv=5, scoring='neg_mean_absolute_error')\n",
    "    # Fit it to the data\n",
    "    gbr_cv.fit(X_train,y_train)\n",
    "\n",
    "    gbr.set_params(**gbr_cv.best_params_)\n",
    "    gbr.fit(X_train,y_train)\n",
    "    train_predictions = gbr.predict(X_train)\n",
    "    test_predictions = gbr.predict(X_test)\n",
    "\n",
    "    score[2][i]=mean_absolute_error(y_train,  train_predictions)\n",
    "    score[3][i]=mean_absolute_error(y_test,  test_predictions)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #with log(target)------------------------------------------------------------------------------------\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y_log, test_size=0.20, random_state=37+i)\n",
    "    ##Random forrest\n",
    "    \n",
    "    # Setup the hyperparameter grid\n",
    "    grid = {'n_estimators': [200], 'max_depth': [3,5,9], 'max_features': [4,6,8,12], 'random_state': [42]} \n",
    "    # Instantiate rfr\n",
    "    rfr = RandomForestRegressor()\n",
    "    # Instantiate the RandomizedSearchCV object: gbr_cv\n",
    "    rfr_cv = RandomizedSearchCV(rfr, grid, n_iter=10, cv=5, scoring='neg_mean_absolute_error')\n",
    "    # Fit it to the data\n",
    "    rfr_cv.fit(X_train,y_train)\n",
    "\n",
    "    rfr.set_params(**rfr_cv.best_params_)\n",
    "    rfr.fit(X_train,y_train)\n",
    "    train_predictions = np.exp(rfr.predict(X_train))\n",
    "    test_predictions = np.exp(rfr.predict(X_test))\n",
    "    \n",
    "    score[4][i]=mean_absolute_error(np.exp(y_train),  train_predictions)\n",
    "    score[5][i]=mean_absolute_error(np.exp(y_test),  test_predictions)\n",
    "    \n",
    "    ## Gradient boost\n",
    "\n",
    "    # Setup the hyperparameter grid\n",
    "    grid = {'n_estimators': [200], 'learning_rate': [0.01,0.05,0.1], 'max_features': [12,6,8,4], 'subsample':[0.3,0.6,0.8], 'random_state': [42]}\n",
    "    # Instantiate gbr\n",
    "    gbr = GradientBoostingRegressor()\n",
    "    # Instantiate the RandomizedSearchCV object: gbr_cv\n",
    "    gbr_cv = RandomizedSearchCV(gbr, grid, n_iter=20, cv=5, scoring='neg_mean_absolute_error')\n",
    "    # Fit it to the data\n",
    "    gbr_cv.fit(X_train,y_train)\n",
    "\n",
    "    gbr.set_params(**gbr_cv.best_params_)\n",
    "    gbr.fit(X_train,y_train)\n",
    "    train_predictions = np.exp(gbr.predict(X_train))\n",
    "    test_predictions = np.exp(gbr.predict(X_test))\n",
    "\n",
    "    score[6][i]=mean_absolute_error(np.exp(y_train),  train_predictions)\n",
    "    score[7][i]=mean_absolute_error(np.exp(y_test),  test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rs37</th>\n",
       "      <th>rs38</th>\n",
       "      <th>rs39</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>rfr_train</th>\n",
       "      <td>94.418813</td>\n",
       "      <td>89.892766</td>\n",
       "      <td>95.731857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rfr_test</th>\n",
       "      <td>153.856743</td>\n",
       "      <td>156.508796</td>\n",
       "      <td>131.294116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gbr_train</th>\n",
       "      <td>121.330137</td>\n",
       "      <td>107.256286</td>\n",
       "      <td>135.334284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gbr_test</th>\n",
       "      <td>159.682048</td>\n",
       "      <td>170.314470</td>\n",
       "      <td>140.969743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rfr_train_log</th>\n",
       "      <td>91.977888</td>\n",
       "      <td>93.302255</td>\n",
       "      <td>92.094036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rfr_test_log</th>\n",
       "      <td>151.707037</td>\n",
       "      <td>151.240092</td>\n",
       "      <td>124.896922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gbr_train_log</th>\n",
       "      <td>111.660942</td>\n",
       "      <td>101.731656</td>\n",
       "      <td>94.950184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gbr_test_log</th>\n",
       "      <td>160.208018</td>\n",
       "      <td>159.247034</td>\n",
       "      <td>135.390251</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     rs37        rs38        rs39\n",
       "rfr_train       94.418813   89.892766   95.731857\n",
       "rfr_test       153.856743  156.508796  131.294116\n",
       "gbr_train      121.330137  107.256286  135.334284\n",
       "gbr_test       159.682048  170.314470  140.969743\n",
       "rfr_train_log   91.977888   93.302255   92.094036\n",
       "rfr_test_log   151.707037  151.240092  124.896922\n",
       "gbr_train_log  111.660942  101.731656   94.950184\n",
       "gbr_test_log   160.208018  159.247034  135.390251"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scoresdf=pd.DataFrame(data=score, index=['rfr_train', 'rfr_test',\"gbr_train\", \"gbr_test\",'rfr_train_log',\\\n",
    "                                'rfr_test_log',\"gbr_train_log\", \"gbr_test_log\"], columns=[\"rs37\", \"rs38\",\"rs39\"])\n",
    "scoresdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rfr_train         93.347812\n",
       "rfr_test         147.219885\n",
       "gbr_train        121.306902\n",
       "gbr_test         156.988754\n",
       "rfr_train_log     92.458060\n",
       "rfr_test_log     142.614684\n",
       "gbr_train_log    102.780927\n",
       "gbr_test_log     151.615101\n",
       "dtype: float64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scoresdf.mean(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only have to look on how well the regressors perform on the test set. So Random forest with first log transform min price performs best: the mean MAE over 3 different random splits in test and train data is 143."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion: average MAE=292"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
