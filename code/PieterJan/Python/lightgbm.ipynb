{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# auto reload libraries (you do need to re-import libraries if you make changes)\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# base \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "import os\n",
    "\n",
    "# preprocessing \n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.experimental import enable_iterative_imputer \n",
    "from sklearn.impute import SimpleImputer, IterativeImputer,KNNImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, RobustScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "# variable importance\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# models\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "# visualize\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# own defined functions/classes \n",
    "from preprocessing.preprocess_pipe import MakeLowerCase\n",
    "from preprocessing.preprocess_pipe import HdResolutionCategorizer\n",
    "from preprocessing.preprocess_pipe import StorageCategorizer\n",
    "from preprocessing.preprocess_pipe import SsdCategorizer\n",
    "\n",
    "from preprocessing.preprocess_pipe import print_missing\n",
    "from preprocessing.preprocess_pipe import calculate_perf\n",
    "from preprocessing.preprocess_pipe import custom_scoring_func_single_p\n",
    "\n",
    "from modelling.weight_samples import weights_samples\n",
    "\n",
    "from postprocessing.postprocessing import plot_predictions_results\n",
    "from postprocessing.postprocessing import plot_residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Pieter-Jan\\Documents\\KuLeuven\\Semester2\\AA\\AAForB_Assignment2\n"
     ]
    }
   ],
   "source": [
    "# global parameters\n",
    "pd.set_option('display.max_columns', 500)\n",
    "# style for plotting\n",
    "plt.style.use('ggplot')\n",
    "# interactive plotting\n",
    "%matplotlib widget\n",
    "# run grid search\n",
    "RUN_GRID_SEARCH = True\n",
    "# set working directory\n",
    "uppath = lambda _path, n: os.sep.join(_path.split(os.sep)[:-n])\n",
    "__file__ = 'C:\\\\Users\\\\Pieter-Jan\\\\Documents\\\\KuLeuven\\\\Semester2\\\\AA\\\\AAForB_Assignment2\\\\code\\\\PieterJan'\n",
    "out = uppath(__file__, 2)\n",
    "os.chdir(out)\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: import data and split in train and validation set\n",
    "The validation set is more our own kind of test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions of all training data (510, 22)\n",
      "Dimension test data (222, 20)\n"
     ]
    }
   ],
   "source": [
    "# read in trainig and validation data\n",
    "# use the same data split as we did in R\n",
    "df_all_train = pd.read_csv(\"data\\\\train.csv\", sep=',')\n",
    "df_test = pd.read_csv(\"data\\\\test.csv\", sep=',')\n",
    "\n",
    "print(f'Dimensions of all training data {df_all_train.shape}')\n",
    "print(f'Dimension test data {df_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split in training and validation set\n",
    "df_train = df_all_train.sample(frac=0.75, random_state=0, replace=False)\n",
    "df_val = df_all_train.drop(df_train.index)\n",
    "\n",
    "# reset index, if you don't resit missing rows get inserted in the pipeline\n",
    "# see: https://stackoverflow.com/questions/31323499/sklearn-error-valueerror-input-contains-nan-infinity-or-a-value-too-large-for\n",
    "df_train = df_train.reset_index().drop('index',axis=1)\n",
    "df_val = df_val.reset_index().drop('index',axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: define pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Numerical features to pass down the numerical pipeline \n",
    "numerical_features = ['screen_size' ,'pixels_x','pixels_y',\n",
    "                      'ram', 'weight','ssd','storage']\n",
    "\n",
    "#Categrical features to pass down the categorical pipeline \n",
    "categorical_features = ['brand','base_name', 'screen_surface','touchscreen',\n",
    "                        'cpu','cpu_details','detachable_keyboard',\n",
    "                        'discrete_gpu','gpu', 'os','os_details']\n",
    "\n",
    "# define all unique features\n",
    "features = np.unique(numerical_features).tolist() + np.unique(categorical_features).tolist()\n",
    "\n",
    "# target variables\n",
    "target = ['min_price','max_price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "X_train = df_train[features]\n",
    "y_train = df_train[target]\n",
    "\n",
    "# validation (this is kind our own test set)\n",
    "X_val  = df_val[features]\n",
    "y_val = df_val[target]\n",
    "\n",
    "# train_validation (this is all training data we have) for fitting the model\n",
    "X_all_train = df_all_train[features]\n",
    "y_all_train = df_all_train[target]\n",
    "\n",
    "# test\n",
    "X_test = df_test[features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can add many more and \n",
    "# you can even define custom preprocessing steps like 'MakeLowerCase()'\n",
    "\n",
    "# pipeline  uses only numerical features,\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    #('imputer', SimpleImputer(missing_values=np.nan, strategy='mean')),\n",
    "    #('imputer', KNNImputer(n_neighbors=5, weights='uniform',metric='nan_euclidean')),\n",
    "    ('imputer', IterativeImputer(missing_values=np.nan, random_state=0, n_nearest_features=None, sample_posterior=True)),\n",
    "    #('transformation', PowerTransformer(method='yeo-johnson',standardize=False)),\n",
    "     #Scale features using statistics that are robust to outliers.\n",
    "    ('scaler', RobustScaler(with_centering=True, with_scaling=True, quantile_range=(10.0, 90.0)))]) \n",
    "\n",
    "# pipeline use only categorical features\n",
    "categorical_transformer = Pipeline(steps=[ \n",
    "    ('lowercase', MakeLowerCase()), # lower cases all columns containing strings\n",
    "    #('sd_category' ,SsdCategorizer(drop_original_feature=True)),\n",
    "    #('storage_category', StorageCategorizer(drop_original_feature=True)),\n",
    "    ('imputer', SimpleImputer(missing_values=np.nan, strategy=\"most_frequent\")),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "# add both preprocessing pipelines in one pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numerical_features),\n",
    "        ('cat', categorical_transformer, categorical_features)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(382, 1)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is how the preporcessed pipeline looks like (just to have an idea)\n",
    "pd.DataFrame(preprocessor.fit_transform(X_train)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: add models to pipeline\n",
    "\n",
    "Possible to fit multiple target variabels, so you **don't** need to fit a different models for min. price and max. price\n",
    "\n",
    "### A) Training and parameter tuning\n",
    "\n",
    "##### 1) Automatic tuning via grid search\n",
    "\n",
    "I will only **do the tuning for the minimum price** and use the found parameters also for the maximum price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters currently in use:\n",
      "\n",
      "{'boosting_type': 'gbdt',\n",
      " 'class_weight': None,\n",
      " 'colsample_bytree': 1.0,\n",
      " 'importance_type': 'split',\n",
      " 'learning_rate': 0.1,\n",
      " 'max_depth': -1,\n",
      " 'min_child_samples': 20,\n",
      " 'min_child_weight': 0.001,\n",
      " 'min_split_gain': 0.0,\n",
      " 'n_estimators': 100,\n",
      " 'n_jobs': -1,\n",
      " 'num_leaves': 31,\n",
      " 'objective': None,\n",
      " 'random_state': 0,\n",
      " 'reg_alpha': 0.0,\n",
      " 'reg_lambda': 0.0,\n",
      " 'silent': True,\n",
      " 'subsample': 1.0,\n",
      " 'subsample_for_bin': 200000,\n",
      " 'subsample_freq': 0}\n"
     ]
    }
   ],
   "source": [
    "# define model: I just add some default parameters but you could\n",
    "# also just write: RandomForestRegressor() since we will perform a grid search \n",
    "# to find good hyperparameter values\n",
    "model = LGBMRegressor(random_state=0)\n",
    "\n",
    "# Look at parameters used by our current forest\n",
    "print('Parameters currently in use:\\n')\n",
    "pprint(model.get_params())\n",
    "\n",
    "# add to pipeline\n",
    "# 1) min price\n",
    "pipeline_min_p = Pipeline(memory=None,\n",
    "              steps=[('preprocessor', preprocessor),\n",
    "                     ('regressor', model)])\n",
    "\n",
    "\n",
    "# add transformation on the target variable, by default power transformation \n",
    "# also performs standardization after performing the power transformation\n",
    "# and back transform to the original space when outputting predictions \n",
    "\n",
    "# 1) min price\n",
    "transformer_target = PowerTransformer(method='yeo-johnson',standardize=False)\n",
    "scale_target = RobustScaler(with_centering=True, with_scaling=True, quantile_range=(10.0, 90.0))\n",
    "pipeline_y = Pipeline(memory=None,\n",
    "              steps=[('transformer', transformer_target),\n",
    "                     ('scaler',scale_target)])\n",
    "\n",
    "pipeline_min_p_update = TransformedTargetRegressor(regressor=pipeline_min_p, \n",
    "                                         transformer=pipeline_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = [x for x in np.linspace(0.0001, 1, num = 20)]\n",
    "\n",
    "# The number of boosting stages to perform. \n",
    "# Gradient boosting is fairly robust to over-fitting so a large number usually results in better performance.\n",
    "n_estimators = [int(x) for x in np.linspace(start = 500, stop = 3000, num = 20)]\n",
    "\n",
    "\n",
    "# maximum depth of the individual regression estimators\n",
    "max_depth = [int(x) for x in np.linspace(1, 80, num = 20)]\n",
    "\n",
    "num_leaves= [int(x) for x in np.linspace(5, 100, num = 20)]\n",
    "\n",
    "\n",
    "\n",
    "# Create the random grid\n",
    "random_grid = {\n",
    "   'regressor__regressor__n_estimators': n_estimators,\n",
    "   'regressor__regressor__learning_rate': learning_rate,\n",
    "   'regressor__regressor__max_depth': max_depth,\n",
    "    'regressor__regressor__num_leaves':num_leaves\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum weights: 1.0\n",
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:   32.4s\n",
      "[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=-1)]: Done 250 out of 250 | elapsed:  2.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'regressor__regressor__n_estimators': 763, 'regressor__regressor__max_depth': 34, 'regressor__regressor__learning_rate': 0.05272631578947369}\n",
      "-174.36856768528145\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_regressor__regressor__n_estimators</th>\n",
       "      <th>param_regressor__regressor__max_depth</th>\n",
       "      <th>param_regressor__regressor__learning_rate</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1.107021</td>\n",
       "      <td>0.040114</td>\n",
       "      <td>0.056870</td>\n",
       "      <td>0.014300</td>\n",
       "      <td>763</td>\n",
       "      <td>34</td>\n",
       "      <td>0.0527263</td>\n",
       "      <td>{'regressor__regressor__n_estimators': 763, 'r...</td>\n",
       "      <td>-180.278256</td>\n",
       "      <td>-155.574076</td>\n",
       "      <td>-205.285527</td>\n",
       "      <td>-181.881855</td>\n",
       "      <td>-148.823126</td>\n",
       "      <td>-174.368568</td>\n",
       "      <td>20.263439</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2.341467</td>\n",
       "      <td>0.126914</td>\n",
       "      <td>0.097942</td>\n",
       "      <td>0.039946</td>\n",
       "      <td>1157</td>\n",
       "      <td>21</td>\n",
       "      <td>0.105353</td>\n",
       "      <td>{'regressor__regressor__n_estimators': 1157, '...</td>\n",
       "      <td>-178.872383</td>\n",
       "      <td>-163.480214</td>\n",
       "      <td>-208.836780</td>\n",
       "      <td>-200.570298</td>\n",
       "      <td>-147.090291</td>\n",
       "      <td>-179.769993</td>\n",
       "      <td>22.854568</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>3.016376</td>\n",
       "      <td>0.076691</td>\n",
       "      <td>0.108069</td>\n",
       "      <td>0.033652</td>\n",
       "      <td>2210</td>\n",
       "      <td>71</td>\n",
       "      <td>0.0527263</td>\n",
       "      <td>{'regressor__regressor__n_estimators': 2210, '...</td>\n",
       "      <td>-182.637075</td>\n",
       "      <td>-168.793762</td>\n",
       "      <td>-205.558668</td>\n",
       "      <td>-196.643044</td>\n",
       "      <td>-151.982320</td>\n",
       "      <td>-181.122974</td>\n",
       "      <td>19.190912</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>3.502502</td>\n",
       "      <td>0.125171</td>\n",
       "      <td>0.116435</td>\n",
       "      <td>0.034350</td>\n",
       "      <td>1815</td>\n",
       "      <td>63</td>\n",
       "      <td>0.105353</td>\n",
       "      <td>{'regressor__regressor__n_estimators': 1815, '...</td>\n",
       "      <td>-176.099995</td>\n",
       "      <td>-165.612825</td>\n",
       "      <td>-209.390125</td>\n",
       "      <td>-207.091379</td>\n",
       "      <td>-147.751406</td>\n",
       "      <td>-181.189146</td>\n",
       "      <td>23.886501</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>3.628979</td>\n",
       "      <td>0.293593</td>\n",
       "      <td>0.107994</td>\n",
       "      <td>0.042733</td>\n",
       "      <td>1947</td>\n",
       "      <td>46</td>\n",
       "      <td>0.105353</td>\n",
       "      <td>{'regressor__regressor__n_estimators': 1947, '...</td>\n",
       "      <td>-175.833574</td>\n",
       "      <td>-165.638800</td>\n",
       "      <td>-209.390512</td>\n",
       "      <td>-207.854662</td>\n",
       "      <td>-147.905873</td>\n",
       "      <td>-181.324684</td>\n",
       "      <td>24.018937</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "28       1.107021      0.040114         0.056870        0.014300   \n",
       "5        2.341467      0.126914         0.097942        0.039946   \n",
       "32       3.016376      0.076691         0.108069        0.033652   \n",
       "36       3.502502      0.125171         0.116435        0.034350   \n",
       "40       3.628979      0.293593         0.107994        0.042733   \n",
       "\n",
       "   param_regressor__regressor__n_estimators  \\\n",
       "28                                      763   \n",
       "5                                      1157   \n",
       "32                                     2210   \n",
       "36                                     1815   \n",
       "40                                     1947   \n",
       "\n",
       "   param_regressor__regressor__max_depth  \\\n",
       "28                                    34   \n",
       "5                                     21   \n",
       "32                                    71   \n",
       "36                                    63   \n",
       "40                                    46   \n",
       "\n",
       "   param_regressor__regressor__learning_rate  \\\n",
       "28                                 0.0527263   \n",
       "5                                   0.105353   \n",
       "32                                 0.0527263   \n",
       "36                                  0.105353   \n",
       "40                                  0.105353   \n",
       "\n",
       "                                               params  split0_test_score  \\\n",
       "28  {'regressor__regressor__n_estimators': 763, 'r...        -180.278256   \n",
       "5   {'regressor__regressor__n_estimators': 1157, '...        -178.872383   \n",
       "32  {'regressor__regressor__n_estimators': 2210, '...        -182.637075   \n",
       "36  {'regressor__regressor__n_estimators': 1815, '...        -176.099995   \n",
       "40  {'regressor__regressor__n_estimators': 1947, '...        -175.833574   \n",
       "\n",
       "    split1_test_score  split2_test_score  split3_test_score  \\\n",
       "28        -155.574076        -205.285527        -181.881855   \n",
       "5         -163.480214        -208.836780        -200.570298   \n",
       "32        -168.793762        -205.558668        -196.643044   \n",
       "36        -165.612825        -209.390125        -207.091379   \n",
       "40        -165.638800        -209.390512        -207.854662   \n",
       "\n",
       "    split4_test_score  mean_test_score  std_test_score  rank_test_score  \n",
       "28        -148.823126      -174.368568       20.263439                1  \n",
       "5         -147.090291      -179.769993       22.854568                2  \n",
       "32        -151.982320      -181.122974       19.190912                3  \n",
       "36        -147.751406      -181.189146       23.886501                4  \n",
       "40        -147.905873      -181.324684       24.018937                5  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define random search (and narrow down time grid search)\n",
    "\n",
    "if RUN_GRID_SEARCH:\n",
    "    min_p_random_search = RandomizedSearchCV(\n",
    "       estimator = pipeline_min_p_update, \n",
    "       param_distributions = random_grid, n_iter = 50,\n",
    "       cv = 5, verbose=2, random_state=1, n_jobs = -1, refit=True,\n",
    "       scoring=make_scorer(custom_scoring_func_single_p, greater_is_better=False)\n",
    "    )\n",
    "\n",
    "    # run grid search and refit with best hyper parameters\n",
    "    weights_train_min_p =  weights_samples(df=y_train.iloc[:,0], order=0, plot_weights=False)\n",
    "    min_p_random_search.fit(X_train, y_train.iloc[:,0])  \n",
    "    print(min_p_random_search.best_params_)    \n",
    "    print(min_p_random_search.best_score_)\n",
    "\n",
    "\n",
    "# have look at the best hyperparameters and their respective performance (maybe also look at the sd)\n",
    "pd.DataFrame(min_p_random_search.cv_results_).sort_values(\n",
    "        by=['mean_test_score'],ascending=False).head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2) Manual parameters tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum weights: 1.0\n",
      "[Pipeline] ...... (step 1 of 2) Processing preprocessor, total=   0.4s\n",
      "[Pipeline] ......... (step 2 of 2) Processing regressor, total=   0.8s\n",
      "Sum weights: 1.0\n",
      "[Pipeline] ...... (step 1 of 2) Processing preprocessor, total=   0.5s\n",
      "[Pipeline] ......... (step 2 of 2) Processing regressor, total=   0.8s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TransformedTargetRegressor(check_inverse=True, func=None, inverse_func=None,\n",
       "                           regressor=Pipeline(memory=None,\n",
       "                                              steps=[('preprocessor',\n",
       "                                                      ColumnTransformer(n_jobs=None,\n",
       "                                                                        remainder='drop',\n",
       "                                                                        sparse_threshold=0.3,\n",
       "                                                                        transformer_weights=None,\n",
       "                                                                        transformers=[('num',\n",
       "                                                                                       Pipeline(memory=None,\n",
       "                                                                                                steps=[('imputer',\n",
       "                                                                                                        IterativeImputer(add_indicator=False,\n",
       "                                                                                                                         estimator=None,\n",
       "                                                                                                                         imputation_order=...\n",
       "                                                                    reg_alpha=0.0,\n",
       "                                                                    reg_lambda=0.0,\n",
       "                                                                    silent=True,\n",
       "                                                                    subsample=1.0,\n",
       "                                                                    subsample_for_bin=200000,\n",
       "                                                                    subsample_freq=0))],\n",
       "                                              verbose=True),\n",
       "                           transformer=Pipeline(memory=None,\n",
       "                                                steps=[('transformer',\n",
       "                                                        PowerTransformer(copy=True,\n",
       "                                                                         method='yeo-johnson',\n",
       "                                                                         standardize=False)),\n",
       "                                                       ('scaler',\n",
       "                                                        RobustScaler(copy=True,\n",
       "                                                                     quantile_range=(10.0,\n",
       "                                                                                     90.0),\n",
       "                                                                     with_centering=True,\n",
       "                                                                     with_scaling=True))],\n",
       "                                                verbose=False))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train your final model on all data with best parameters \n",
    "\n",
    "# 1) min price\n",
    "model = LGBMRegressor(\n",
    "    n_estimators = 736,\n",
    "    max_depth = 34,\n",
    "    learning_rate = 0.05\n",
    ")\n",
    "\n",
    "\n",
    "# add to pipeline\n",
    "\n",
    "# 1) min price\n",
    "pipeline_min_p = Pipeline(memory=None,\n",
    "              steps=[('preprocessor', preprocessor),\n",
    "                     ('regressor', model)],\n",
    "              verbose=True)\n",
    "\n",
    "# 2) min price\n",
    "pipeline_max_p = Pipeline(memory=None,\n",
    "              steps=[('preprocessor', preprocessor),\n",
    "                     ('regressor', model)],\n",
    "              verbose=True)\n",
    "\n",
    "# again add transformer for target variable\n",
    "\n",
    "# 1) min price\n",
    "pipeline_min_p_update = TransformedTargetRegressor(regressor=pipeline_min_p, \n",
    "                                         transformer=pipeline_y)\n",
    "# 2) max price\n",
    "pipeline_max_p_update = TransformedTargetRegressor(regressor=pipeline_max_p, \n",
    "                                         transformer=pipeline_y)\n",
    "\n",
    "# fit final model on all training data we have at hand\n",
    "\n",
    "# 1) min price\n",
    "weights_train_min_p = weights_samples(y_train.iloc[:,0], order=0)\n",
    "pipeline_min_p_update.fit(X_train, y_train.iloc[:,0])\n",
    "\n",
    "# 2) max price\n",
    "weights_train_max_p = weights_samples(y_train.iloc[:,1], order=0)\n",
    "pipeline_max_p_update.fit(X_train, y_train.iloc[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'minimum price': 54.69972231644909,\n",
       " 'maximum price': 55.226924700567636,\n",
       " 'total error': 109.92664701701673}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# performance on data where the model was fit one (should be very low)\n",
    "pred_train_min_p = pipeline_min_p_update.predict(X_train)\n",
    "pred_train_max_p = pipeline_max_p_update.predict(X_train)\n",
    "\n",
    "# calculate performance\n",
    "pred_train = pd.DataFrame([pred_train_min_p,pred_train_max_p]).T\n",
    "calculate_perf(y_train, pred_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Performance on validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'minimum price': 112.11404573718464,\n",
       " 'maximum price': 114.11000864305959,\n",
       " 'total error': 226.22405438024424}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# performance on validation data\n",
    "pred_val_min_p = pipeline_min_p_update.predict(X_val)\n",
    "pred_val_max_p = pipeline_max_p_update.predict(X_val)\n",
    "\n",
    "# calculate performance \n",
    "pred_val = pd.DataFrame([pred_val_min_p,pred_val_max_p]).T\n",
    "calculate_perf(y_val, pred_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save predictions on validation data\n",
    "# submission format\n",
    "submission_format_validation = pd.DataFrame.from_dict(\n",
    " {'ID':df_val['id'].values,\n",
    " 'MIN':pred_val_min_p,\n",
    " 'MAX':pred_val_max_p}).set_index('ID')\n",
    "\n",
    "# write to csv\n",
    "submission_format_validation.to_csv('code\\\\PieterJan\\\\python\\\\output\\\\validation\\\\light_gbm.csv' ,\n",
    "                            header=True, index=True, sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C) Post processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65f39966c86745d0995a2abdfba4262d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = plt.subplots(2, 2, figsize=(9, 7))\n",
    "axs = np.ravel(axs)\n",
    "\n",
    "# fitted against true predictions minimum price\n",
    "plot_predictions_results(ax=axs[0], \n",
    "                        y_true=y_val.iloc[:,0], \n",
    "                        y_pred=pred_val_min_p, \n",
    "                        title=\"Boosting Min. Price Test Set\", \n",
    "                        log_scale=True)\n",
    "\n",
    "# fitted against true predictions maximum price\n",
    "plot_predictions_results(ax=axs[1], \n",
    "                        y_true=y_val.iloc[:,1], \n",
    "                        y_pred=pred_val_max_p, \n",
    "                        title=\"Boosting Max. Price Test Set\", \n",
    "                        log_scale=True)\n",
    "\n",
    "# residuals minimum price\n",
    "plot_residuals(ax=axs[2], \n",
    "               y_true=y_val.iloc[:,0], \n",
    "               y_pred=pred_val_min_p, \n",
    "               title=\"\", \n",
    "               log_scale=False,\n",
    "               order=1)\n",
    "\n",
    "# residuals maximum price\n",
    "plot_residuals(ax=axs[3], \n",
    "               y_true=y_val.iloc[:,1], \n",
    "               y_pred=pred_val_max_p, \n",
    "               title=\"\", \n",
    "               log_scale=False,\n",
    "               order=1)\n",
    "fig.tight_layout()\n",
    "plt.savefig('code\\\\PieterJan\\\\python\\\\output\\\\figures\\\\light_gbm\\\\fig1.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c058c8fa1786468ebc4c0773bfff51c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(8,4))\n",
    "plt.plot(pred_val_max_p - pred_val_min_p, label=\"Predictions Test\", linestyle=\"--\")\n",
    "plt.plot(y_val.iloc[:,1] - y_val.iloc[:,0], label=\"Truth Test\", linestyle=':')\n",
    "plt.xlabel(\"Observations\")\n",
    "plt.ylabel(\"Max. Price - Min. Price\")\n",
    "plt.legend(loc=\"upper right\");\n",
    "plt.savefig('code\\\\PieterJan\\\\python\\\\output\\\\figures\\\\light_gbm\\\\fig2.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "vip_val_min_p = permutation_importance(pipeline_min_p_update, X_val, y_val.iloc[:,0], n_repeats=10,\n",
    "                                random_state=1, n_jobs=3)\n",
    "\n",
    "vip_val_max_p = permutation_importance(pipeline_min_p_update, X_val, y_val.iloc[:,1], n_repeats=10,\n",
    "                                random_state=1, n_jobs=3)\n",
    "\n",
    "sorted_idx_min_p = vip_val_min_p.importances_mean.argsort()\n",
    "sorted_idx_max_p = vip_val_max_p.importances_mean.argsort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19f75a6cdb9f404a8936a4318b7c7a91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(9, 4))\n",
    "ax = np.ravel(ax)\n",
    "# minimum price\n",
    "ax[0].boxplot(vip_val_min_p.importances[sorted_idx_min_p].T,\n",
    "           vert=False, labels=X_val.columns[sorted_idx_min_p])\n",
    "ax[0].set_title(\"Permutation Importances: Min. Price (Test set)\", fontsize=9)\n",
    "ax[0].xaxis.set_tick_params(labelsize=8)\n",
    "ax[0].yaxis.set_tick_params(labelsize=8)\n",
    "\n",
    "# maximum price\n",
    "ax[1].boxplot(vip_val_min_p.importances[sorted_idx_max_p].T,\n",
    "           vert=False, labels=X_val.columns[sorted_idx_max_p])\n",
    "ax[1].set_title(\"Permutation Importances: Max. Price (Test set)\", fontsize=9)\n",
    "ax[1].xaxis.set_tick_params(labelsize=8)\n",
    "ax[1].yaxis.set_tick_params(labelsize=8)\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "plt.savefig('code\\\\PieterJan\\\\python\\\\output\\\\figures\\\\light_gbm\\\\fig3.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D) Predictions test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum weights: 1.0\n",
      "[Pipeline] ...... (step 1 of 2) Processing preprocessor, total=   0.4s\n",
      "[Pipeline] ......... (step 2 of 2) Processing regressor, total=   1.4s\n",
      "Sum weights: 1.0\n",
      "[Pipeline] ...... (step 1 of 2) Processing preprocessor, total=   0.4s\n",
      "[Pipeline] ......... (step 2 of 2) Processing regressor, total=   1.3s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TransformedTargetRegressor(check_inverse=True, func=None, inverse_func=None,\n",
       "                           regressor=Pipeline(memory=None,\n",
       "                                              steps=[('preprocessor',\n",
       "                                                      ColumnTransformer(n_jobs=None,\n",
       "                                                                        remainder='drop',\n",
       "                                                                        sparse_threshold=0.3,\n",
       "                                                                        transformer_weights=None,\n",
       "                                                                        transformers=[('num',\n",
       "                                                                                       Pipeline(memory=None,\n",
       "                                                                                                steps=[('imputer',\n",
       "                                                                                                        IterativeImputer(add_indicator=False,\n",
       "                                                                                                                         estimator=None,\n",
       "                                                                                                                         imputation_order=...\n",
       "                                                                    reg_alpha=0.0,\n",
       "                                                                    reg_lambda=0.0,\n",
       "                                                                    silent=True,\n",
       "                                                                    subsample=1.0,\n",
       "                                                                    subsample_for_bin=200000,\n",
       "                                                                    subsample_freq=0))],\n",
       "                                              verbose=True),\n",
       "                           transformer=Pipeline(memory=None,\n",
       "                                                steps=[('transformer',\n",
       "                                                        PowerTransformer(copy=True,\n",
       "                                                                         method='yeo-johnson',\n",
       "                                                                         standardize=False)),\n",
       "                                                       ('scaler',\n",
       "                                                        RobustScaler(copy=True,\n",
       "                                                                     quantile_range=(10.0,\n",
       "                                                                                     90.0),\n",
       "                                                                     with_centering=True,\n",
       "                                                                     with_scaling=True))],\n",
       "                                                verbose=False))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train your final model on all data with best parameters \n",
    "\n",
    "\n",
    "model_final = LGBMRegressor(\n",
    "    n_estimators = 736,\n",
    "    max_depth = 34,\n",
    "    learning_rate = 0.05\n",
    ")\n",
    "\n",
    "\n",
    "# add to pipeline\n",
    "\n",
    "# 1) min price\n",
    "pipeline_min_p_final = Pipeline(memory=None,\n",
    "              steps=[('preprocessor', preprocessor),\n",
    "                     ('regressor', model_final)],\n",
    "              verbose=True)\n",
    "\n",
    "# 2) min price\n",
    "pipeline_max_p_final = Pipeline(memory=None,\n",
    "              steps=[('preprocessor', preprocessor),\n",
    "                     ('regressor', model_final)],\n",
    "              verbose=True)\n",
    "\n",
    "# again add transformer for target variable\n",
    "\n",
    "# 1) min price\n",
    "pipeline_min_p_final = TransformedTargetRegressor(regressor=pipeline_min_p_final, \n",
    "                                         transformer=pipeline_y)\n",
    "# 2) max price\n",
    "pipeline_max_p_final = TransformedTargetRegressor(regressor=pipeline_max_p_final, \n",
    "                                         transformer=pipeline_y)\n",
    "\n",
    "# fit final model on all training data we have at hand\n",
    "\n",
    "# 1) min price\n",
    "weights_all_train_min_p = weights_samples(y_all_train.iloc[:,0], order=2)\n",
    "pipeline_min_p_final.fit(X_all_train, y_all_train.iloc[:,0])\n",
    "\n",
    "# 2) max price\n",
    "weights_all_train_max_p = weights_samples(y_all_train.iloc[:,1], order=2)\n",
    "pipeline_max_p_final.fit(X_all_train, y_all_train.iloc[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'minimum price': 42.3977290645379,\n",
       " 'maximum price': 45.68958814185016,\n",
       " 'total error': 88.08731720638806}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# performance on all data where the model was fit one (should be very low)\n",
    "pred_all_train_min_p = pipeline_min_p_final.predict(X_all_train)\n",
    "pred_all_train_max_p = pipeline_max_p_final.predict(X_all_train)\n",
    "\n",
    "# calculate performance\n",
    "pred_all_train = pd.DataFrame([pred_all_train_min_p, pred_all_train_max_p]).T\n",
    "calculate_perf(y_all_train, pred_all_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions on test data\n",
    "pred_test_min_p = pipeline_min_p_final.predict(X_test)\n",
    "pred_test_max_p = pipeline_max_p_final.predict(X_test)\n",
    "\n",
    "# submission format\n",
    "submission_format = pd.DataFrame.from_dict(\n",
    " {'ID':df_test['id'].values,\n",
    " 'MIN':pred_test_min_p,\n",
    " 'MAX':pred_test_max_p}).set_index('ID')\n",
    "\n",
    "# write to csv\n",
    "submission_format.to_csv('code\\\\PieterJan\\\\python\\\\output\\\\submission\\\\light_gbm.csv' ,\n",
    "                            header=True, index=True, sep=',')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
