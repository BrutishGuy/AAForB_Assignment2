{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# auto reload libraries (you do need to re-import libraries if you make changes)\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# base \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "import os\n",
    "\n",
    "# preprocessing \n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.experimental import enable_iterative_imputer \n",
    "from sklearn.impute import SimpleImputer, IterativeImputer,KNNImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, RobustScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "# variable importance\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# models\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "# visualize\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# own defined functions/classes \n",
    "from preprocessing.preprocess_pipe import MakeLowerCase\n",
    "from preprocessing.preprocess_pipe import HdResolutionCategorizer\n",
    "from preprocessing.preprocess_pipe import StorageCategorizer\n",
    "from preprocessing.preprocess_pipe import SsdCategorizer\n",
    "\n",
    "from preprocessing.preprocess_pipe import print_missing\n",
    "from preprocessing.preprocess_pipe import calculate_perf\n",
    "from preprocessing.preprocess_pipe import custom_scoring_func_single_p\n",
    "\n",
    "from modelling.weight_samples import weights_samples\n",
    "\n",
    "from postprocessing.postprocessing import plot_predictions_results\n",
    "from postprocessing.postprocessing import plot_residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Pieter-Jan\\Documents\\KuLeuven\\Semester2\\AA\\AAForB_Assignment2\n"
     ]
    }
   ],
   "source": [
    "# global parameters\n",
    "pd.set_option('display.max_columns', 500)\n",
    "# interactive plotting\n",
    "# %matplotlib widget\n",
    "# run grid search\n",
    "RUN_GRID_SEARCH = True\n",
    "# style for plotting\n",
    "plt.style.use('ggplot')\n",
    "current_palette = sns.color_palette(\"colorblind\")\n",
    "sns.palplot(current_palette)\n",
    "sns.set_palette(\"colorblind\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: import data and split in train and validation set\n",
    "The validation set is more our own kind of test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions of all training data (510, 22)\n",
      "Dimension test data (222, 20)\n"
     ]
    }
   ],
   "source": [
    "# read in trainig and validation data\n",
    "df_all_train = pd.read_csv(\"../../../data/train.csv\", sep=',')\n",
    "df_test = pd.read_csv(\"../../../data/test.csv\", sep=',')\n",
    "\n",
    "print(f'Dimensions of all training data {df_all_train.shape}')\n",
    "print(f'Dimension test data {df_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split in training and validation set\n",
    "df_train = df_all_train.sample(frac=0.75, random_state=0, replace=False)\n",
    "df_val = df_all_train.drop(df_train.index)\n",
    "\n",
    "# reset index, if you don't resit missing rows get inserted in the pipeline\n",
    "# see: https://stackoverflow.com/questions/31323499/sklearn-error-valueerror-input-contains-nan-infinity-or-a-value-too-large-for\n",
    "df_train = df_train.reset_index().drop('index',axis=1)\n",
    "df_val = df_val.reset_index().drop('index',axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: define pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Numerical features to pass down the numerical pipeline \n",
    "numerical_features = ['screen_size' ,'pixels_x','pixels_y',\n",
    "                      'ram', 'weight','ssd','storage']\n",
    "\n",
    "#Categrical features to pass down the categorical pipeline \n",
    "categorical_features = ['brand','base_name', 'screen_surface','touchscreen',\n",
    "                        'cpu','cpu_details','detachable_keyboard',\n",
    "                        'discrete_gpu','gpu', 'os','os_details']\n",
    "\n",
    "# define all unique features\n",
    "features = np.unique(numerical_features).tolist() + np.unique(categorical_features).tolist()\n",
    "\n",
    "# target variables\n",
    "target = ['min_price','max_price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "X_train = df_train[features]\n",
    "y_train = df_train[target]\n",
    "\n",
    "# validation (this is kind our own test set)\n",
    "X_val  = df_val[features]\n",
    "y_val = df_val[target]\n",
    "\n",
    "# train_validation (this is all training data we have) for fitting the model\n",
    "X_all_train = df_all_train[features]\n",
    "y_all_train = df_all_train[target]\n",
    "\n",
    "# test\n",
    "X_test = df_test[features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can add many more and \n",
    "# you can even define custom preprocessing steps like 'MakeLowerCase()'\n",
    "\n",
    "# pipeline  uses only numerical features,\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    #('imputer', SimpleImputer(missing_values=np.nan, strategy='mean')),\n",
    "    #('imputer', KNNImputer(n_neighbors=5, weights='uniform',metric='nan_euclidean')),\n",
    "    ('imputer', IterativeImputer(missing_values=np.nan, random_state=0, n_nearest_features=None, sample_posterior=True)),\n",
    "    #('transformation', PowerTransformer(method='yeo-johnson',standardize=False)),\n",
    "     #Scale features using statistics that are robust to outliers.\n",
    "    ('scaler', RobustScaler(with_centering=True, with_scaling=True, quantile_range=(10.0, 90.0)))]) \n",
    "\n",
    "# pipeline use only categorical features\n",
    "categorical_transformer = Pipeline(steps=[ \n",
    "    ('lowercase', MakeLowerCase()), # lower cases all columns containing strings\n",
    "    #('sd_category' ,SsdCategorizer(drop_original_feature=True)),\n",
    "    #('storage_category', StorageCategorizer(drop_original_feature=True)),\n",
    "    ('imputer', SimpleImputer(missing_values=np.nan, strategy=\"most_frequent\")),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "# add both preprocessing pipelines in one pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numerical_features),\n",
    "        ('cat', categorical_transformer, categorical_features)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(382, 1)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is how the preporcessed pipeline looks like (just to have an idea)\n",
    "pd.DataFrame(preprocessor.fit_transform(X_train)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: add models to pipeline\n",
    "\n",
    "Possible to fit multiple target variabels, so you **don't** need to fit a different models for min. price and max. price\n",
    "\n",
    "### A) Training and parameter tuning\n",
    "\n",
    "##### 1) Automatic tuning via grid search\n",
    "\n",
    "I will only **do the tuning for the minimum price** and use the found parameters also for the maximum price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters currently in use:\n",
      "\n",
      "{'boosting_type': 'gbdt',\n",
      " 'class_weight': None,\n",
      " 'colsample_bytree': 1.0,\n",
      " 'importance_type': 'split',\n",
      " 'learning_rate': 0.1,\n",
      " 'max_depth': -1,\n",
      " 'min_child_samples': 20,\n",
      " 'min_child_weight': 0.001,\n",
      " 'min_split_gain': 0.0,\n",
      " 'n_estimators': 100,\n",
      " 'n_jobs': -1,\n",
      " 'num_leaves': 31,\n",
      " 'objective': None,\n",
      " 'random_state': 0,\n",
      " 'reg_alpha': 0.0,\n",
      " 'reg_lambda': 0.0,\n",
      " 'silent': True,\n",
      " 'subsample': 1.0,\n",
      " 'subsample_for_bin': 200000,\n",
      " 'subsample_freq': 0}\n"
     ]
    }
   ],
   "source": [
    "# define model: I just add some default parameters but you could\n",
    "# also just write: RandomForestRegressor() since we will perform a grid search \n",
    "# to find good hyperparameter values\n",
    "model = LGBMRegressor(random_state=0)\n",
    "\n",
    "# Look at parameters used by our current forest\n",
    "print('Parameters currently in use:\\n')\n",
    "pprint(model.get_params())\n",
    "\n",
    "# add to pipeline\n",
    "# 1) min price\n",
    "pipeline_min_p = Pipeline(memory=None,\n",
    "              steps=[('preprocessor', preprocessor),\n",
    "                     ('regressor', model)])\n",
    "\n",
    "\n",
    "# add transformation on the target variable, by default power transformation \n",
    "# also performs standardization after performing the power transformation\n",
    "# and back transform to the original space when outputting predictions \n",
    "\n",
    "# 1) min price\n",
    "transformer_target = PowerTransformer(method='yeo-johnson',standardize=False)\n",
    "scale_target = RobustScaler(with_centering=True, with_scaling=True, quantile_range=(10.0, 90.0))\n",
    "pipeline_y = Pipeline(memory=None,\n",
    "              steps=[('transformer', transformer_target),\n",
    "                     ('scaler',scale_target)])\n",
    "\n",
    "pipeline_min_p_update = TransformedTargetRegressor(regressor=pipeline_min_p, \n",
    "                                         transformer=pipeline_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# boosting_type : string, optional (default='gbdt')\n",
    "boosting_type = ['gbdt', 'dart', 'goss']\n",
    "\n",
    "learning_rate = [x for x in np.linspace(0.005, .25, num = 20)]\n",
    "\n",
    "# The number of boosting stages to perform. \n",
    "# Gradient boosting is fairly robust to over-fitting so a large number usually results in better performance.\n",
    "n_estimators = [int(x) for x in np.linspace(start = 500, stop = 3000, num = 20)]\n",
    "\n",
    "# Number of samples for constructing bins.\n",
    "subsample_for_bin = [int(x) for x in np.linspace(20, 500, num = 20)]\n",
    "\n",
    "min_split_gain = [int(x) for x in np.linspace(0, .5, num = 20)]\n",
    "\n",
    "min_child_samples = [int(x) for x in np.linspace(5, 50, num = 20)]\n",
    "\n",
    "min_child_weight = [int(x) for x in np.linspace(0.001, 0.1, num = 20)]\n",
    "\n",
    "colsample_bytree = [int(x) for x in np.linspace(0.1, 1, num = 20)]\n",
    "\n",
    "# maximum depth of the individual regression estimators\n",
    "max_depth = [int(x) for x in np.linspace(2, 50, num = 10)]\n",
    "\n",
    "num_leaves= [int(x) for x in np.linspace(5, 100, num = 10)]\n",
    "# L1 regul\n",
    "reg_alpha = [int(x) for x in np.linspace(0, .1, num = 10)]\n",
    "\n",
    "reg_lambda  = [int(x) for x in np.linspace(0, .1, num = 10)]\n",
    "\n",
    "# Create the random grid\n",
    "random_grid = {\n",
    "   'regressor__regressor__boosting_type': boosting_type,\n",
    "   'regressor__regressor__n_estimators': n_estimators,\n",
    "   'regressor__regressor__learning_rate': learning_rate,\n",
    "  # 'regressor__regressor__subsample_for_bin': subsample_for_bin,\n",
    "   'regressor__regressor__min_child_samples': min_child_samples,\n",
    "   'regressor__regressor__colsample_bytree': colsample_bytree,\n",
    "   'regressor__regressor__min_split_gain': min_split_gain,\n",
    "   # 'regressor__regressor__min_child_weight': min_child_weight,\n",
    "   #'regressor__regressor__reg_alpha': reg_alpha,\n",
    "   #'regressor__regressor__reg_lambda': reg_lambda,\n",
    "   'regressor__regressor__max_depth': max_depth,\n",
    "   'regressor__regressor__num_leaves':num_leaves\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum weights: 1.0\n",
      "Fitting 5 folds for each of 1000 candidates, totalling 5000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:   14.1s\n",
      "[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed:   42.1s\n",
      "[Parallel(n_jobs=-1)]: Done 357 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 640 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=-1)]: Done 1005 tasks      | elapsed:  2.7min\n",
      "[Parallel(n_jobs=-1)]: Done 1450 tasks      | elapsed:  3.6min\n",
      "[Parallel(n_jobs=-1)]: Done 1977 tasks      | elapsed:  4.3min\n",
      "[Parallel(n_jobs=-1)]: Done 2584 tasks      | elapsed:  5.1min\n",
      "[Parallel(n_jobs=-1)]: Done 3273 tasks      | elapsed:  6.1min\n",
      "[Parallel(n_jobs=-1)]: Done 4042 tasks      | elapsed:  7.1min\n",
      "[Parallel(n_jobs=-1)]: Done 4893 tasks      | elapsed:  8.3min\n",
      "[Parallel(n_jobs=-1)]: Done 5000 out of 5000 | elapsed:  8.4min finished\n",
      "C:\\Users\\Pieter-Jan\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:788: RuntimeWarning: invalid value encountered in subtract\n",
      "  array_means[:, np.newaxis]) ** 2,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'regressor__regressor__num_leaves': 26, 'regressor__regressor__n_estimators': 1157, 'regressor__regressor__min_split_gain': 0, 'regressor__regressor__min_child_samples': 14, 'regressor__regressor__max_depth': 12, 'regressor__regressor__learning_rate': 0.017894736842105262, 'regressor__regressor__colsample_bytree': 1, 'regressor__regressor__boosting_type': 'gbdt'}\n",
      "-150.14222780519\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_regressor__regressor__num_leaves</th>\n",
       "      <th>param_regressor__regressor__n_estimators</th>\n",
       "      <th>param_regressor__regressor__min_split_gain</th>\n",
       "      <th>param_regressor__regressor__min_child_samples</th>\n",
       "      <th>param_regressor__regressor__max_depth</th>\n",
       "      <th>param_regressor__regressor__learning_rate</th>\n",
       "      <th>param_regressor__regressor__colsample_bytree</th>\n",
       "      <th>param_regressor__regressor__boosting_type</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>1.116219</td>\n",
       "      <td>0.032716</td>\n",
       "      <td>0.045680</td>\n",
       "      <td>0.006597</td>\n",
       "      <td>26</td>\n",
       "      <td>1157</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>12</td>\n",
       "      <td>0.0178947</td>\n",
       "      <td>1</td>\n",
       "      <td>gbdt</td>\n",
       "      <td>{'regressor__regressor__num_leaves': 26, 'regr...</td>\n",
       "      <td>-158.897127</td>\n",
       "      <td>-158.566219</td>\n",
       "      <td>-132.060030</td>\n",
       "      <td>-143.539602</td>\n",
       "      <td>-157.648162</td>\n",
       "      <td>-150.142228</td>\n",
       "      <td>10.719249</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>2.219085</td>\n",
       "      <td>0.035945</td>\n",
       "      <td>0.042089</td>\n",
       "      <td>0.007007</td>\n",
       "      <td>36</td>\n",
       "      <td>763</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>12</td>\n",
       "      <td>0.172632</td>\n",
       "      <td>1</td>\n",
       "      <td>dart</td>\n",
       "      <td>{'regressor__regressor__num_leaves': 36, 'regr...</td>\n",
       "      <td>-158.877406</td>\n",
       "      <td>-162.753068</td>\n",
       "      <td>-135.014015</td>\n",
       "      <td>-154.441036</td>\n",
       "      <td>-153.392415</td>\n",
       "      <td>-152.895588</td>\n",
       "      <td>9.542934</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>1.127986</td>\n",
       "      <td>0.007192</td>\n",
       "      <td>0.031716</td>\n",
       "      <td>0.005997</td>\n",
       "      <td>100</td>\n",
       "      <td>631</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>39</td>\n",
       "      <td>0.0952632</td>\n",
       "      <td>1</td>\n",
       "      <td>dart</td>\n",
       "      <td>{'regressor__regressor__num_leaves': 100, 'reg...</td>\n",
       "      <td>-169.692399</td>\n",
       "      <td>-151.112689</td>\n",
       "      <td>-134.838963</td>\n",
       "      <td>-147.767617</td>\n",
       "      <td>-161.466107</td>\n",
       "      <td>-152.975555</td>\n",
       "      <td>11.925361</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>365</th>\n",
       "      <td>0.717279</td>\n",
       "      <td>0.014710</td>\n",
       "      <td>0.034708</td>\n",
       "      <td>0.006382</td>\n",
       "      <td>78</td>\n",
       "      <td>500</td>\n",
       "      <td>0</td>\n",
       "      <td>42</td>\n",
       "      <td>23</td>\n",
       "      <td>0.159737</td>\n",
       "      <td>1</td>\n",
       "      <td>dart</td>\n",
       "      <td>{'regressor__regressor__num_leaves': 78, 'regr...</td>\n",
       "      <td>-160.188322</td>\n",
       "      <td>-151.662950</td>\n",
       "      <td>-142.012890</td>\n",
       "      <td>-150.918884</td>\n",
       "      <td>-160.153380</td>\n",
       "      <td>-152.987285</td>\n",
       "      <td>6.777571</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>6.811346</td>\n",
       "      <td>0.104205</td>\n",
       "      <td>0.078193</td>\n",
       "      <td>0.009368</td>\n",
       "      <td>47</td>\n",
       "      <td>2078</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>12</td>\n",
       "      <td>0.0436842</td>\n",
       "      <td>1</td>\n",
       "      <td>dart</td>\n",
       "      <td>{'regressor__regressor__num_leaves': 47, 'regr...</td>\n",
       "      <td>-162.850340</td>\n",
       "      <td>-163.166604</td>\n",
       "      <td>-132.112735</td>\n",
       "      <td>-150.492896</td>\n",
       "      <td>-158.235569</td>\n",
       "      <td>-153.371629</td>\n",
       "      <td>11.573012</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "146       1.116219      0.032716         0.045680        0.006597   \n",
       "153       2.219085      0.035945         0.042089        0.007007   \n",
       "107       1.127986      0.007192         0.031716        0.005997   \n",
       "365       0.717279      0.014710         0.034708        0.006382   \n",
       "160       6.811346      0.104205         0.078193        0.009368   \n",
       "\n",
       "    param_regressor__regressor__num_leaves  \\\n",
       "146                                     26   \n",
       "153                                     36   \n",
       "107                                    100   \n",
       "365                                     78   \n",
       "160                                     47   \n",
       "\n",
       "    param_regressor__regressor__n_estimators  \\\n",
       "146                                     1157   \n",
       "153                                      763   \n",
       "107                                      631   \n",
       "365                                      500   \n",
       "160                                     2078   \n",
       "\n",
       "    param_regressor__regressor__min_split_gain  \\\n",
       "146                                          0   \n",
       "153                                          0   \n",
       "107                                          0   \n",
       "365                                          0   \n",
       "160                                          0   \n",
       "\n",
       "    param_regressor__regressor__min_child_samples  \\\n",
       "146                                            14   \n",
       "153                                            14   \n",
       "107                                            23   \n",
       "365                                            42   \n",
       "160                                            14   \n",
       "\n",
       "    param_regressor__regressor__max_depth  \\\n",
       "146                                    12   \n",
       "153                                    12   \n",
       "107                                    39   \n",
       "365                                    23   \n",
       "160                                    12   \n",
       "\n",
       "    param_regressor__regressor__learning_rate  \\\n",
       "146                                 0.0178947   \n",
       "153                                  0.172632   \n",
       "107                                 0.0952632   \n",
       "365                                  0.159737   \n",
       "160                                 0.0436842   \n",
       "\n",
       "    param_regressor__regressor__colsample_bytree  \\\n",
       "146                                            1   \n",
       "153                                            1   \n",
       "107                                            1   \n",
       "365                                            1   \n",
       "160                                            1   \n",
       "\n",
       "    param_regressor__regressor__boosting_type  \\\n",
       "146                                      gbdt   \n",
       "153                                      dart   \n",
       "107                                      dart   \n",
       "365                                      dart   \n",
       "160                                      dart   \n",
       "\n",
       "                                                params  split0_test_score  \\\n",
       "146  {'regressor__regressor__num_leaves': 26, 'regr...        -158.897127   \n",
       "153  {'regressor__regressor__num_leaves': 36, 'regr...        -158.877406   \n",
       "107  {'regressor__regressor__num_leaves': 100, 'reg...        -169.692399   \n",
       "365  {'regressor__regressor__num_leaves': 78, 'regr...        -160.188322   \n",
       "160  {'regressor__regressor__num_leaves': 47, 'regr...        -162.850340   \n",
       "\n",
       "     split1_test_score  split2_test_score  split3_test_score  \\\n",
       "146        -158.566219        -132.060030        -143.539602   \n",
       "153        -162.753068        -135.014015        -154.441036   \n",
       "107        -151.112689        -134.838963        -147.767617   \n",
       "365        -151.662950        -142.012890        -150.918884   \n",
       "160        -163.166604        -132.112735        -150.492896   \n",
       "\n",
       "     split4_test_score  mean_test_score  std_test_score  rank_test_score  \n",
       "146        -157.648162      -150.142228       10.719249                1  \n",
       "153        -153.392415      -152.895588        9.542934                2  \n",
       "107        -161.466107      -152.975555       11.925361                3  \n",
       "365        -160.153380      -152.987285        6.777571                4  \n",
       "160        -158.235569      -153.371629       11.573012                5  "
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define random search (and narrow down time grid search)\n",
    "\n",
    "if RUN_GRID_SEARCH:\n",
    "    min_p_random_search = RandomizedSearchCV(\n",
    "       estimator = pipeline_min_p_update, \n",
    "       param_distributions = random_grid, n_iter = 1000,\n",
    "       cv = 5, verbose=2, random_state=1, n_jobs = -1, refit=True,\n",
    "       scoring=make_scorer(custom_scoring_func_single_p, greater_is_better=False)\n",
    "    )\n",
    "\n",
    "    # run grid search and refit with best hyper parameters\n",
    "    weights_train_min_p =  weights_samples(df=y_train.iloc[:,0], order=0, plot_weights=False)\n",
    "    min_p_random_search.fit(X_train, y_train.iloc[:,0])  \n",
    "    print(min_p_random_search.best_params_)    \n",
    "    print(min_p_random_search.best_score_)\n",
    "\n",
    "\n",
    "# have look at the best hyperparameters and their respective performance (maybe also look at the sd)\n",
    "pd.DataFrame(min_p_random_search.cv_results_).sort_values(\n",
    "        by=['mean_test_score'],ascending=False).head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2) Manual parameters tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum weights: 1.0\n",
      "[Pipeline] ...... (step 1 of 2) Processing preprocessor, total=   0.1s\n",
      "[Pipeline] ......... (step 2 of 2) Processing regressor, total=   0.8s\n",
      "Sum weights: 1.0\n",
      "[Pipeline] ...... (step 1 of 2) Processing preprocessor, total=   0.2s\n",
      "[Pipeline] ......... (step 2 of 2) Processing regressor, total=   0.9s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TransformedTargetRegressor(check_inverse=True, func=None, inverse_func=None,\n",
       "                           regressor=Pipeline(memory=None,\n",
       "                                              steps=[('preprocessor',\n",
       "                                                      ColumnTransformer(n_jobs=None,\n",
       "                                                                        remainder='drop',\n",
       "                                                                        sparse_threshold=0.3,\n",
       "                                                                        transformer_weights=None,\n",
       "                                                                        transformers=[('num',\n",
       "                                                                                       Pipeline(memory=None,\n",
       "                                                                                                steps=[('imputer',\n",
       "                                                                                                        IterativeImputer(add_indicator=False,\n",
       "                                                                                                                         estimator=None,\n",
       "                                                                                                                         imputation_order=...\n",
       "                                                                    reg_alpha=0.0,\n",
       "                                                                    reg_lambda=0.0,\n",
       "                                                                    silent=True,\n",
       "                                                                    subsample=1.0,\n",
       "                                                                    subsample_for_bin=200000,\n",
       "                                                                    subsample_freq=0))],\n",
       "                                              verbose=True),\n",
       "                           transformer=Pipeline(memory=None,\n",
       "                                                steps=[('transformer',\n",
       "                                                        PowerTransformer(copy=True,\n",
       "                                                                         method='yeo-johnson',\n",
       "                                                                         standardize=False)),\n",
       "                                                       ('scaler',\n",
       "                                                        RobustScaler(copy=True,\n",
       "                                                                     quantile_range=(10.0,\n",
       "                                                                                     90.0),\n",
       "                                                                     with_centering=True,\n",
       "                                                                     with_scaling=True))],\n",
       "                                                verbose=False))"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train your final model on all data with best parameters \n",
    "\n",
    "# 1) min price\n",
    "model = LGBMRegressor(\n",
    "    boosting_type='gbdt',\n",
    "    n_estimators=1157,\n",
    "    learning_rate =0.0178947,\n",
    "    min_child_samples=14,\n",
    "    min_split_gain=0,\n",
    "    colsample_bytree=1,\n",
    "    max_depth=12,\n",
    "    num_leaves = 26,\n",
    "    random_state=0\n",
    "    \n",
    ")\n",
    "\n",
    "\n",
    "# add to pipeline\n",
    "\n",
    "# 1) min price\n",
    "pipeline_min_p = Pipeline(memory=None,\n",
    "              steps=[('preprocessor', preprocessor),\n",
    "                     ('regressor', model)],\n",
    "              verbose=True)\n",
    "\n",
    "# 2) min price\n",
    "pipeline_max_p = Pipeline(memory=None,\n",
    "              steps=[('preprocessor', preprocessor),\n",
    "                     ('regressor', model)],\n",
    "              verbose=True)\n",
    "\n",
    "# again add transformer for target variable\n",
    "\n",
    "# 1) min price\n",
    "pipeline_min_p_update = TransformedTargetRegressor(regressor=pipeline_min_p, \n",
    "                                         transformer=pipeline_y)\n",
    "# 2) max price\n",
    "pipeline_max_p_update = TransformedTargetRegressor(regressor=pipeline_max_p, \n",
    "                                         transformer=pipeline_y)\n",
    "\n",
    "# fit final model on all training data we have at hand\n",
    "\n",
    "# 1) min price\n",
    "weights_train_min_p = weights_samples(y_train.iloc[:,0], order=0)\n",
    "pipeline_min_p_update.fit(X_train, y_train.iloc[:,0])\n",
    "\n",
    "# 2) max price\n",
    "weights_train_max_p = weights_samples(y_train.iloc[:,1], order=0)\n",
    "pipeline_max_p_update.fit(X_train, y_train.iloc[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'minimum price': 57.43002055563661,\n",
       " 'maximum price': 56.296107171417844,\n",
       " 'total error': 113.72612772705446}"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# performance on data where the model was fit one (should be very low)\n",
    "pred_train_min_p = pipeline_min_p_update.predict(X_train)\n",
    "pred_train_max_p = pipeline_max_p_update.predict(X_train)\n",
    "\n",
    "# calculate performance\n",
    "pred_train = pd.DataFrame([pred_train_min_p,pred_train_max_p]).T\n",
    "calculate_perf(y_train, pred_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Performance on validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'minimum price': 117.34538654789144,\n",
       " 'maximum price': 116.9530964073627,\n",
       " 'total error': 234.29848295525414}"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# performance on validation data\n",
    "pred_val_min_p = pipeline_min_p_update.predict(X_val)\n",
    "pred_val_max_p = pipeline_max_p_update.predict(X_val)\n",
    "\n",
    "# calculate performance \n",
    "pred_val = pd.DataFrame([pred_val_min_p,pred_val_max_p]).T\n",
    "calculate_perf(y_val, pred_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save predictions on validation data\n",
    "# submission format\n",
    "submission_format_validation = pd.DataFrame.from_dict(\n",
    " {'ID':df_val['id'].values,\n",
    " 'MIN':pred_val_min_p,\n",
    " 'MAX':pred_val_max_p}).set_index('ID')\n",
    "\n",
    "# write to csv\n",
    "submission_format_validation.to_csv('output/validation/light_gbm.csv' ,\n",
    "                            header=True, index=True, sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C) Post processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc1392c394e94eef87e633636e04955d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = plt.subplots(2, 2, figsize=(9, 7))\n",
    "axs = np.ravel(axs)\n",
    "\n",
    "# fitted against true predictions minimum price\n",
    "plot_predictions_results(ax=axs[0], \n",
    "                        y_true=y_val.iloc[:,0], \n",
    "                        y_pred=pred_val_min_p, \n",
    "                        title=\"Boosting Min. Price Test Set\", \n",
    "                        log_scale=True)\n",
    "\n",
    "# fitted against true predictions maximum price\n",
    "plot_predictions_results(ax=axs[1], \n",
    "                        y_true=y_val.iloc[:,1], \n",
    "                        y_pred=pred_val_max_p, \n",
    "                        title=\"Boosting Max. Price Test Set\", \n",
    "                        log_scale=True)\n",
    "\n",
    "# residuals minimum price\n",
    "plot_residuals(ax=axs[2], \n",
    "               y_true=y_val.iloc[:,0], \n",
    "               y_pred=pred_val_min_p, \n",
    "               title=\"\", \n",
    "               log_scale=False,\n",
    "               order=1)\n",
    "\n",
    "# residuals maximum price\n",
    "plot_residuals(ax=axs[3], \n",
    "               y_true=y_val.iloc[:,1], \n",
    "               y_pred=pred_val_max_p, \n",
    "               title=\"\", \n",
    "               log_scale=False,\n",
    "               order=1)\n",
    "fig.tight_layout()\n",
    "plt.savefig('output/figures/light_gbm/fig1.png', dpi=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b94addeebe3c4728925c9b07b805b367",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(8,4))\n",
    "plt.plot(pred_val_max_p - pred_val_min_p, label=\"Predictions Test\", linestyle=\"--\")\n",
    "plt.plot(y_val.iloc[:,1] - y_val.iloc[:,0], label=\"Truth Test\", linestyle=':')\n",
    "plt.xlabel(\"Observations\")\n",
    "plt.ylabel(\"Max. Price - Min. Price\")\n",
    "plt.legend(loc=\"upper right\");\n",
    "plt.savefig('output/figures/light_gbm/fig2.png', dpi=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "vip_val_min_p = permutation_importance(pipeline_min_p_update, X_val, y_val.iloc[:,0], n_repeats=10,\n",
    "                                random_state=1, n_jobs=3)\n",
    "\n",
    "vip_val_max_p = permutation_importance(pipeline_min_p_update, X_val, y_val.iloc[:,1], n_repeats=10,\n",
    "                                random_state=1, n_jobs=3)\n",
    "\n",
    "sorted_idx_min_p = vip_val_min_p.importances_mean.argsort()\n",
    "sorted_idx_max_p = vip_val_max_p.importances_mean.argsort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4dabc3c957f44c9c99eba67af71fdcd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(12, 4))\n",
    "ax = np.ravel(ax)\n",
    "# minimum price\n",
    "ax[0].boxplot(vip_val_min_p.importances[sorted_idx_min_p].T,\n",
    "           vert=False, labels=X_val.columns[sorted_idx_min_p])\n",
    "ax[0].set_title(\"Permutation Importances: Min. Price (Test set)\", fontsize=9)\n",
    "ax[0].xaxis.set_tick_params(labelsize=8)\n",
    "ax[0].yaxis.set_tick_params(labelsize=8)\n",
    "\n",
    "# maximum price\n",
    "ax[1].boxplot(vip_val_min_p.importances[sorted_idx_max_p].T,\n",
    "           vert=False, labels=X_val.columns[sorted_idx_max_p])\n",
    "ax[1].set_title(\"Permutation Importances: Max. Price (Test set)\", fontsize=9)\n",
    "ax[1].xaxis.set_tick_params(labelsize=8)\n",
    "ax[1].yaxis.set_tick_params(labelsize=8)\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "plt.savefig('output/figures/light_gbm/fig3.png', dpi=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D) Predictions test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum weights: 1.0\n",
      "[Pipeline] ...... (step 1 of 2) Processing preprocessor, total=   0.1s\n",
      "[Pipeline] ......... (step 2 of 2) Processing regressor, total=   1.0s\n",
      "Sum weights: 1.0\n",
      "[Pipeline] ...... (step 1 of 2) Processing preprocessor, total=   0.2s\n",
      "[Pipeline] ......... (step 2 of 2) Processing regressor, total=   0.9s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TransformedTargetRegressor(check_inverse=True, func=None, inverse_func=None,\n",
       "                           regressor=Pipeline(memory=None,\n",
       "                                              steps=[('preprocessor',\n",
       "                                                      ColumnTransformer(n_jobs=None,\n",
       "                                                                        remainder='drop',\n",
       "                                                                        sparse_threshold=0.3,\n",
       "                                                                        transformer_weights=None,\n",
       "                                                                        transformers=[('num',\n",
       "                                                                                       Pipeline(memory=None,\n",
       "                                                                                                steps=[('imputer',\n",
       "                                                                                                        IterativeImputer(add_indicator=False,\n",
       "                                                                                                                         estimator=None,\n",
       "                                                                                                                         imputation_order=...\n",
       "                                                                    reg_alpha=0.0,\n",
       "                                                                    reg_lambda=0.0,\n",
       "                                                                    silent=True,\n",
       "                                                                    subsample=1.0,\n",
       "                                                                    subsample_for_bin=200000,\n",
       "                                                                    subsample_freq=0))],\n",
       "                                              verbose=True),\n",
       "                           transformer=Pipeline(memory=None,\n",
       "                                                steps=[('transformer',\n",
       "                                                        PowerTransformer(copy=True,\n",
       "                                                                         method='yeo-johnson',\n",
       "                                                                         standardize=False)),\n",
       "                                                       ('scaler',\n",
       "                                                        RobustScaler(copy=True,\n",
       "                                                                     quantile_range=(10.0,\n",
       "                                                                                     90.0),\n",
       "                                                                     with_centering=True,\n",
       "                                                                     with_scaling=True))],\n",
       "                                                verbose=False))"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train your final model on all data with best parameters \n",
    "\n",
    "\n",
    "model_final = LGBMRegressor(\n",
    "    boosting_type='gbdt',\n",
    "    n_estimators=1157,\n",
    "    learning_rate =0.0178947,\n",
    "    min_child_samples=14,\n",
    "    min_split_gain=0,\n",
    "    colsample_bytree=1,\n",
    "    max_depth=12,\n",
    "    num_leaves = 26,\n",
    "    random_state=0\n",
    ")\n",
    "\n",
    "\n",
    "# add to pipeline\n",
    "\n",
    "# 1) min price\n",
    "pipeline_min_p_final = Pipeline(memory=None,\n",
    "              steps=[('preprocessor', preprocessor),\n",
    "                     ('regressor', model_final)],\n",
    "              verbose=True)\n",
    "\n",
    "# 2) min price\n",
    "pipeline_max_p_final = Pipeline(memory=None,\n",
    "              steps=[('preprocessor', preprocessor),\n",
    "                     ('regressor', model_final)],\n",
    "              verbose=True)\n",
    "\n",
    "# again add transformer for target variable\n",
    "\n",
    "# 1) min price\n",
    "pipeline_min_p_final = TransformedTargetRegressor(regressor=pipeline_min_p_final, \n",
    "                                         transformer=pipeline_y)\n",
    "# 2) max price\n",
    "pipeline_max_p_final = TransformedTargetRegressor(regressor=pipeline_max_p_final, \n",
    "                                         transformer=pipeline_y)\n",
    "\n",
    "# fit final model on all training data we have at hand\n",
    "\n",
    "# 1) min price\n",
    "weights_all_train_min_p = weights_samples(y_all_train.iloc[:,0], order=2)\n",
    "pipeline_min_p_final.fit(X_all_train, y_all_train.iloc[:,0])\n",
    "\n",
    "# 2) max price\n",
    "weights_all_train_max_p = weights_samples(y_all_train.iloc[:,1], order=2)\n",
    "pipeline_max_p_final.fit(X_all_train, y_all_train.iloc[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'minimum price': 48.42264183468457,\n",
       " 'maximum price': 49.526094510301775,\n",
       " 'total error': 97.94873634498634}"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# performance on all data where the model was fit one (should be very low)\n",
    "pred_all_train_min_p = pipeline_min_p_final.predict(X_all_train)\n",
    "pred_all_train_max_p = pipeline_max_p_final.predict(X_all_train)\n",
    "\n",
    "# calculate performance\n",
    "pred_all_train = pd.DataFrame([pred_all_train_min_p, pred_all_train_max_p]).T\n",
    "calculate_perf(y_all_train, pred_all_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions on test data\n",
    "pred_test_min_p = pipeline_min_p_final.predict(X_test)\n",
    "pred_test_max_p = pipeline_max_p_final.predict(X_test)\n",
    "\n",
    "# submission format\n",
    "submission_format = pd.DataFrame.from_dict(\n",
    " {'ID':df_test['id'].values,\n",
    " 'MIN':pred_test_min_p,\n",
    " 'MAX':pred_test_max_p}).set_index('ID')\n",
    "\n",
    "# write to csv\n",
    "submission_format.to_csv('output/submission/light_gbm.csv' ,\n",
    "                            header=True, index=True, sep=',')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
