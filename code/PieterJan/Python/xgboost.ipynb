{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# auto reload libraries (you do need to re-import libraries if you make changes)\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# base \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "import os\n",
    "\n",
    "# preprocessing \n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.experimental import enable_iterative_imputer \n",
    "from sklearn.impute import SimpleImputer, IterativeImputer,KNNImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, RobustScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "# variable importance\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# models\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# visualize\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# own defined functions/classes \n",
    "from preprocessing.preprocess_pipe import MakeLowerCase\n",
    "from preprocessing.preprocess_pipe import HdResolutionCategorizer\n",
    "from preprocessing.preprocess_pipe import StorageCategorizer\n",
    "from preprocessing.preprocess_pipe import SsdCategorizer\n",
    "\n",
    "from preprocessing.preprocess_pipe import print_missing\n",
    "from preprocessing.preprocess_pipe import calculate_perf\n",
    "from preprocessing.preprocess_pipe import custom_scoring_func_single_p\n",
    "\n",
    "from modelling.weight_samples import weights_samples\n",
    "\n",
    "from postprocessing.postprocessing import plot_predictions_results\n",
    "from postprocessing.postprocessing import plot_residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Pieter-Jan\\Documents\\KuLeuven\\Semester2\\AA\\AAForB_Assignment2\n"
     ]
    }
   ],
   "source": [
    "# global parameters\n",
    "pd.set_option('display.max_columns', 500)\n",
    "# style for plotting\n",
    "# interactive plotting\n",
    "# %matplotlib widget\n",
    "# run grid search\n",
    "RUN_GRID_SEARCH = True\n",
    "# style for plotting\n",
    "plt.style.use('ggplot')\n",
    "current_palette = sns.color_palette(\"colorblind\")\n",
    "sns.palplot(current_palette)\n",
    "sns.set_palette(\"colorblind\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: import data and split in train and validation set\n",
    "The validation set is more our own kind of test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions of all training data (510, 22)\n",
      "Dimension test data (222, 20)\n"
     ]
    }
   ],
   "source": [
    "# read in trainig and validation data\n",
    "df_all_train = pd.read_csv(\"../../../data/train.csv\", sep=',')\n",
    "df_test = pd.read_csv(\"../../../data/test.csv\", sep=',')\n",
    "\n",
    "print(f'Dimensions of all training data {df_all_train.shape}')\n",
    "print(f'Dimension test data {df_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split in training and validation set\n",
    "df_train = df_all_train.sample(frac=0.75, random_state=0, replace=False)\n",
    "df_val = df_all_train.drop(df_train.index)\n",
    "\n",
    "# reset index, if you don't resit missing rows get inserted in the pipeline\n",
    "# see: https://stackoverflow.com/questions/31323499/sklearn-error-valueerror-input-contains-nan-infinity-or-a-value-too-large-for\n",
    "df_train = df_train.reset_index().drop('index',axis=1)\n",
    "df_val = df_val.reset_index().drop('index',axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: define pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Numerical features to pass down the numerical pipeline \n",
    "numerical_features = ['screen_size' ,'pixels_x','pixels_y',\n",
    "                      'ram', 'weight','ssd','storage']\n",
    "\n",
    "#Categrical features to pass down the categorical pipeline \n",
    "categorical_features = ['brand','base_name', 'screen_surface','touchscreen',\n",
    "                        'cpu','cpu_details','detachable_keyboard',\n",
    "                        'discrete_gpu','gpu', 'os','os_details']\n",
    "\n",
    "# define all unique features\n",
    "features = np.unique(numerical_features).tolist() + np.unique(categorical_features).tolist()\n",
    "\n",
    "# target variables\n",
    "target = ['min_price','max_price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "X_train = df_train[features]\n",
    "y_train = df_train[target]\n",
    "\n",
    "# validation (this is kind our own test set)\n",
    "X_val  = df_val[features]\n",
    "y_val = df_val[target]\n",
    "\n",
    "# train_validation (this is all training data we have) for fitting the model\n",
    "X_all_train = df_all_train[features]\n",
    "y_all_train = df_all_train[target]\n",
    "\n",
    "# test\n",
    "X_test = df_test[features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can add many more and \n",
    "# you can even define custom preprocessing steps like 'MakeLowerCase()'\n",
    "\n",
    "# pipeline  uses only numerical features,\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    #('imputer', SimpleImputer(missing_values=np.nan, strategy='mean')),\n",
    "    #('imputer', KNNImputer(n_neighbors=5, weights='uniform',metric='nan_euclidean')),\n",
    "    ('imputer', IterativeImputer(missing_values=np.nan, random_state=0, n_nearest_features=None, sample_posterior=True)),\n",
    "    #('transformation', PowerTransformer(method='yeo-johnson',standardize=False)),\n",
    "     #Scale features using statistics that are robust to outliers.\n",
    "    ('scaler', RobustScaler(with_centering=True, with_scaling=True, quantile_range=(10.0, 90.0)))]) \n",
    "\n",
    "# pipeline use only categorical features\n",
    "categorical_transformer = Pipeline(steps=[ \n",
    "    ('lowercase', MakeLowerCase()), # lower cases all columns containing strings\n",
    "    #('sd_category' ,SsdCategorizer(drop_original_feature=True)),\n",
    "    #('storage_category', StorageCategorizer(drop_original_feature=True)),\n",
    "    ('imputer', SimpleImputer(missing_values=np.nan, strategy=\"most_frequent\")),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "# add both preprocessing pipelines in one pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numerical_features),\n",
    "        ('cat', categorical_transformer, categorical_features)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(382, 1)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is how the preporcessed pipeline looks like (just to have an idea)\n",
    "pd.DataFrame(preprocessor.fit_transform(X_train)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: add models to pipeline\n",
    "\n",
    "Possible to fit multiple target variabels, so you **don't** need to fit a different models for min. price and max. price\n",
    "\n",
    "### A) Training and parameter tuning\n",
    "\n",
    "##### 1) Automatic tuning via grid search\n",
    "\n",
    "I will only **do the tuning for the minimum price** and use the found parameters also for the maximum price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters currently in use:\n",
      "\n",
      "{'base_score': 0.5,\n",
      " 'booster': 'gbtree',\n",
      " 'colsample_bylevel': 1,\n",
      " 'colsample_bynode': 1,\n",
      " 'colsample_bytree': 1,\n",
      " 'gamma': 0,\n",
      " 'importance_type': 'gain',\n",
      " 'learning_rate': 0.1,\n",
      " 'max_delta_step': 0,\n",
      " 'max_depth': 3,\n",
      " 'min_child_weight': 1,\n",
      " 'missing': None,\n",
      " 'n_estimators': 100,\n",
      " 'n_jobs': 1,\n",
      " 'nthread': None,\n",
      " 'objective': 'reg:linear',\n",
      " 'random_state': 0,\n",
      " 'reg_alpha': 0,\n",
      " 'reg_lambda': 1,\n",
      " 'scale_pos_weight': 1,\n",
      " 'seed': None,\n",
      " 'silent': None,\n",
      " 'subsample': 1,\n",
      " 'verbosity': 1}\n"
     ]
    }
   ],
   "source": [
    "# define model: I just add some default parameters but you could\n",
    "# also just write: RandomForestRegressor() since we will perform a grid search \n",
    "# to find good hyperparameter values\n",
    "model_min_p = XGBRegressor()\n",
    "model_max_p = XGBRegressor()\n",
    "\n",
    "# Look at parameters used by our current forest\n",
    "print('Parameters currently in use:\\n')\n",
    "pprint(model_min_p.get_params())\n",
    "\n",
    "# add to pipeline\n",
    "# 1) min price\n",
    "pipeline_min_p = Pipeline(memory=None,\n",
    "              steps=[('preprocessor', preprocessor),\n",
    "                     ('regressor', model_min_p)])\n",
    "\n",
    "\n",
    "# add transformation on the target variable, by default power transformation \n",
    "# also performs standardization after performing the power transformation\n",
    "# and back transform to the original space when outputting predictions \n",
    "\n",
    "# 1) min price\n",
    "transformer_target = PowerTransformer(method='yeo-johnson',standardize=False)\n",
    "scale_target = RobustScaler(with_centering=True, with_scaling=True, quantile_range=(10.0, 90.0))\n",
    "pipeline_y = Pipeline(memory=None,\n",
    "              steps=[('transformer', transformer_target),\n",
    "                     ('scaler',scale_target)])\n",
    "\n",
    "pipeline_min_p_update = TransformedTargetRegressor(regressor=pipeline_min_p, \n",
    "                                         transformer=pipeline_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = [0.1, 0.01, 0.005, 0.001]\n",
    "\n",
    "# The number of boosting stages to perform. \n",
    "# Gradient boosting is fairly robust to over-fitting so a large number usually results in better performance.\n",
    "n_estimators = [int(x) for x in np.linspace(start = 500, stop = 5000, num = 20)]\n",
    "\n",
    "\n",
    "# maximum depth of the individual regression estimators\n",
    "max_depth = [int(x) for x in np.linspace(3, 80, num = 50)]\n",
    "\n",
    "\n",
    "# Create the random grid\n",
    "random_grid = {\n",
    "   'regressor__regressor__n_estimators': n_estimators,\n",
    "   'regressor__regressor__learning_rate': learning_rate,\n",
    "   'regressor__regressor__max_depth': max_depth\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum weights: 1.0\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:  9.2min\n",
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed: 17.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23:59:04] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "{'regressor__regressor__n_estimators': 736, 'regressor__regressor__max_depth': 7, 'regressor__regressor__learning_rate': 0.01}\n",
      "-165.38224405708237\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_regressor__regressor__n_estimators</th>\n",
       "      <th>param_regressor__regressor__max_depth</th>\n",
       "      <th>param_regressor__regressor__learning_rate</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14.797972</td>\n",
       "      <td>0.357327</td>\n",
       "      <td>0.070694</td>\n",
       "      <td>0.013126</td>\n",
       "      <td>736</td>\n",
       "      <td>7</td>\n",
       "      <td>0.01</td>\n",
       "      <td>{'regressor__regressor__n_estimators': 736, 'r...</td>\n",
       "      <td>-186.457748</td>\n",
       "      <td>-134.510213</td>\n",
       "      <td>-195.961868</td>\n",
       "      <td>-173.750286</td>\n",
       "      <td>-136.231106</td>\n",
       "      <td>-165.382244</td>\n",
       "      <td>25.503669</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>82.834422</td>\n",
       "      <td>6.094726</td>\n",
       "      <td>0.107540</td>\n",
       "      <td>0.013167</td>\n",
       "      <td>4289</td>\n",
       "      <td>9</td>\n",
       "      <td>0.01</td>\n",
       "      <td>{'regressor__regressor__n_estimators': 4289, '...</td>\n",
       "      <td>-190.603834</td>\n",
       "      <td>-148.496731</td>\n",
       "      <td>-199.770421</td>\n",
       "      <td>-183.681988</td>\n",
       "      <td>-137.685603</td>\n",
       "      <td>-172.047715</td>\n",
       "      <td>24.428008</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14.834876</td>\n",
       "      <td>1.659136</td>\n",
       "      <td>0.052375</td>\n",
       "      <td>0.014771</td>\n",
       "      <td>4052</td>\n",
       "      <td>20</td>\n",
       "      <td>0.1</td>\n",
       "      <td>{'regressor__regressor__n_estimators': 4052, '...</td>\n",
       "      <td>-196.516695</td>\n",
       "      <td>-151.941331</td>\n",
       "      <td>-205.623881</td>\n",
       "      <td>-198.984469</td>\n",
       "      <td>-144.387022</td>\n",
       "      <td>-179.490680</td>\n",
       "      <td>25.861441</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>7.421734</td>\n",
       "      <td>1.463195</td>\n",
       "      <td>0.048917</td>\n",
       "      <td>0.014022</td>\n",
       "      <td>500</td>\n",
       "      <td>78</td>\n",
       "      <td>0.1</td>\n",
       "      <td>{'regressor__regressor__n_estimators': 500, 'r...</td>\n",
       "      <td>-197.658251</td>\n",
       "      <td>-150.920527</td>\n",
       "      <td>-205.259629</td>\n",
       "      <td>-199.188775</td>\n",
       "      <td>-144.461363</td>\n",
       "      <td>-179.497709</td>\n",
       "      <td>26.174112</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8.907787</td>\n",
       "      <td>0.870712</td>\n",
       "      <td>0.048062</td>\n",
       "      <td>0.012482</td>\n",
       "      <td>1684</td>\n",
       "      <td>73</td>\n",
       "      <td>0.1</td>\n",
       "      <td>{'regressor__regressor__n_estimators': 1684, '...</td>\n",
       "      <td>-197.658251</td>\n",
       "      <td>-150.920540</td>\n",
       "      <td>-205.259635</td>\n",
       "      <td>-199.188764</td>\n",
       "      <td>-144.461363</td>\n",
       "      <td>-179.497710</td>\n",
       "      <td>26.174109</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0      14.797972      0.357327         0.070694        0.013126   \n",
       "3      82.834422      6.094726         0.107540        0.013167   \n",
       "1      14.834876      1.659136         0.052375        0.014771   \n",
       "9       7.421734      1.463195         0.048917        0.014022   \n",
       "5       8.907787      0.870712         0.048062        0.012482   \n",
       "\n",
       "  param_regressor__regressor__n_estimators  \\\n",
       "0                                      736   \n",
       "3                                     4289   \n",
       "1                                     4052   \n",
       "9                                      500   \n",
       "5                                     1684   \n",
       "\n",
       "  param_regressor__regressor__max_depth  \\\n",
       "0                                     7   \n",
       "3                                     9   \n",
       "1                                    20   \n",
       "9                                    78   \n",
       "5                                    73   \n",
       "\n",
       "  param_regressor__regressor__learning_rate  \\\n",
       "0                                      0.01   \n",
       "3                                      0.01   \n",
       "1                                       0.1   \n",
       "9                                       0.1   \n",
       "5                                       0.1   \n",
       "\n",
       "                                              params  split0_test_score  \\\n",
       "0  {'regressor__regressor__n_estimators': 736, 'r...        -186.457748   \n",
       "3  {'regressor__regressor__n_estimators': 4289, '...        -190.603834   \n",
       "1  {'regressor__regressor__n_estimators': 4052, '...        -196.516695   \n",
       "9  {'regressor__regressor__n_estimators': 500, 'r...        -197.658251   \n",
       "5  {'regressor__regressor__n_estimators': 1684, '...        -197.658251   \n",
       "\n",
       "   split1_test_score  split2_test_score  split3_test_score  split4_test_score  \\\n",
       "0        -134.510213        -195.961868        -173.750286        -136.231106   \n",
       "3        -148.496731        -199.770421        -183.681988        -137.685603   \n",
       "1        -151.941331        -205.623881        -198.984469        -144.387022   \n",
       "9        -150.920527        -205.259629        -199.188775        -144.461363   \n",
       "5        -150.920540        -205.259635        -199.188764        -144.461363   \n",
       "\n",
       "   mean_test_score  std_test_score  rank_test_score  \n",
       "0      -165.382244       25.503669                1  \n",
       "3      -172.047715       24.428008                2  \n",
       "1      -179.490680       25.861441                3  \n",
       "9      -179.497709       26.174112                4  \n",
       "5      -179.497710       26.174109                5  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define random search (and narrow down time grid search)\n",
    "\n",
    "if RUN_GRID_SEARCH:\n",
    "    min_p_random_search = RandomizedSearchCV(\n",
    "       estimator = pipeline_min_p_update, \n",
    "       param_distributions = random_grid, n_iter = 10,\n",
    "       cv = 5, verbose=2, random_state=1, n_jobs = -1, refit=True,\n",
    "       scoring=make_scorer(custom_scoring_func_single_p, greater_is_better=False)\n",
    "    )\n",
    "\n",
    "    # run grid search and refit with best hyper parameters\n",
    "    weights_train_min_p =  weights_samples(df=y_train.iloc[:,0], order=0, plot_weights=False)\n",
    "    min_p_random_search.fit(X_train, y_train.iloc[:,0])  \n",
    "    print(min_p_random_search.best_params_)    \n",
    "    print(min_p_random_search.best_score_)\n",
    "\n",
    "\n",
    "# have look at the best hyperparameters and their respective performance (maybe also look at the sd)\n",
    "pd.DataFrame(min_p_random_search.cv_results_).sort_values(\n",
    "        by=['mean_test_score'],ascending=False).head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2) Manual parameters tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum weights: 1.0\n",
      "[Pipeline] ...... (step 1 of 2) Processing preprocessor, total=   0.5s\n",
      "[00:03:35] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[Pipeline] ......... (step 2 of 2) Processing regressor, total=   6.3s\n",
      "Sum weights: 1.0\n",
      "[Pipeline] ...... (step 1 of 2) Processing preprocessor, total=   0.4s\n",
      "[00:03:42] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[Pipeline] ......... (step 2 of 2) Processing regressor, total=   6.6s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TransformedTargetRegressor(check_inverse=True, func=None, inverse_func=None,\n",
       "                           regressor=Pipeline(memory=None,\n",
       "                                              steps=[('preprocessor',\n",
       "                                                      ColumnTransformer(n_jobs=None,\n",
       "                                                                        remainder='drop',\n",
       "                                                                        sparse_threshold=0.3,\n",
       "                                                                        transformer_weights=None,\n",
       "                                                                        transformers=[('num',\n",
       "                                                                                       Pipeline(memory=None,\n",
       "                                                                                                steps=[('imputer',\n",
       "                                                                                                        IterativeImputer(add_indicator=False,\n",
       "                                                                                                                         estimator=None,\n",
       "                                                                                                                         imputation_order=...\n",
       "                                                                   random_state=0,\n",
       "                                                                   reg_alpha=0,\n",
       "                                                                   reg_lambda=1,\n",
       "                                                                   scale_pos_weight=1,\n",
       "                                                                   seed=None,\n",
       "                                                                   silent=None,\n",
       "                                                                   subsample=1,\n",
       "                                                                   verbosity=1))],\n",
       "                                              verbose=True),\n",
       "                           transformer=Pipeline(memory=None,\n",
       "                                                steps=[('transformer',\n",
       "                                                        PowerTransformer(copy=True,\n",
       "                                                                         method='yeo-johnson',\n",
       "                                                                         standardize=False)),\n",
       "                                                       ('scaler',\n",
       "                                                        RobustScaler(copy=True,\n",
       "                                                                     quantile_range=(10.0,\n",
       "                                                                                     90.0),\n",
       "                                                                     with_centering=True,\n",
       "                                                                     with_scaling=True))],\n",
       "                                                verbose=False))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train your final model on all data with best parameters \n",
    "\n",
    "# 1) min price\n",
    "model_min_p = XGBRegressor(\n",
    "    n_estimators = 736,\n",
    "    max_depth = 7,\n",
    "    learning_rate = 0.01\n",
    ")\n",
    "\n",
    "# 2)  min price\n",
    "model_max_p = XGBRegressor(\n",
    "    n_estimators = 736,\n",
    "    max_depth = 7,\n",
    "    learning_rate = 0.01\n",
    ")\n",
    "\n",
    "\n",
    "# add to pipeline\n",
    "\n",
    "# 1) min price\n",
    "pipeline_min_p = Pipeline(memory=None,\n",
    "              steps=[('preprocessor', preprocessor),\n",
    "                     ('regressor', model_min_p)],\n",
    "              verbose=True)\n",
    "\n",
    "# 2) min price\n",
    "pipeline_max_p = Pipeline(memory=None,\n",
    "              steps=[('preprocessor', preprocessor),\n",
    "                     ('regressor', model_max_p)],\n",
    "              verbose=True)\n",
    "\n",
    "# again add transformer for target variable\n",
    "\n",
    "# 1) min price\n",
    "pipeline_min_p_update = TransformedTargetRegressor(regressor=pipeline_min_p, \n",
    "                                         transformer=pipeline_y)\n",
    "# 2) max price\n",
    "pipeline_max_p_update = TransformedTargetRegressor(regressor=pipeline_max_p, \n",
    "                                         transformer=pipeline_y)\n",
    "\n",
    "# fit final model on all training data we have at hand\n",
    "\n",
    "# 1) min price\n",
    "weights_train_min_p = weights_samples(y_train.iloc[:,0], order=0)\n",
    "pipeline_min_p_update.fit(X_train, y_train.iloc[:,0])\n",
    "\n",
    "# 2) max price\n",
    "weights_train_max_p = weights_samples(y_train.iloc[:,1], order=0)\n",
    "pipeline_max_p_update.fit(X_train, y_train.iloc[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'minimum price': 38.983681718916166,\n",
       " 'maximum price': 39.50561005677228,\n",
       " 'total error': 78.48929177568844}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# performance on data where the model was fit one (should be very low)\n",
    "pred_train_min_p = pipeline_min_p_update.predict(X_train)\n",
    "pred_train_max_p = pipeline_max_p_update.predict(X_train)\n",
    "\n",
    "# calculate performance\n",
    "pred_train = pd.DataFrame([pred_train_min_p,pred_train_max_p]).T\n",
    "calculate_perf(y_train, pred_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Performance on validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'minimum price': 131.2029311347008,\n",
       " 'maximum price': 135.79809481620788,\n",
       " 'total error': 267.0010259509087}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# performance on validation data\n",
    "pred_val_min_p = pipeline_min_p_update.predict(X_val)\n",
    "pred_val_max_p = pipeline_max_p_update.predict(X_val)\n",
    "\n",
    "# calculate performance \n",
    "pred_val = pd.DataFrame([pred_val_min_p,pred_val_max_p]).T\n",
    "calculate_perf(y_val, pred_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save predictions on validation data\n",
    "# submission format\n",
    "submission_format_validation = pd.DataFrame.from_dict(\n",
    " {'ID':df_val['id'].values,\n",
    " 'MIN':pred_val_min_p,\n",
    " 'MAX':pred_val_max_p}).set_index('ID')\n",
    "\n",
    "# write to csv\n",
    "submission_format_validation.to_csv('output/validation/xgboost.csv' ,\n",
    "                            header=True, index=True, sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C) Post processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c9066acfc804d15aa568a1fe49f5e5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = plt.subplots(2, 2, figsize=(9, 7))\n",
    "axs = np.ravel(axs)\n",
    "\n",
    "# fitted against true predictions minimum price\n",
    "plot_predictions_results(ax=axs[0], \n",
    "                        y_true=y_val.iloc[:,0], \n",
    "                        y_pred=pred_val_min_p, \n",
    "                        title=\"Boosting Min. Price Test Set\", \n",
    "                        log_scale=True)\n",
    "\n",
    "# fitted against true predictions maximum price\n",
    "plot_predictions_results(ax=axs[1], \n",
    "                        y_true=y_val.iloc[:,1], \n",
    "                        y_pred=pred_val_max_p, \n",
    "                        title=\"Boosting Max. Price Test Set\", \n",
    "                        log_scale=True)\n",
    "\n",
    "# residuals minimum price\n",
    "plot_residuals(ax=axs[2], \n",
    "               y_true=y_val.iloc[:,0], \n",
    "               y_pred=pred_val_min_p, \n",
    "               title=\"\", \n",
    "               log_scale=False,\n",
    "               order=1)\n",
    "\n",
    "# residuals maximum price\n",
    "plot_residuals(ax=axs[3], \n",
    "               y_true=y_val.iloc[:,1], \n",
    "               y_pred=pred_val_max_p, \n",
    "               title=\"\", \n",
    "               log_scale=False,\n",
    "               order=1)\n",
    "fig.tight_layout()\n",
    "plt.savefig('output/figures/xgboost/fig1.png', dpi=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f634db499ba45248a1c1afbb6a4e417",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(8,4))\n",
    "plt.plot(pred_val_max_p - pred_val_min_p, label=\"Predictions Test\", linestyle=\"--\")\n",
    "plt.plot(y_val.iloc[:,1] - y_val.iloc[:,0], label=\"Truth Test\", linestyle=':')\n",
    "plt.xlabel(\"Observations\")\n",
    "plt.ylabel(\"Max. Price - Min. Price\")\n",
    "plt.legend(loc=\"upper right\");\n",
    "plt.savefig('output/figures/xgboost/fig2.png', dpi=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "vip_val_min_p = permutation_importance(pipeline_min_p_update, X_val, y_val.iloc[:,0], n_repeats=10,\n",
    "                                random_state=1, n_jobs=3)\n",
    "\n",
    "vip_val_max_p = permutation_importance(pipeline_min_p_update, X_val, y_val.iloc[:,1], n_repeats=10,\n",
    "                                random_state=1, n_jobs=3)\n",
    "\n",
    "sorted_idx_min_p = vip_val_min_p.importances_mean.argsort()\n",
    "sorted_idx_max_p = vip_val_max_p.importances_mean.argsort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8827d2517814ecc94e40a195a69e464",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(9, 4))\n",
    "ax = np.ravel(ax)\n",
    "# minimum price\n",
    "ax[0].boxplot(vip_val_min_p.importances[sorted_idx_min_p].T,\n",
    "           vert=False, labels=X_val.columns[sorted_idx_min_p])\n",
    "ax[0].set_title(\"Permutation Importances: Min. Price (Test set)\", fontsize=9)\n",
    "ax[0].xaxis.set_tick_params(labelsize=8)\n",
    "ax[0].yaxis.set_tick_params(labelsize=8)\n",
    "\n",
    "# maximum price\n",
    "ax[1].boxplot(vip_val_min_p.importances[sorted_idx_max_p].T,\n",
    "           vert=False, labels=X_val.columns[sorted_idx_max_p])\n",
    "ax[1].set_title(\"Permutation Importances: Max. Price (Test set)\", fontsize=9)\n",
    "ax[1].xaxis.set_tick_params(labelsize=8)\n",
    "ax[1].yaxis.set_tick_params(labelsize=8)\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "plt.savefig('output/figures/boosting/fig3.png', dpi=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D) Predictions test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum weights: 1.0\n",
      "[Pipeline] ...... (step 1 of 2) Processing preprocessor, total=   0.5s\n",
      "[00:07:52] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[Pipeline] ......... (step 2 of 2) Processing regressor, total=  11.5s\n",
      "Sum weights: 1.0\n",
      "[Pipeline] ...... (step 1 of 2) Processing preprocessor, total=   0.4s\n",
      "[00:08:04] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[Pipeline] ......... (step 2 of 2) Processing regressor, total=  11.3s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TransformedTargetRegressor(check_inverse=True, func=None, inverse_func=None,\n",
       "                           regressor=Pipeline(memory=None,\n",
       "                                              steps=[('preprocessor',\n",
       "                                                      ColumnTransformer(n_jobs=None,\n",
       "                                                                        remainder='drop',\n",
       "                                                                        sparse_threshold=0.3,\n",
       "                                                                        transformer_weights=None,\n",
       "                                                                        transformers=[('num',\n",
       "                                                                                       Pipeline(memory=None,\n",
       "                                                                                                steps=[('imputer',\n",
       "                                                                                                        IterativeImputer(add_indicator=False,\n",
       "                                                                                                                         estimator=None,\n",
       "                                                                                                                         imputation_order=...\n",
       "                                                                   random_state=0,\n",
       "                                                                   reg_alpha=0,\n",
       "                                                                   reg_lambda=1,\n",
       "                                                                   scale_pos_weight=1,\n",
       "                                                                   seed=None,\n",
       "                                                                   silent=None,\n",
       "                                                                   subsample=1,\n",
       "                                                                   verbosity=1))],\n",
       "                                              verbose=True),\n",
       "                           transformer=Pipeline(memory=None,\n",
       "                                                steps=[('transformer',\n",
       "                                                        PowerTransformer(copy=True,\n",
       "                                                                         method='yeo-johnson',\n",
       "                                                                         standardize=False)),\n",
       "                                                       ('scaler',\n",
       "                                                        RobustScaler(copy=True,\n",
       "                                                                     quantile_range=(10.0,\n",
       "                                                                                     90.0),\n",
       "                                                                     with_centering=True,\n",
       "                                                                     with_scaling=True))],\n",
       "                                                verbose=False))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train your final model on all data with best parameters \n",
    "\n",
    "# 1) min price\n",
    "model_min_p_final = XGBRegressor(\n",
    "    n_estimators = 736,\n",
    "    max_depth = 7,\n",
    "    learning_rate = 0.01\n",
    ")\n",
    "\n",
    "# 2)  min price\n",
    "model_max_p_final = XGBRegressor(\n",
    "    n_estimators = 736,\n",
    "    max_depth = 7,\n",
    "    learning_rate = 0.01\n",
    ")\n",
    "\n",
    "\n",
    "# add to pipeline\n",
    "\n",
    "# 1) min price\n",
    "pipeline_min_p_final = Pipeline(memory=None,\n",
    "              steps=[('preprocessor', preprocessor),\n",
    "                     ('regressor', model_min_p_final)],\n",
    "              verbose=True)\n",
    "\n",
    "# 2) min price\n",
    "pipeline_max_p_final = Pipeline(memory=None,\n",
    "              steps=[('preprocessor', preprocessor),\n",
    "                     ('regressor', model_max_p_final)],\n",
    "              verbose=True)\n",
    "\n",
    "# again add transformer for target variable\n",
    "\n",
    "# 1) min price\n",
    "pipeline_min_p_final = TransformedTargetRegressor(regressor=pipeline_min_p_final, \n",
    "                                         transformer=pipeline_y)\n",
    "# 2) max price\n",
    "pipeline_max_p_final = TransformedTargetRegressor(regressor=pipeline_max_p_final, \n",
    "                                         transformer=pipeline_y)\n",
    "\n",
    "# fit final model on all training data we have at hand\n",
    "\n",
    "# 1) min price\n",
    "weights_all_train_min_p = weights_samples(y_all_train.iloc[:,0], order=2)\n",
    "pipeline_min_p_final.fit(X_all_train, y_all_train.iloc[:,0])\n",
    "\n",
    "# 2) max price\n",
    "weights_all_train_max_p = weights_samples(y_all_train.iloc[:,1], order=2)\n",
    "pipeline_max_p_final.fit(X_all_train, y_all_train.iloc[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'minimum price': 41.18461220774931,\n",
       " 'maximum price': 44.883610522101904,\n",
       " 'total error': 86.06822272985121}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# performance on all data where the model was fit one (should be very low)\n",
    "pred_all_train_min_p = pipeline_min_p_final.predict(X_all_train)\n",
    "pred_all_train_max_p = pipeline_max_p_final.predict(X_all_train)\n",
    "\n",
    "# calculate performance\n",
    "pred_all_train = pd.DataFrame([pred_all_train_min_p, pred_all_train_max_p]).T\n",
    "calculate_perf(y_all_train, pred_all_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions on test data\n",
    "pred_test_min_p = pipeline_min_p_final.predict(X_test)\n",
    "pred_test_max_p = pipeline_max_p_final.predict(X_test)\n",
    "\n",
    "# submission format\n",
    "submission_format = pd.DataFrame.from_dict(\n",
    " {'ID':df_test['id'].values,\n",
    " 'MIN':pred_test_min_p,\n",
    " 'MAX':pred_test_max_p}).set_index('ID')\n",
    "\n",
    "# write to csv\n",
    "submission_format.to_csv('output/submission/xgboosting.csv' ,\n",
    "                            header=True, index=True, sep=',')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
