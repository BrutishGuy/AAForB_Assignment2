{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# auto reload libraries (you do need to re-import libraries if you make changes)\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# base \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "import os\n",
    "\n",
    "# preprocessing \n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.experimental import enable_iterative_imputer \n",
    "from sklearn.impute import SimpleImputer, IterativeImputer,KNNImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, RobustScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# models\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# own defined functions/classes \n",
    "from preprocessing.preprocess_pipe import MakeLowerCase\n",
    "from preprocessing.preprocess_pipe import HdResolutionCategorizer\n",
    "from preprocessing.preprocess_pipe import StorageCategorizer\n",
    "from preprocessing.preprocess_pipe import SsdCategorizer\n",
    "\n",
    "from preprocessing.preprocess_pipe import print_missing\n",
    "from preprocessing.preprocess_pipe import calculate_perf\n",
    "from preprocessing.preprocess_pipe import custom_scoring_func_single_p\n",
    "\n",
    "from modelling.weight_samples import weights_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Pieter-Jan\\Documents\\KuLeuven\\Semester2\\AA\\AAForB_Assignment2\n"
     ]
    }
   ],
   "source": [
    "# global parameters\n",
    "pd.set_option('display.max_columns', 500)\n",
    "\n",
    "# set working directory\n",
    "uppath = lambda _path, n: os.sep.join(_path.split(os.sep)[:-n])\n",
    "__file__ = 'C:\\\\Users\\\\Pieter-Jan\\\\Documents\\\\KuLeuven\\\\Semester2\\\\AA\\\\AAForB_Assignment2\\\\code\\\\PieterJan'\n",
    "out = uppath(__file__, 2)\n",
    "os.chdir(out)\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: import data and split in train and validation set\n",
    "The validation set is more our own kind of test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions of all training data (510, 22)\n",
      "Dimension test data (222, 20)\n"
     ]
    }
   ],
   "source": [
    "# read in trainig and validation data\n",
    "# use the same data split as we did in R\n",
    "df_all_train = pd.read_csv(\"data\\\\train.csv\", sep=',')\n",
    "df_test = pd.read_csv(\"data\\\\test.csv\", sep=',')\n",
    "\n",
    "print(f'Dimensions of all training data {df_all_train.shape}')\n",
    "print(f'Dimension test data {df_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split in training and validation set\n",
    "df_train = df_all_train.sample(frac=0.75, random_state=0, replace=False)\n",
    "df_val = df_all_train.drop(df_train.index)\n",
    "\n",
    "# reset index, if you don't resit missing rows get inserted in the pipeline\n",
    "# see: https://stackoverflow.com/questions/31323499/sklearn-error-valueerror-input-contains-nan-infinity-or-a-value-too-large-for\n",
    "df_train = df_train.reset_index().drop('index',axis=1)\n",
    "df_val = df_val.reset_index().drop('index',axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: define pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Numerical features to pass down the numerical pipeline \n",
    "numerical_features = ['screen_size' ,'pixels_x','pixels_y',\n",
    "                      'ram', 'weight','ssd','storage']\n",
    "\n",
    "#Categrical features to pass down the categorical pipeline \n",
    "categorical_features = ['brand','base_name', 'screen_surface','touchscreen',\n",
    "                        'cpu','cpu_details','detachable_keyboard',\n",
    "                        'discrete_gpu','gpu', 'os','os_details']\n",
    "\n",
    "# define all unique features\n",
    "features = np.unique(numerical_features).tolist() + np.unique(categorical_features).tolist()\n",
    "\n",
    "# target variables\n",
    "target = ['min_price','max_price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "X_train = df_train[features]\n",
    "y_train = df_train[target]\n",
    "\n",
    "# validation (this is kind our own test set)\n",
    "X_val  = df_val[features]\n",
    "y_val = df_val[target]\n",
    "\n",
    "# train_validation (this is all training data we have) for fitting the model\n",
    "X_all_train = df_all_train[features]\n",
    "y_all_train = df_all_train[target]\n",
    "\n",
    "# test\n",
    "X_test = df_test[features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can add many more and \n",
    "# you can even define custom preprocessing steps like 'MakeLowerCase()'\n",
    "\n",
    "# pipeline  uses only numerical features,\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    #('imputer', SimpleImputer(missing_values=np.nan, strategy='mean')),\n",
    "    #('imputer', KNNImputer(n_neighbors=5, weights='uniform',metric='nan_euclidean')),\n",
    "    ('imputer', IterativeImputer(missing_values=np.nan, random_state=0, n_nearest_features=None, sample_posterior=True)),\n",
    "    #('transformation', PowerTransformer(method='yeo-johnson',standardize=False)),\n",
    "     #Scale features using statistics that are robust to outliers.\n",
    "    ('scaler', RobustScaler(with_centering=True, with_scaling=True, quantile_range=(10.0, 90.0)))]) \n",
    "\n",
    "# pipeline use only categorical features\n",
    "categorical_transformer = Pipeline(steps=[ \n",
    "    ('lowercase', MakeLowerCase()), # lower cases all columns containing strings\n",
    "    #('sd_category' ,SsdCategorizer(drop_original_feature=True)),\n",
    "    #('storage_category', StorageCategorizer(drop_original_feature=True)),\n",
    "    ('imputer', SimpleImputer(missing_values=np.nan, strategy=\"most_frequent\")),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "# add both preprocessing pipelines in one pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numerical_features),\n",
    "        ('cat', categorical_transformer, categorical_features)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(382, 1)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is how the preporcessed pipeline looks like (just to have an idea)\n",
    "pd.DataFrame(preprocessor.fit_transform(X_train)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: add models to pipeline\n",
    "\n",
    "Possible to fit multiple target variabels, so you **don't** need to fit a different models for min. price and max. price\n",
    "\n",
    "### A) Training and parameter tuning\n",
    "\n",
    "##### 1) Automatic tuning via grid search\n",
    "\n",
    "I will only **do the tuning for the minimum price** and use the found parameters also for the maximum price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters currently in use:\n",
      "\n",
      "{'alpha': 0.9,\n",
      " 'ccp_alpha': 0.0,\n",
      " 'criterion': 'friedman_mse',\n",
      " 'init': None,\n",
      " 'learning_rate': 0.1,\n",
      " 'loss': 'ls',\n",
      " 'max_depth': 3,\n",
      " 'max_features': None,\n",
      " 'max_leaf_nodes': None,\n",
      " 'min_impurity_decrease': 0.0,\n",
      " 'min_impurity_split': None,\n",
      " 'min_samples_leaf': 1,\n",
      " 'min_samples_split': 2,\n",
      " 'min_weight_fraction_leaf': 0.0,\n",
      " 'n_estimators': 100,\n",
      " 'n_iter_no_change': None,\n",
      " 'presort': 'deprecated',\n",
      " 'random_state': 1,\n",
      " 'subsample': 1.0,\n",
      " 'tol': 0.0001,\n",
      " 'validation_fraction': 0.1,\n",
      " 'verbose': 0,\n",
      " 'warm_start': False}\n"
     ]
    }
   ],
   "source": [
    "# define model: I just add some default parameters but you could\n",
    "# also just write: RandomForestRegressor() since we will perform a grid search \n",
    "# to find good hyperparameter values\n",
    "model_min_p = GradientBoostingRegressor(random_state=1)\n",
    "model_max_p = GradientBoostingRegressor(random_state=1)\n",
    "\n",
    "# Look at parameters used by our current forest\n",
    "print('Parameters currently in use:\\n')\n",
    "pprint(model_min_p.get_params())\n",
    "\n",
    "# add to pipeline\n",
    "# 1) min price\n",
    "pipeline_min_p = Pipeline(memory=None,\n",
    "              steps=[('preprocessor', preprocessor),\n",
    "                     ('regressor', model_min_p)])\n",
    "\n",
    "\n",
    "# add transformation on the target variable, by default power transformation \n",
    "# also performs standardization after performing the power transformation\n",
    "# and back transform to the original space when outputting predictions \n",
    "\n",
    "# 1) min price\n",
    "transformer_target = PowerTransformer(method='yeo-johnson',standardize=True)\n",
    "pipeline_min_p_update = TransformedTargetRegressor(regressor=pipeline_min_p, \n",
    "                                         transformer=transformer_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function\n",
    "loss_function = ['ls', 'lad']\n",
    "\n",
    "learning_rate = [0.1, 0.01, 0.005, 0.001]\n",
    "\n",
    "# The number of boosting stages to perform. \n",
    "# Gradient boosting is fairly robust to over-fitting so a large number usually results in better performance.\n",
    "n_estimators = [int(x) for x in np.linspace(start = 2000, stop = 5000, num = 10)]\n",
    "\n",
    "# The number of features to consider when looking for the best split:\n",
    "# - If “auto”, then max_features=n_features.\n",
    "# - If “sqrt”, then max_features=sqrt(n_features).\n",
    "# - If “log2”, then max_features=log2(n_features).\n",
    "# - If None, then max_features=n_features\n",
    "max_features = ['auto', 'sqrt','log2']\n",
    "\n",
    "# maximum depth of the individual regression estimators\n",
    "max_depth = [int(x) for x in np.linspace(20, 80, num = 10)]\n",
    "\n",
    "n_iter_no_change = [10,20,50]\n",
    "\n",
    "# The function to measure the quality of a split\n",
    "criterion = ['friedman_mse']\n",
    "\n",
    "# Minimum number of samples required to split a node\n",
    "# - If int, then consider min_samples_split as the minimum number.\n",
    "# - If float, then min_samples_split is a fraction and ceil(min_samples_split * n_samples) are the minimum number of samples for each split.\n",
    "min_samples_split = [0.001, 0.01, 0.1, 0.3, 0.5, 0.7 , 0.9]\n",
    "# Minimum number of samples required at each leaf node\n",
    "\n",
    "# - If int, then consider min_samples_leaf as the minimum number.\n",
    "# - If float, then min_samples_leaf is a fraction and ceil(min_samples_leaf * n_samples) are the minimum number of samples for each node.\n",
    "min_samples_leaf = [0.001, 0.01, 0.1, 0.3, 0.5]\n",
    "\n",
    "# Create the random grid\n",
    "random_grid = {\n",
    "   'regressor__regressor__n_estimators': n_estimators,\n",
    "   'regressor__regressor__learning_rate': learning_rate,\n",
    "   'regressor__regressor__max_features': max_features,\n",
    "   'regressor__regressor__max_depth': max_depth,\n",
    "   'regressor__regressor__criterion': criterion,\n",
    "   'regressor__regressor__min_samples_split': min_samples_split,\n",
    "   'regressor__regressor__min_samples_leaf': min_samples_leaf,\n",
    "   'regressor__regressor__n_iter_no_change': n_iter_no_change,\n",
    "   'regressor__regressor__loss': loss_function \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum weights: 1.0\n",
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:   54.5s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:  2.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ...... (step 1 of 2) Processing preprocessor, total=   0.4s\n",
      "[Pipeline] ......... (step 2 of 2) Processing regressor, total=  50.5s\n",
      "{'regressor__regressor__n_iter_no_change': 50, 'regressor__regressor__n_estimators': 5000, 'regressor__regressor__min_samples_split': 0.01, 'regressor__regressor__min_samples_leaf': 0.001, 'regressor__regressor__max_features': 'log2', 'regressor__regressor__max_depth': 46, 'regressor__regressor__loss': 'lad', 'regressor__regressor__learning_rate': 0.01, 'regressor__regressor__criterion': 'friedman_mse'}\n",
      "-160.0080915995132\n"
     ]
    }
   ],
   "source": [
    "# define random search (and narrow down time grid search)\n",
    "min_p_random_search = RandomizedSearchCV(\n",
    "   estimator = pipeline_min_p_update, \n",
    "   param_distributions = random_grid, n_iter = 20,\n",
    "   cv = 5, verbose=2, random_state=1, n_jobs = -1, refit=True,\n",
    "   scoring=make_scorer(custom_scoring_func_single_p, greater_is_better=False)\n",
    ")\n",
    "\n",
    "# run grid search and refit with best hyper parameters\n",
    "weights_train_min_p =  weights_samples(df=y_train.iloc[:,0], order=0, plot_weights=False)\n",
    "min_p_random_search.fit(X_train, y_train.iloc[:,0])  \n",
    "print(min_p_random_search.best_params_)    \n",
    "print(min_p_random_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_regressor__regressor__n_iter_no_change</th>\n",
       "      <th>param_regressor__regressor__n_estimators</th>\n",
       "      <th>param_regressor__regressor__min_samples_split</th>\n",
       "      <th>param_regressor__regressor__min_samples_leaf</th>\n",
       "      <th>param_regressor__regressor__max_features</th>\n",
       "      <th>param_regressor__regressor__max_depth</th>\n",
       "      <th>param_regressor__regressor__loss</th>\n",
       "      <th>param_regressor__regressor__learning_rate</th>\n",
       "      <th>param_regressor__regressor__criterion</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>42.468070</td>\n",
       "      <td>15.166462</td>\n",
       "      <td>0.097424</td>\n",
       "      <td>0.034483</td>\n",
       "      <td>50</td>\n",
       "      <td>5000</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.001</td>\n",
       "      <td>log2</td>\n",
       "      <td>46</td>\n",
       "      <td>lad</td>\n",
       "      <td>0.01</td>\n",
       "      <td>friedman_mse</td>\n",
       "      <td>{'regressor__regressor__n_iter_no_change': 50,...</td>\n",
       "      <td>-184.336493</td>\n",
       "      <td>-144.072509</td>\n",
       "      <td>-181.548667</td>\n",
       "      <td>-167.570625</td>\n",
       "      <td>-122.512164</td>\n",
       "      <td>-160.008092</td>\n",
       "      <td>23.549721</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>5.357716</td>\n",
       "      <td>1.781079</td>\n",
       "      <td>0.047893</td>\n",
       "      <td>0.012286</td>\n",
       "      <td>20</td>\n",
       "      <td>4333</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.01</td>\n",
       "      <td>auto</td>\n",
       "      <td>40</td>\n",
       "      <td>lad</td>\n",
       "      <td>0.005</td>\n",
       "      <td>friedman_mse</td>\n",
       "      <td>{'regressor__regressor__n_iter_no_change': 20,...</td>\n",
       "      <td>-201.646441</td>\n",
       "      <td>-164.601983</td>\n",
       "      <td>-174.201127</td>\n",
       "      <td>-167.351437</td>\n",
       "      <td>-146.895780</td>\n",
       "      <td>-170.939354</td>\n",
       "      <td>17.802771</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>3.945026</td>\n",
       "      <td>0.843686</td>\n",
       "      <td>0.055480</td>\n",
       "      <td>0.024793</td>\n",
       "      <td>20</td>\n",
       "      <td>2000</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>auto</td>\n",
       "      <td>73</td>\n",
       "      <td>ls</td>\n",
       "      <td>0.005</td>\n",
       "      <td>friedman_mse</td>\n",
       "      <td>{'regressor__regressor__n_iter_no_change': 20,...</td>\n",
       "      <td>-189.116588</td>\n",
       "      <td>-153.472549</td>\n",
       "      <td>-208.643365</td>\n",
       "      <td>-173.647745</td>\n",
       "      <td>-137.795585</td>\n",
       "      <td>-172.535166</td>\n",
       "      <td>25.100619</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>5.249522</td>\n",
       "      <td>0.672106</td>\n",
       "      <td>0.050315</td>\n",
       "      <td>0.011809</td>\n",
       "      <td>20</td>\n",
       "      <td>3333</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.01</td>\n",
       "      <td>auto</td>\n",
       "      <td>46</td>\n",
       "      <td>ls</td>\n",
       "      <td>0.005</td>\n",
       "      <td>friedman_mse</td>\n",
       "      <td>{'regressor__regressor__n_iter_no_change': 20,...</td>\n",
       "      <td>-230.135074</td>\n",
       "      <td>-135.010345</td>\n",
       "      <td>-202.963229</td>\n",
       "      <td>-183.595405</td>\n",
       "      <td>-130.531215</td>\n",
       "      <td>-176.447054</td>\n",
       "      <td>38.631270</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.515796</td>\n",
       "      <td>0.355725</td>\n",
       "      <td>0.051715</td>\n",
       "      <td>0.017439</td>\n",
       "      <td>50</td>\n",
       "      <td>4000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>log2</td>\n",
       "      <td>60</td>\n",
       "      <td>ls</td>\n",
       "      <td>0.1</td>\n",
       "      <td>friedman_mse</td>\n",
       "      <td>{'regressor__regressor__n_iter_no_change': 50,...</td>\n",
       "      <td>-232.980835</td>\n",
       "      <td>-176.754541</td>\n",
       "      <td>-208.912433</td>\n",
       "      <td>-181.249874</td>\n",
       "      <td>-143.960496</td>\n",
       "      <td>-188.771636</td>\n",
       "      <td>30.229557</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "6       42.468070     15.166462         0.097424        0.034483   \n",
       "16       5.357716      1.781079         0.047893        0.012286   \n",
       "11       3.945026      0.843686         0.055480        0.024793   \n",
       "13       5.249522      0.672106         0.050315        0.011809   \n",
       "9        1.515796      0.355725         0.051715        0.017439   \n",
       "\n",
       "   param_regressor__regressor__n_iter_no_change  \\\n",
       "6                                            50   \n",
       "16                                           20   \n",
       "11                                           20   \n",
       "13                                           20   \n",
       "9                                            50   \n",
       "\n",
       "   param_regressor__regressor__n_estimators  \\\n",
       "6                                      5000   \n",
       "16                                     4333   \n",
       "11                                     2000   \n",
       "13                                     3333   \n",
       "9                                      4000   \n",
       "\n",
       "   param_regressor__regressor__min_samples_split  \\\n",
       "6                                           0.01   \n",
       "16                                           0.5   \n",
       "11                                           0.1   \n",
       "13                                         0.001   \n",
       "9                                          0.001   \n",
       "\n",
       "   param_regressor__regressor__min_samples_leaf  \\\n",
       "6                                         0.001   \n",
       "16                                         0.01   \n",
       "11                                          0.1   \n",
       "13                                         0.01   \n",
       "9                                           0.1   \n",
       "\n",
       "   param_regressor__regressor__max_features  \\\n",
       "6                                      log2   \n",
       "16                                     auto   \n",
       "11                                     auto   \n",
       "13                                     auto   \n",
       "9                                      log2   \n",
       "\n",
       "   param_regressor__regressor__max_depth param_regressor__regressor__loss  \\\n",
       "6                                     46                              lad   \n",
       "16                                    40                              lad   \n",
       "11                                    73                               ls   \n",
       "13                                    46                               ls   \n",
       "9                                     60                               ls   \n",
       "\n",
       "   param_regressor__regressor__learning_rate  \\\n",
       "6                                       0.01   \n",
       "16                                     0.005   \n",
       "11                                     0.005   \n",
       "13                                     0.005   \n",
       "9                                        0.1   \n",
       "\n",
       "   param_regressor__regressor__criterion  \\\n",
       "6                           friedman_mse   \n",
       "16                          friedman_mse   \n",
       "11                          friedman_mse   \n",
       "13                          friedman_mse   \n",
       "9                           friedman_mse   \n",
       "\n",
       "                                               params  split0_test_score  \\\n",
       "6   {'regressor__regressor__n_iter_no_change': 50,...        -184.336493   \n",
       "16  {'regressor__regressor__n_iter_no_change': 20,...        -201.646441   \n",
       "11  {'regressor__regressor__n_iter_no_change': 20,...        -189.116588   \n",
       "13  {'regressor__regressor__n_iter_no_change': 20,...        -230.135074   \n",
       "9   {'regressor__regressor__n_iter_no_change': 50,...        -232.980835   \n",
       "\n",
       "    split1_test_score  split2_test_score  split3_test_score  \\\n",
       "6         -144.072509        -181.548667        -167.570625   \n",
       "16        -164.601983        -174.201127        -167.351437   \n",
       "11        -153.472549        -208.643365        -173.647745   \n",
       "13        -135.010345        -202.963229        -183.595405   \n",
       "9         -176.754541        -208.912433        -181.249874   \n",
       "\n",
       "    split4_test_score  mean_test_score  std_test_score  rank_test_score  \n",
       "6         -122.512164      -160.008092       23.549721                1  \n",
       "16        -146.895780      -170.939354       17.802771                2  \n",
       "11        -137.795585      -172.535166       25.100619                3  \n",
       "13        -130.531215      -176.447054       38.631270                4  \n",
       "9         -143.960496      -188.771636       30.229557                5  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# have look at the best hyperparameters and their respective performance (maybe also look at the sd)\n",
    "pd.DataFrame(min_p_random_search.cv_results_).sort_values(\n",
    "    by=['mean_test_score'],ascending=False).head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2) Manual parameters tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum weights: 1.0\n",
      "[Pipeline] ...... (step 1 of 2) Processing preprocessor, total=   0.5s\n",
      "[Pipeline] ......... (step 2 of 2) Processing regressor, total=  15.6s\n",
      "Sum weights: 1.0\n",
      "[Pipeline] ...... (step 1 of 2) Processing preprocessor, total=   0.5s\n",
      "[Pipeline] ......... (step 2 of 2) Processing regressor, total=   6.8s\n"
     ]
    }
   ],
   "source": [
    "# train your final model on all data with best parameters \n",
    "\n",
    "# 1) min price\n",
    "model_min_p = GradientBoostingRegressor(\n",
    "     criterion='friedman_mse',\n",
    "     n_estimators=5000,\n",
    "     learning_rate=0.005,\n",
    "     loss = 'lad',\n",
    "     max_depth=56,\n",
    "     max_features='log2',\n",
    "     n_iter_no_change = 46,\n",
    "     min_samples_split=0.001,\n",
    "     min_samples_leaf=0.01,\n",
    "     random_state=0\n",
    ")\n",
    "\n",
    "# 2)  min price\n",
    "model_max_p = GradientBoostingRegressor(\n",
    "     criterion='friedman_mse',\n",
    "     n_estimators=2000, \n",
    "     learning_rate=0.005,\n",
    "     loss = 'lad',\n",
    "     max_depth=56,\n",
    "     max_features='log2',\n",
    "     n_iter_no_change = 46,\n",
    "     min_samples_split=0.001,\n",
    "     min_samples_leaf=0.01,\n",
    "     random_state=0\n",
    ")\n",
    "\n",
    "\n",
    "# add to pipeline\n",
    "\n",
    "# 1) min price\n",
    "pipeline_min_p = Pipeline(memory=None,\n",
    "              steps=[('preprocessor', preprocessor),\n",
    "                     ('regressor', model_min_p)],\n",
    "              verbose=True)\n",
    "\n",
    "# 2) min price\n",
    "pipeline_max_p = Pipeline(memory=None,\n",
    "              steps=[('preprocessor', preprocessor),\n",
    "                     ('regressor', model_max_p)],\n",
    "              verbose=True)\n",
    "\n",
    "# again add transformer for target variable\n",
    "\n",
    "# 1) min price\n",
    "pipeline_min_p_update = TransformedTargetRegressor(regressor=pipeline_min_p, \n",
    "                                         transformer=transformer_target)\n",
    "# 2) max price\n",
    "pipeline_max_p_update = TransformedTargetRegressor(regressor=pipeline_max_p, \n",
    "                                         transformer=transformer_target)\n",
    "\n",
    "# fit final model on all training data we have at hand\n",
    "\n",
    "# 1) min price\n",
    "weights_train_min_p = weights_samples(y_train.iloc[:,0], order=0)\n",
    "pipeline_min_p = pipeline_min_p_update.fit(X_train, y_train.iloc[:,0])\n",
    "\n",
    "# 2) max price\n",
    "weights_train_max_p = weights_samples(y_train.iloc[:,1], order=0)\n",
    "pipeline_max_p = pipeline_max_p_update.fit(X_train, y_train.iloc[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'minimum price': 125.62047511214338,\n",
       " 'maximum price': 151.68247344955546,\n",
       " 'total error': 277.3029485616988}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# performance on data where the model was fit one (should be very low)\n",
    "pred_train_min_p = pipeline_min_p.predict(X_train)\n",
    "pred_train_max_p = pipeline_max_p.predict(X_train)\n",
    "\n",
    "# calculate performance\n",
    "pred_train = pd.DataFrame([pred_train_min_p,pred_train_max_p]).T\n",
    "calculate_perf(y_train, pred_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Performance on validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'minimum price': 126.50532149437015,\n",
       " 'maximum price': 131.12928454823998,\n",
       " 'total error': 257.63460604261013}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# performance on validation data\n",
    "pred_val_min_p = pipeline_min_p.predict(X_val)\n",
    "pred_val_max_p = pipeline_max_p.predict(X_val)\n",
    "\n",
    "# calculate performance \n",
    "pred_val = pd.DataFrame([pred_val_min_p,pred_val_max_p]).T\n",
    "calculate_perf(y_val, pred_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C) Post processing\n",
    "\n",
    " - inspect predictions/residuals (make visualisations) (See Bram)\n",
    " - feature importance (see Bram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D) Predictions test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum weights: 1.0\n",
      "[Pipeline] ...... (step 1 of 2) Processing preprocessor, total=   0.2s\n",
      "[Pipeline] ......... (step 2 of 2) Processing regressor, total=   3.8s\n",
      "Sum weights: 1.0\n",
      "[Pipeline] ...... (step 1 of 2) Processing preprocessor, total=   0.1s\n",
      "[Pipeline] ......... (step 2 of 2) Processing regressor, total=   3.7s\n"
     ]
    }
   ],
   "source": [
    "# train your final model on all data with best parameters \n",
    "\n",
    "# 1) min price\n",
    "model_min_p_final = GradientBoostingRegressor(\n",
    "     criterion='friedman_mse',\n",
    "     n_estimators=2000,\n",
    "     learning_rate=0.005,\n",
    "     loss = 'lad',\n",
    "     max_depth=56,\n",
    "     max_features='sqrt',\n",
    "     n_iter_no_change = 50,\n",
    "     min_samples_split=0.3,\n",
    "     min_samples_leaf=0.01,\n",
    "     random_state=0\n",
    ")\n",
    "\n",
    "# 2)  min price\n",
    "model_max_p_final = GradientBoostingRegressor(\n",
    "     criterion='friedman_mse',\n",
    "     n_estimators=2000, \n",
    "     learning_rate=0.005,\n",
    "     loss = 'lad',\n",
    "     max_depth=56,\n",
    "     max_features='sqrt',\n",
    "     n_iter_no_change = 50,\n",
    "     min_samples_split=0.3,\n",
    "     min_samples_leaf=0.01,\n",
    "     random_state=0\n",
    ")\n",
    "\n",
    "\n",
    "# add to pipeline\n",
    "\n",
    "# 1) min price\n",
    "pipeline_min_p_final = Pipeline(memory=None,\n",
    "              steps=[('preprocessor', preprocessor),\n",
    "                     ('regressor', model_min_p_final)],\n",
    "              verbose=True)\n",
    "\n",
    "# 2) min price\n",
    "pipeline_max_p_final = Pipeline(memory=None,\n",
    "              steps=[('preprocessor', preprocessor),\n",
    "                     ('regressor', model_max_p_final)],\n",
    "              verbose=True)\n",
    "\n",
    "# again add transformer for target variable\n",
    "\n",
    "# 1) min price\n",
    "pipeline_min_p = TransformedTargetRegressor(regressor=pipeline_min_p_final, \n",
    "                                         transformer=transformer_target)\n",
    "# 2) max price\n",
    "pipeline_max_p = TransformedTargetRegressor(regressor=pipeline_max_p_final, \n",
    "                                         transformer=transformer_target)\n",
    "\n",
    "# fit final model on all training data we have at hand\n",
    "\n",
    "# 1) min price\n",
    "weights_all_train_min_p = weights_samples(y_all_train.iloc[:,0], order=2)\n",
    "pipeline_min_p_final = pipeline_min_p.fit(X_all_train, y_all_train.iloc[:,0])\n",
    "\n",
    "# 2) max price\n",
    "weights_all_train_max_p = weights_samples(y_all_train.iloc[:,1], order=2)\n",
    "pipeline_max_p_final = pipeline_max_p.fit(X_all_train, y_all_train.iloc[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'minimum price': 121.02430493986304,\n",
       " 'maximum price': 126.12226313119426,\n",
       " 'total error': 247.1465680710573}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# performance on all data where the model was fit one (should be very low)\n",
    "pred_all_train_min_p = pipeline_min_p_final.predict(X_all_train)\n",
    "pred_all_train_max_p = pipeline_max_p_final.predict(X_all_train)\n",
    "\n",
    "# calculate performance\n",
    "pred_all_train = pd.DataFrame([pred_all_train_min_p, pred_all_train_max_p]).T\n",
    "calculate_perf(y_all_train, pred_all_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions on test data\n",
    "pred_test_min_p = pipeline_min_p_final.predict(X_test)\n",
    "pred_test_max_p = pipeline_max_p_final.predict(X_test)\n",
    "\n",
    "# submission format\n",
    "submission_format = pd.DataFrame.from_dict(\n",
    " {'ID':df_test['id'].values,\n",
    " 'MIN':pred_test_min_p,\n",
    " 'MAX':pred_test_max_p}).set_index('ID')\n",
    "\n",
    "# write to csv\n",
    "submission_format.to_csv('code\\\\PieterJan\\\\python\\\\output\\\\submission\\\\boosting_python.csv' ,\n",
    "                            header=True, index=True, sep=',')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
