{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# auto reload libraries (you do need to re-import libraries if you make changes)\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# base \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "import os\n",
    "\n",
    "# preprocessing \n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.experimental import enable_iterative_imputer \n",
    "from sklearn.impute import SimpleImputer, IterativeImputer,KNNImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, RobustScaler, MinMaxScaler, MaxAbsScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "# variable importance\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# models\n",
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "# visualize\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# own defined functions/classes \n",
    "from preprocessing.preprocess_pipe import MakeLowerCase\n",
    "from preprocessing.preprocess_pipe import HdResolutionCategorizer\n",
    "from preprocessing.preprocess_pipe import StorageCategorizer\n",
    "from preprocessing.preprocess_pipe import SsdCategorizer\n",
    "\n",
    "from preprocessing.preprocess_pipe import print_missing\n",
    "from preprocessing.preprocess_pipe import calculate_perf\n",
    "from preprocessing.preprocess_pipe import custom_scoring_func_single_p\n",
    "\n",
    "from modelling.weight_samples import weights_samples\n",
    "\n",
    "from postprocessing.postprocessing import plot_predictions_results\n",
    "from postprocessing.postprocessing import plot_residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Pieter-Jan\\Documents\\KuLeuven\\Semester2\\AA\\AAForB_Assignment2\n"
     ]
    }
   ],
   "source": [
    "# global parameters\n",
    "pd.set_option('display.max_columns', 500)\n",
    "# style for plotting\n",
    "plt.style.use('ggplot')\n",
    "# interactive plotting\n",
    "%matplotlib widget\n",
    "# run grid search\n",
    "RUN_GRID_SEARCH = True\n",
    "# set working directory\n",
    "uppath = lambda _path, n: os.sep.join(_path.split(os.sep)[:-n])\n",
    "__file__ = 'C:\\\\Users\\\\Pieter-Jan\\\\Documents\\\\KuLeuven\\\\Semester2\\\\AA\\\\AAForB_Assignment2\\\\code\\\\PieterJan'\n",
    "out = uppath(__file__, 2)\n",
    "os.chdir(out)\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: import data and split in train and validation set\n",
    "The validation set is more our own kind of test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions of all training data (510, 22)\n",
      "Dimension test data (222, 20)\n"
     ]
    }
   ],
   "source": [
    "# read in trainig and validation data\n",
    "# use the same data split as we did in R\n",
    "df_all_train = pd.read_csv(\"data\\\\train.csv\", sep=',')\n",
    "df_test = pd.read_csv(\"data\\\\test.csv\", sep=',')\n",
    "\n",
    "print(f'Dimensions of all training data {df_all_train.shape}')\n",
    "print(f'Dimension test data {df_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split in training and validation set\n",
    "df_train = df_all_train.sample(frac=0.75, random_state=0, replace=False)\n",
    "df_val = df_all_train.drop(df_train.index)\n",
    "\n",
    "# reset index, if you don't resit missing rows get inserted in the pipeline\n",
    "# see: https://stackoverflow.com/questions/31323499/sklearn-error-valueerror-input-contains-nan-infinity-or-a-value-too-large-for\n",
    "df_train = df_train.reset_index().drop('index',axis=1)\n",
    "df_val = df_val.reset_index().drop('index',axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: define pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Numerical features to pass down the numerical pipeline \n",
    "numerical_features = ['screen_size' ,'pixels_x','pixels_y',\n",
    "                      'ram', 'weight','ssd','storage']\n",
    "\n",
    "#Categrical features to pass down the categorical pipeline \n",
    "categorical_features = ['brand','base_name', 'screen_surface','touchscreen',\n",
    "                        'cpu','cpu_details','detachable_keyboard',\n",
    "                        'discrete_gpu','gpu', 'os','os_details']\n",
    "\n",
    "# define all unique features\n",
    "features = np.unique(numerical_features).tolist() + np.unique(categorical_features).tolist()\n",
    "\n",
    "# target variables\n",
    "target = ['min_price','max_price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "X_train = df_train[features]\n",
    "y_train = df_train[target]\n",
    "\n",
    "# validation (this is kind our own test set)\n",
    "X_val  = df_val[features]\n",
    "y_val = df_val[target]\n",
    "\n",
    "# train_validation (this is all training data we have) for fitting the model\n",
    "X_all_train = df_all_train[features]\n",
    "y_all_train = df_all_train[target]\n",
    "\n",
    "# test\n",
    "X_test = df_test[features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can add many more and \n",
    "# you can even define custom preprocessing steps like 'MakeLowerCase()'\n",
    "\n",
    "# pipeline  uses only numerical features,\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    #('imputer', SimpleImputer(missing_values=np.nan, strategy='mean')),\n",
    "    #('imputer', KNNImputer(n_neighbors=5, weights='uniform',metric='nan_euclidean')),\n",
    "    ('imputer', IterativeImputer(missing_values=np.nan, random_state=0, n_nearest_features=None, sample_posterior=True)),\n",
    "    ('transformation', PowerTransformer(method='yeo-johnson',standardize=False)),\n",
    "    #('quantile_trans', QuantileTransformer(output_distribution='normal', random_state=0)),\n",
    "    #('scaler1', RobustScaler(with_centering=True, with_scaling=True, quantile_range=(10.0, 90.0))),\n",
    "    ('poly', PolynomialFeatures(5)),\n",
    "    #Scale features using statistics that are robust to outliers.\n",
    "    ('scaler2', RobustScaler(with_centering=True, with_scaling=True, quantile_range=(10.0, 90.0)))])\n",
    "    #('scaler', StandardScaler())])\n",
    "    #('scaler', MinMaxScaler())])\n",
    "\n",
    "# pipeline use only categorical features\n",
    "categorical_transformer = Pipeline(steps=[ \n",
    "    ('lowercase', MakeLowerCase()), # lower cases all columns containing strings\n",
    "    #('sd_category' ,SsdCategorizer(drop_original_feature=True)),\n",
    "    #('storage_category', StorageCategorizer(drop_original_feature=True)),\n",
    "    ('imputer', SimpleImputer(missing_values=np.nan, strategy=\"most_frequent\")),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "    # ('scaler', MaxAbsScaler()),\n",
    "    #('poly', PolynomialFeatures(2))])\n",
    "\n",
    "# add both preprocessing pipelines in one pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numerical_features),\n",
    "        ('cat', categorical_transformer, categorical_features)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(382, 1343)"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is how the preporcessed pipeline looks like (just to have an idea)\n",
    "pd.DataFrame(preprocessor.fit_transform(X_train)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: add models to pipeline\n",
    "\n",
    "Possible to fit multiple target variabels, so you **don't** need to fit a different models for min. price and max. price\n",
    "\n",
    "### A) Training and parameter tuning\n",
    "\n",
    "##### 1) Automatic tuning via grid search\n",
    "\n",
    "I will only **do the tuning for the minimum price** and use the found parameters also for the maximum price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters currently in use:\n",
      "\n",
      "{'alpha': 1.0,\n",
      " 'copy_X': True,\n",
      " 'fit_intercept': False,\n",
      " 'l1_ratio': 0.5,\n",
      " 'max_iter': 200,\n",
      " 'normalize': True,\n",
      " 'positive': False,\n",
      " 'precompute': False,\n",
      " 'random_state': 0,\n",
      " 'selection': 'cyclic',\n",
      " 'tol': 0.0001,\n",
      " 'warm_start': False}\n"
     ]
    }
   ],
   "source": [
    "# define model: I just add some default parameters but you could\n",
    "# also just write: RandomForestRegressor() since we will perform a grid search \n",
    "# to find good hyperparameter values\n",
    "model_min_p = ElasticNet(random_state=0, max_iter = 200, normalize=True, fit_intercept=False)\n",
    "model_max_p = ElasticNet(random_state=0, max_iter = 200, normalize=True, fit_intercept=False)\n",
    "\n",
    "# Look at parameters used by our current forest\n",
    "print('Parameters currently in use:\\n')\n",
    "pprint(model_min_p.get_params())\n",
    "\n",
    "# add to pipeline\n",
    "# 1) min price\n",
    "pipeline_min_p = Pipeline(memory=None,\n",
    "              steps=[('preprocessor', preprocessor),\n",
    "                     ('regressor', model_min_p)])\n",
    "\n",
    "\n",
    "# add transformation on the target variable, by default power transformation \n",
    "# also performs standardization after performing the power transformation\n",
    "# and back transform to the original space when outputting predictions \n",
    "\n",
    "# 1) min price\n",
    "transformer_target = PowerTransformer(method='box-cox',standardize=False)\n",
    "scale_target = RobustScaler(with_centering=True, with_scaling=True, quantile_range=(10.0, 90.0))\n",
    "pipeline_y = Pipeline(memory=None,\n",
    "              steps=[('transformer', transformer_target)])\n",
    "\n",
    "pipeline_min_p_update = TransformedTargetRegressor(regressor=pipeline_min_p, \n",
    "                                         transformer=pipeline_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alpha float, optional\n",
    "# Constant that multiplies the penalty terms. \n",
    "# Defaults to 1.0. See the notes for the exact mathematical meaning of this parameter. \n",
    "# alpha = 0 is equivalent to an ordinary least square, solved by the LinearRegression object. \n",
    "# For numerical reasons, using alpha = 0 with the Lasso object is not advised. Given this, \n",
    "# you should use the LinearRegression object.\n",
    "alpha = np.linspace(0, 20, num=100)\n",
    "\n",
    "# l1_ratio float\n",
    "# The ElasticNet mixing parameter, with 0 <= l1_ratio <= 1. \n",
    "# For l1_ratio = 0 the penalty is an L2 penalty. For l1_ratio = 1 it is an L1 penalty. \n",
    "# For 0 < l1_ratio < 1, the penalty is a combination of L1 and L2.\n",
    "\n",
    "l1_ratio = np.linspace(0, 1, num=100)\n",
    "\n",
    "# Create the random grid\n",
    "random_grid = {\n",
    "   'regressor__regressor__alpha': alpha,\n",
    "   'regressor__regressor__l1_ratio': l1_ratio\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum weights: 1.0\n",
      "Fitting 5 folds for each of 200 candidates, totalling 1000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:    7.1s\n",
      "[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed:   25.2s\n",
      "[Parallel(n_jobs=-1)]: Done 357 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 640 tasks      | elapsed:  3.5min\n",
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:  6.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'regressor__regressor__l1_ratio': 0.0, 'regressor__regressor__alpha': 15.353535353535353}\n",
      "-256.1996811903008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Pieter-Jan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:460: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 11.849230242193624, tolerance: 0.0053125668153518775\n",
      "  max_iter, tol, rng, random, positive)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_regressor__regressor__l1_ratio</th>\n",
       "      <th>param_regressor__regressor__alpha</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>2.884872</td>\n",
       "      <td>0.074443</td>\n",
       "      <td>0.104501</td>\n",
       "      <td>0.054268</td>\n",
       "      <td>0</td>\n",
       "      <td>15.3535</td>\n",
       "      <td>{'regressor__regressor__l1_ratio': 0.0, 'regre...</td>\n",
       "      <td>-290.327729</td>\n",
       "      <td>-229.367487</td>\n",
       "      <td>-338.702944</td>\n",
       "      <td>-219.053120</td>\n",
       "      <td>-203.547127</td>\n",
       "      <td>-256.199681</td>\n",
       "      <td>50.682614</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>3.698143</td>\n",
       "      <td>0.092847</td>\n",
       "      <td>0.108479</td>\n",
       "      <td>0.044939</td>\n",
       "      <td>0</td>\n",
       "      <td>17.9798</td>\n",
       "      <td>{'regressor__regressor__l1_ratio': 0.0, 'regre...</td>\n",
       "      <td>-296.319075</td>\n",
       "      <td>-235.200627</td>\n",
       "      <td>-338.248374</td>\n",
       "      <td>-223.983783</td>\n",
       "      <td>-207.930995</td>\n",
       "      <td>-260.336571</td>\n",
       "      <td>49.121635</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.770756</td>\n",
       "      <td>0.032684</td>\n",
       "      <td>0.033913</td>\n",
       "      <td>0.009972</td>\n",
       "      <td>0.30303</td>\n",
       "      <td>0.20202</td>\n",
       "      <td>{'regressor__regressor__l1_ratio': 0.303030303...</td>\n",
       "      <td>-345.153767</td>\n",
       "      <td>-270.898565</td>\n",
       "      <td>-305.363196</td>\n",
       "      <td>-248.432002</td>\n",
       "      <td>-242.375986</td>\n",
       "      <td>-282.444703</td>\n",
       "      <td>38.349350</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>2.281450</td>\n",
       "      <td>0.064081</td>\n",
       "      <td>0.111232</td>\n",
       "      <td>0.045028</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.40404</td>\n",
       "      <td>{'regressor__regressor__l1_ratio': 0.181818181...</td>\n",
       "      <td>-387.845010</td>\n",
       "      <td>-301.602924</td>\n",
       "      <td>-332.642130</td>\n",
       "      <td>-280.394639</td>\n",
       "      <td>-276.777349</td>\n",
       "      <td>-315.852410</td>\n",
       "      <td>41.114452</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>2.133853</td>\n",
       "      <td>0.082023</td>\n",
       "      <td>0.121580</td>\n",
       "      <td>0.033310</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.40404</td>\n",
       "      <td>{'regressor__regressor__l1_ratio': 0.222222222...</td>\n",
       "      <td>-437.808969</td>\n",
       "      <td>-335.508943</td>\n",
       "      <td>-367.733986</td>\n",
       "      <td>-310.874099</td>\n",
       "      <td>-316.301105</td>\n",
       "      <td>-353.645421</td>\n",
       "      <td>46.550846</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "129       2.884872      0.074443         0.104501        0.054268   \n",
       "178       3.698143      0.092847         0.108479        0.044939   \n",
       "25        0.770756      0.032684         0.033913        0.009972   \n",
       "91        2.281450      0.064081         0.111232        0.045028   \n",
       "90        2.133853      0.082023         0.121580        0.033310   \n",
       "\n",
       "    param_regressor__regressor__l1_ratio param_regressor__regressor__alpha  \\\n",
       "129                                    0                           15.3535   \n",
       "178                                    0                           17.9798   \n",
       "25                               0.30303                           0.20202   \n",
       "91                              0.181818                           0.40404   \n",
       "90                              0.222222                           0.40404   \n",
       "\n",
       "                                                params  split0_test_score  \\\n",
       "129  {'regressor__regressor__l1_ratio': 0.0, 'regre...        -290.327729   \n",
       "178  {'regressor__regressor__l1_ratio': 0.0, 'regre...        -296.319075   \n",
       "25   {'regressor__regressor__l1_ratio': 0.303030303...        -345.153767   \n",
       "91   {'regressor__regressor__l1_ratio': 0.181818181...        -387.845010   \n",
       "90   {'regressor__regressor__l1_ratio': 0.222222222...        -437.808969   \n",
       "\n",
       "     split1_test_score  split2_test_score  split3_test_score  \\\n",
       "129        -229.367487        -338.702944        -219.053120   \n",
       "178        -235.200627        -338.248374        -223.983783   \n",
       "25         -270.898565        -305.363196        -248.432002   \n",
       "91         -301.602924        -332.642130        -280.394639   \n",
       "90         -335.508943        -367.733986        -310.874099   \n",
       "\n",
       "     split4_test_score  mean_test_score  std_test_score  rank_test_score  \n",
       "129        -203.547127      -256.199681       50.682614                1  \n",
       "178        -207.930995      -260.336571       49.121635                2  \n",
       "25         -242.375986      -282.444703       38.349350                3  \n",
       "91         -276.777349      -315.852410       41.114452                4  \n",
       "90         -316.301105      -353.645421       46.550846                5  "
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define random search (and narrow down time grid search)\n",
    "if RUN_GRID_SEARCH:\n",
    "    min_p_random_search = RandomizedSearchCV(\n",
    "       estimator = pipeline_min_p_update, \n",
    "       param_distributions = random_grid, n_iter = 200,\n",
    "       cv = 5, verbose=2, random_state=1, n_jobs = -1, refit=True,\n",
    "       scoring=make_scorer(custom_scoring_func_single_p, greater_is_better=False)\n",
    "    )\n",
    "\n",
    "    # run grid search and refit with best hyper parameters\n",
    "    weights_train_min_p =  weights_samples(df=y_train.iloc[:,0], order=0, plot_weights=False)\n",
    "    min_p_random_search.fit(X_train, y_train.iloc[:,0])  \n",
    "    print(min_p_random_search.best_params_)    \n",
    "    print(min_p_random_search.best_score_)\n",
    "\n",
    "# have look at the best hyperparameters and their respective performance (maybe also look at the sd)\n",
    "pd.DataFrame(min_p_random_search.cv_results_).sort_values(\n",
    "        by=['mean_test_score'],ascending=False).head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2) Manual parameters tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum weights: 1.0\n",
      "[Pipeline] ...... (step 1 of 2) Processing preprocessor, total=   0.3s\n",
      "[Pipeline] ......... (step 2 of 2) Processing regressor, total=   0.1s\n",
      "Sum weights: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Pieter-Jan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.636016184208529, tolerance: 0.21098206608631992\n",
      "  positive)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ...... (step 1 of 2) Processing preprocessor, total=   0.3s\n",
      "[Pipeline] ......... (step 2 of 2) Processing regressor, total=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Pieter-Jan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.355938676183996, tolerance: 0.17615025023290815\n",
      "  positive)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TransformedTargetRegressor(check_inverse=True, func=None, inverse_func=None,\n",
       "                           regressor=Pipeline(memory=None,\n",
       "                                              steps=[('preprocessor',\n",
       "                                                      ColumnTransformer(n_jobs=None,\n",
       "                                                                        remainder='drop',\n",
       "                                                                        sparse_threshold=0.3,\n",
       "                                                                        transformer_weights=None,\n",
       "                                                                        transformers=[('num',\n",
       "                                                                                       Pipeline(memory=None,\n",
       "                                                                                                steps=[('imputer',\n",
       "                                                                                                        IterativeImputer(add_indicator=False,\n",
       "                                                                                                                         estimator=None,\n",
       "                                                                                                                         imputation_order=...\n",
       "                                                      ElasticNet(alpha=0.202,\n",
       "                                                                 copy_X=True,\n",
       "                                                                 fit_intercept=True,\n",
       "                                                                 l1_ratio=0.30303030303030304,\n",
       "                                                                 max_iter=200,\n",
       "                                                                 normalize=False,\n",
       "                                                                 positive=False,\n",
       "                                                                 precompute=False,\n",
       "                                                                 random_state=0,\n",
       "                                                                 selection='cyclic',\n",
       "                                                                 tol=0.0001,\n",
       "                                                                 warm_start=False))],\n",
       "                                              verbose=True),\n",
       "                           transformer=Pipeline(memory=None,\n",
       "                                                steps=[('transformer',\n",
       "                                                        PowerTransformer(copy=True,\n",
       "                                                                         method='box-cox',\n",
       "                                                                         standardize=False))],\n",
       "                                                verbose=False))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train your final model on all data with best parameters \n",
    "\n",
    "# 1) min price\n",
    "model_min_p = ElasticNet(\n",
    "    random_state=0, \n",
    "    max_iter = 200,\n",
    "    l1_ratio = 0.30303030303030304,\n",
    "    alpha = 0.2020\n",
    ")\n",
    "\n",
    "# 2)  min price\n",
    "model_max_p = ElasticNet(\n",
    "    random_state=0, \n",
    "    max_iter = 200,\n",
    "    l1_ratio = 0.30303030303030304,\n",
    "    alpha = 0.2020\n",
    ")\n",
    "\n",
    "\n",
    "# add to pipeline\n",
    "\n",
    "# 1) min price\n",
    "pipeline_min_p = Pipeline(memory=None,\n",
    "              steps=[('preprocessor', preprocessor),\n",
    "                     ('regressor', model_min_p)],\n",
    "              verbose=True)\n",
    "\n",
    "# 2) min price\n",
    "pipeline_max_p = Pipeline(memory=None,\n",
    "              steps=[('preprocessor', preprocessor),\n",
    "                     ('regressor', model_max_p)],\n",
    "              verbose=True)\n",
    "\n",
    "# again add transformer for target variable\n",
    "\n",
    "# 1) min price\n",
    "pipeline_min_p_update = TransformedTargetRegressor(regressor=pipeline_min_p, \n",
    "                                         transformer=pipeline_y)\n",
    "# 2) max price\n",
    "pipeline_max_p_update = TransformedTargetRegressor(regressor=pipeline_max_p, \n",
    "                                         transformer=pipeline_y)\n",
    "\n",
    "# fit final model on all training data we have at hand\n",
    "\n",
    "# 1) min price\n",
    "weights_train_min_p = weights_samples(y_train.iloc[:,0], order=0)\n",
    "pipeline_min_p_update.fit(X_train, y_train.iloc[:,0])\n",
    "\n",
    "# 2) max price\n",
    "weights_train_max_p = weights_samples(y_train.iloc[:,1], order=0)\n",
    "pipeline_max_p_update.fit(X_train, y_train.iloc[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'minimum price': 191.33729085074108,\n",
       " 'maximum price': 201.5652288780467,\n",
       " 'total error': 392.90251972878775}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# performance on data where the model was fit one (should be very low)\n",
    "pred_train_min_p = pipeline_min_p_update.predict(X_train)\n",
    "pred_train_max_p = pipeline_max_p_update.predict(X_train)\n",
    "\n",
    "# calculate performance\n",
    "pred_train = pd.DataFrame([pred_train_min_p,pred_train_max_p]).T\n",
    "calculate_perf(y_train, pred_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Performance on validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'minimum price': 141.10558650023438,\n",
       " 'maximum price': 145.89398976010645,\n",
       " 'total error': 286.99957626034086}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# performance on validation data\n",
    "pred_val_min_p = pipeline_min_p_update.predict(X_val)\n",
    "pred_val_max_p = pipeline_max_p_update.predict(X_val)\n",
    "\n",
    "# calculate performance \n",
    "pred_val = pd.DataFrame([pred_val_min_p,pred_val_max_p]).T\n",
    "calculate_perf(y_val, pred_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save predictions on validation data\n",
    "# submission format\n",
    "submission_format_validation = pd.DataFrame.from_dict(\n",
    " {'ID':df_val['id'].values,\n",
    " 'MIN':pred_val_min_p,\n",
    " 'MAX':pred_val_max_p}).set_index('ID')\n",
    "\n",
    "# write to csv\n",
    "submission_format_validation.to_csv('code\\\\PieterJan\\\\python\\\\output\\\\validation\\\\elastic_net.csv' ,\n",
    "                            header=True, index=True, sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C) Post processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "996ee46344784101942e70920ed95963",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = plt.subplots(2, 2, figsize=(9, 7))\n",
    "axs = np.ravel(axs)\n",
    "\n",
    "# fitted against true predictions minimum price\n",
    "plot_predictions_results(ax=axs[0], \n",
    "                        y_true=y_val.iloc[:,0], \n",
    "                        y_pred=pred_val_min_p, \n",
    "                        title=\"Enet Min. Price Test Set\", \n",
    "                        log_scale=True)\n",
    "\n",
    "# fitted against true predictions maximum price\n",
    "plot_predictions_results(ax=axs[1], \n",
    "                        y_true=y_val.iloc[:,1], \n",
    "                        y_pred=pred_val_max_p, \n",
    "                        title=\"Enet Max. Price Test Set\", \n",
    "                        log_scale=True)\n",
    "\n",
    "# residuals minimum price\n",
    "plot_residuals(ax=axs[2], \n",
    "               y_true=y_val.iloc[:,0], \n",
    "               y_pred=pred_val_min_p, \n",
    "               title=\"\", \n",
    "               log_scale=False,\n",
    "               order=1)\n",
    "\n",
    "# residuals maximum price\n",
    "plot_residuals(ax=axs[3], \n",
    "               y_true=y_val.iloc[:,1], \n",
    "               y_pred=pred_val_max_p, \n",
    "               title=\"\", \n",
    "               log_scale=False,\n",
    "               order=1)\n",
    "fig.tight_layout()\n",
    "plt.savefig('code\\\\PieterJan\\\\python\\\\output\\\\figures\\\\elastic_net\\\\fig1.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b14ee67d20634fc29798018020c3c585",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(8,4))\n",
    "plt.plot(pred_val_max_p - pred_val_min_p, label=\"Predictions Test\", linestyle=\"--\")\n",
    "plt.plot(y_val.iloc[:,1] - y_val.iloc[:,0], label=\"Truth Test\", linestyle=':')\n",
    "plt.xlabel(\"Observations\")\n",
    "plt.ylabel(\"Max. Price - Min. Price\")\n",
    "plt.legend(loc=\"upper right\");\n",
    "plt.savefig('code\\\\PieterJan\\\\python\\\\output\\\\figures\\\\elastic_net\\\\fig2.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "vip_val_min_p = permutation_importance(pipeline_min_p_update, X_val, y_val.iloc[:,0], n_repeats=10,\n",
    "                                random_state=1, n_jobs=3)\n",
    "\n",
    "vip_val_max_p = permutation_importance(pipeline_min_p_update, X_val, y_val.iloc[:,1], n_repeats=10,\n",
    "                                random_state=1, n_jobs=3)\n",
    "\n",
    "sorted_idx_min_p = vip_val_min_p.importances_mean.argsort()\n",
    "sorted_idx_max_p = vip_val_max_p.importances_mean.argsort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2904550f7d874e9fb8986294f58b07d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(9, 4))\n",
    "ax = np.ravel(ax)\n",
    "# minimum price\n",
    "ax[0].boxplot(vip_val_min_p.importances[sorted_idx_min_p].T,\n",
    "           vert=False, labels=X_val.columns[sorted_idx_min_p])\n",
    "ax[0].set_title(\"Permutation Importances: Min. Price (Test set)\", fontsize=9)\n",
    "ax[0].xaxis.set_tick_params(labelsize=8)\n",
    "ax[0].yaxis.set_tick_params(labelsize=8)\n",
    "\n",
    "# maximum price\n",
    "ax[1].boxplot(vip_val_min_p.importances[sorted_idx_max_p].T,\n",
    "           vert=False, labels=X_val.columns[sorted_idx_max_p])\n",
    "ax[1].set_title(\"Permutation Importances: Max. Price (Test set)\", fontsize=9)\n",
    "ax[1].xaxis.set_tick_params(labelsize=8)\n",
    "ax[1].yaxis.set_tick_params(labelsize=8)\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "plt.savefig('code\\\\PieterJan\\\\python\\\\output\\\\figures\\\\elastic_net\\\\fig3.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D) Predictions test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum weights: 1.0\n",
      "[Pipeline] ...... (step 1 of 2) Processing preprocessor, total=   0.2s\n",
      "[Pipeline] ......... (step 2 of 2) Processing regressor, total=   0.0s\n",
      "Sum weights: 1.0\n",
      "[Pipeline] ...... (step 1 of 2) Processing preprocessor, total=   0.2s\n",
      "[Pipeline] ......... (step 2 of 2) Processing regressor, total=   0.0s\n"
     ]
    }
   ],
   "source": [
    "# train your final model on all data with best parameters \n",
    "\n",
    "# 1) min price\n",
    "model_min_p_final = ElasticNet(\n",
    "    random_state=0, \n",
    "    max_iter = 200,\n",
    "    l1_ratio = 0.30303030303030304,\n",
    "    alpha = 0.2020\n",
    ")\n",
    "\n",
    "# 2)  min price\n",
    "model_max_p_final = ElasticNet(\n",
    "       random_state=0, \n",
    "    max_iter = 200,\n",
    "    l1_ratio = 0.30303030303030304,\n",
    "    alpha = 0.2020\n",
    ")\n",
    "\n",
    "\n",
    "# add to pipeline\n",
    "\n",
    "# 1) min price\n",
    "pipeline_min_p_final = Pipeline(memory=None,\n",
    "              steps=[('preprocessor', preprocessor),\n",
    "                     ('regressor', model_min_p_final)],\n",
    "              verbose=True)\n",
    "\n",
    "# 2) min price\n",
    "pipeline_max_p_final = Pipeline(memory=None,\n",
    "              steps=[('preprocessor', preprocessor),\n",
    "                     ('regressor', model_max_p_final)],\n",
    "              verbose=True)\n",
    "\n",
    "# again add transformer for target variable\n",
    "\n",
    "# 1) min price\n",
    "pipeline_min_p_final = TransformedTargetRegressor(regressor=pipeline_min_p_final, \n",
    "                                         transformer=pipeline_y)\n",
    "# 2) max price\n",
    "pipeline_max_p_final = TransformedTargetRegressor(regressor=pipeline_max_p_final, \n",
    "                                         transformer=pipeline_y)\n",
    "\n",
    "# fit final model on all training data we have at hand\n",
    "\n",
    "# 1) min price\n",
    "weights_all_train_min_p = weights_samples(y_all_train.iloc[:,0], order=2)\n",
    "pipeline_min_p_final.fit(X_train, y_train.iloc[:,0])\n",
    "\n",
    "# 2) max price\n",
    "weights_all_train_max_p = weights_samples(y_train.iloc[:,1], order=2)\n",
    "pipeline_max_p_final.fit(X_train, y_train.iloc[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'minimum price': 219.88151889556264,\n",
       " 'maximum price': 228.71350557672497,\n",
       " 'total error': 448.5950244722876}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# performance on all data where the model was fit one (should be very low)\n",
    "pred_all_train_min_p = pipeline_min_p_final.predict(X_all_train)\n",
    "pred_all_train_max_p = pipeline_max_p_final.predict(X_all_train)\n",
    "\n",
    "# calculate performance \n",
    "pred_all_train = pd.DataFrame([pred_all_train_min_p, pred_all_train_max_p]).T\n",
    "calculate_perf(y_all_train, pred_all_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions on test data\n",
    "pred_test_min_p = pipeline_min_p_final.predict(X_test)\n",
    "pred_test_max_p = pipeline_max_p_final.predict(X_test)\n",
    "\n",
    "# submission format\n",
    "submission_format = pd.DataFrame.from_dict(\n",
    " {'ID':df_test['id'].values,\n",
    " 'MIN':pred_test_min_p,\n",
    " 'MAX':pred_test_max_p}).set_index('ID')\n",
    "\n",
    "# write to csv\n",
    "submission_format.to_csv('code\\\\PieterJan\\\\python\\\\output\\\\submission\\\\elastic_net.csv' ,\n",
    "                            header=True, index=True, sep=',')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
