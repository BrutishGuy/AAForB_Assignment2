{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# auto reload libraries (you do need to re-import libraries if you make changes)\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# base \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "import os\n",
    "\n",
    "# preprocessing \n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.experimental import enable_iterative_imputer \n",
    "from sklearn.impute import SimpleImputer, IterativeImputer,KNNImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, RobustScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "# variable importance\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# visualize\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# models\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# own defined functions/classes \n",
    "from preprocessing.preprocess_pipe import MakeLowerCase\n",
    "from preprocessing.preprocess_pipe import HdResolutionCategorizer\n",
    "from preprocessing.preprocess_pipe import StorageCategorizer\n",
    "from preprocessing.preprocess_pipe import SsdCategorizer\n",
    "\n",
    "from preprocessing.preprocess_pipe import print_missing\n",
    "from preprocessing.preprocess_pipe import calculate_perf\n",
    "from preprocessing.preprocess_pipe import custom_scoring_func\n",
    "\n",
    "from modelling.weight_samples import weights_samples\n",
    "\n",
    "from postprocessing.postprocessing import plot_predictions_results\n",
    "from postprocessing.postprocessing import plot_residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Pieter-Jan\\Documents\\KuLeuven\\Semester2\\AA\\AAForB_Assignment2\n"
     ]
    }
   ],
   "source": [
    "# global parameters\n",
    "pd.set_option('display.max_columns', 500)\n",
    "# style for plotting\n",
    "# interactive plotting\n",
    "# %matplotlib widget\n",
    "# run grid search\n",
    "RUN_GRID_SEARCH = True\n",
    "# style for plotting\n",
    "plt.style.use('ggplot')\n",
    "current_palette = sns.color_palette(\"colorblind\")\n",
    "sns.palplot(current_palette)\n",
    "sns.set_palette(\"colorblind\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO\n",
    "\n",
    "### PLEASE DEFINE FUNCTIONS IN SEPERATE FILES TO KEEP THE NOTEBOOK CLEAN\n",
    "\n",
    "- Add custum scoring metric: **DONE**\n",
    "- further integrate preprocessing steps from Arnaud **DONE**\n",
    "- Integerate steps from Victor **Partly DONE**\n",
    "- Look at comments of Victor for next steps\n",
    "- Look at feature engineering ideas\n",
    "- Integrate steps from Bram \n",
    "- Integrate custum scoring metric (also when performing the cross validation/grid search) **DONE**\n",
    "- Make function that checkts whether prediction of the maximum price are >= predictions of the minimum price\n",
    "- Investigate why there is such a huge difference between the performance on 'our test set' and the performance of submission test set\n",
    "- Make function that saves trained models\n",
    "- Try out other models (Boosting, Support Vector Machines, Penalized Linear models like lasso, stacked models ...)\n",
    "- Implement better missing values imputations methods **DONE**\n",
    "- Add some post processing visualizations:\n",
    "    - feature importance plots\n",
    "    - look at our predictions visually (do they make sense?)\n",
    "    - look at the residuals\n",
    "    - Have a look at this especially section 5 for model interpretability ideas\n",
    "    https://christophm.github.io/interpretable-ml-book/pdp.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: import data and split in train and validation set\n",
    "The validation set is more our own kind of test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions of all training data (510, 22)\n",
      "Dimension test data (222, 20)\n"
     ]
    }
   ],
   "source": [
    "# read in trainig and validation data\n",
    "df_all_train = pd.read_csv(\"../../../data/train.csv\", sep=',')\n",
    "df_test = pd.read_csv(\"../../../data/test.csv\", sep=',')\n",
    "\n",
    "print(f'Dimensions of all training data {df_all_train.shape}')\n",
    "print(f'Dimension test data {df_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split in training and validation set\n",
    "df_train = df_all_train.sample(frac=0.75, random_state=0, replace=False)\n",
    "df_val = df_all_train.drop(df_train.index)\n",
    "\n",
    "# reset index, if you don't resit missing rows get inserted in the pipeline\n",
    "# see: https://stackoverflow.com/questions/31323499/sklearn-error-valueerror-input-contains-nan-infinity-or-a-value-too-large-for\n",
    "df_train = df_train.reset_index().drop('index',axis=1)\n",
    "df_val = df_val.reset_index().drop('index',axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: define pipeline\n",
    "\n",
    "The pipeline should handle all steps performed on the data from data cleaning till the acutal model prediction. \n",
    "It should also be fairly easily to apply this to the test data. Therefore we make use of the scikit learn pipeline which allows all of this functionality. You can also implement your own `custom` transformations.\n",
    "\n",
    "For more information on how to use and why using pipelines see:\n",
    " - https://medium.com/vickdata/a-simple-guide-to-scikit-learn-pipelines-4ac0d974bdcf \n",
    " - scikit learn api on pipelines: https://scikit-learn.org/stable/modules/compose.html\n",
    " - Scikit learn api for more preprocessing steps: https://scikit-learn.org/stable/modules/preprocessing.html#non-linear-transformation\n",
    "\n",
    "We make a difference between the pre processing steps for\n",
    "- numerical features\n",
    "- categorical features\n",
    "\n",
    "It's important to note that if a feature is in the numerical feature pipeline it can't be in the categorical feature pipeline. Also if some preprocessing steps in the numerical feature pipeline need features from the categorical pipeline you will get an error. All preprocessing steps in each pipeline should be able to perform the transformation only with the categorical or numerical features. \n",
    "\n",
    "You can implement some custom transformation steps like 'MakeLowerCase' ==> see preprocessing.preprocess_pipe for more details or check this article  these two article (also gives some explanation on how to implement pipelines)\n",
    "\n",
    "- https://gist.github.com/amberjrivera/8c5c145516f5a2e894681e16a8095b5c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Numerical features to pass down the numerical pipeline \n",
    "numerical_features = ['screen_size' ,'pixels_x','pixels_y',\n",
    "                      'ram', 'weight','ssd','storage']\n",
    "\n",
    "#Categrical features to pass down the categorical pipeline \n",
    "categorical_features = ['brand','base_name', 'screen_surface','touchscreen',\n",
    "                        'cpu','cpu_details','detachable_keyboard',\n",
    "                        'discrete_gpu','gpu', 'os','os_details']\n",
    "\n",
    "# define all unique features\n",
    "features = np.unique(numerical_features).tolist() + np.unique(categorical_features).tolist()\n",
    "\n",
    "# target variables\n",
    "target = ['min_price','max_price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "X_train = df_train[features]\n",
    "y_train = df_train[target]\n",
    "\n",
    "# validation (this is kind our own test set)\n",
    "X_val  = df_val[features]\n",
    "y_val = df_val[target]\n",
    "\n",
    "# train_validation (this is all training data we have) for fitting the model\n",
    "X_all_train = df_all_train[features]\n",
    "y_all_train = df_all_train[target]\n",
    "\n",
    "# test\n",
    "X_test = df_test[features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can add many more and \n",
    "# you can even define custom preprocessing steps like 'MakeLowerCase()'\n",
    "\n",
    "# pipeline  uses only numerical features,\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    #('imputer', SimpleImputer(missing_values=np.nan, strategy='mean')),\n",
    "    ('imputer', KNNImputer(n_neighbors=5, weights='uniform',metric='nan_euclidean')),\n",
    "    #('imputer', IterativeImputer(missing_values=np.nan, random_state=0, n_nearest_features=None, sample_posterior=True)),\n",
    "    #('transformation', PowerTransformer(method='yeo-johnson',standardize=False)),\n",
    "     #Scale features using statistics that are robust to outliers.\n",
    "    ('scaler', RobustScaler(with_centering=True, with_scaling=True, quantile_range=(25, 75.0)))]) \n",
    "\n",
    "# pipeline use only categorical features\n",
    "categorical_transformer = Pipeline(steps=[ \n",
    "    ('lowercase', MakeLowerCase()), # lower cases all columns containing strings\n",
    "    #('sd_category' ,SsdCategorizer(drop_original_feature=False)),\n",
    "    #('storage_category', StorageCategorizer(drop_original_feature=True)),\n",
    "    ('imputer', SimpleImputer(missing_values=np.nan, strategy=\"most_frequent\")),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "# add both preprocessing pipelines in one pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numerical_features),\n",
    "        ('cat', categorical_transformer, categorical_features)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(382, 1)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is how the preporcessed pipeline looks like (just to have an idea)\n",
    "pd.DataFrame(preprocessor.fit_transform(X_train)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Give more weights to higher priced computers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: add models to pipeline\n",
    "\n",
    "Possible to fit multiple target variabels, so you **don't** need to fit a different models for min. price and max. price\n",
    "\n",
    "### A) Training and parameter tuning\n",
    "\n",
    "##### 1) Automatic tuning via grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters currently in use:\n",
      "\n",
      "{'bootstrap': True,\n",
      " 'ccp_alpha': 0.0,\n",
      " 'criterion': 'mse',\n",
      " 'max_depth': None,\n",
      " 'max_features': 'auto',\n",
      " 'max_leaf_nodes': None,\n",
      " 'max_samples': None,\n",
      " 'min_impurity_decrease': 0.0,\n",
      " 'min_impurity_split': None,\n",
      " 'min_samples_leaf': 1,\n",
      " 'min_samples_split': 2,\n",
      " 'min_weight_fraction_leaf': 0.0,\n",
      " 'n_estimators': 100,\n",
      " 'n_jobs': None,\n",
      " 'oob_score': False,\n",
      " 'random_state': 1,\n",
      " 'verbose': 0,\n",
      " 'warm_start': False}\n"
     ]
    }
   ],
   "source": [
    "# define model: I just add some default parameters but you could\n",
    "# also just write: RandomForestRegressor() since we will perform a grid search \n",
    "# to find good hyperparameter values\n",
    "model = RandomForestRegressor(random_state=1, criterion='mse')\n",
    "\n",
    "# Look at parameters used by our current forest\n",
    "print('Parameters currently in use:\\n')\n",
    "pprint(model.get_params())\n",
    "\n",
    "# add to pipeline\n",
    "pipeline = Pipeline(memory=None,\n",
    "              steps=[('preprocessor', preprocessor),\n",
    "                     ('regressor', model)])\n",
    "\n",
    "\n",
    "# 1) min price\n",
    "transformer_target = PowerTransformer(method='yeo-johnson',standardize=False)\n",
    "scale_target = RobustScaler(with_centering=True, with_scaling=True, quantile_range=(10.0, 90.0))\n",
    "pipeline_y = Pipeline(memory=None,\n",
    "              steps=[('transformer', transformer_target)])\n",
    "\n",
    "pipeline_update = TransformedTargetRegressor(regressor=pipeline, \n",
    "                                         transformer=pipeline_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 500, stop = 4000, num = 20)]\n",
    "\n",
    "# The number of features to consider when looking for the best split:\n",
    "# - If “auto”, then max_features=n_features.\n",
    "# - If “sqrt”, then max_features=sqrt(n_features).\n",
    "# - If “log2”, then max_features=log2(n_features).\n",
    "# - If None, then max_features=n_features\n",
    "max_features = ['auto', 'sqrt','log2',None]\n",
    "\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(20, 200, num = 20)]\n",
    "max_depth.append(None)\n",
    "\n",
    "# The function to measure the quality of a split\n",
    "criterion = ['mse','mae']\n",
    "\n",
    "# Minimum number of samples required to split a node\n",
    "# - If int, then consider min_samples_split as the minimum number.\n",
    "# - If float, then min_samples_split is a fraction and ceil(min_samples_split * n_samples) are the minimum number of samples for each split.\n",
    "min_samples_split = [0.001, 0.01, 0.025, 0.05]\n",
    "# Minimum number of samples required at each leaf node\n",
    "\n",
    "# - If int, then consider min_samples_leaf as the minimum number.\n",
    "# - If float, then min_samples_leaf is a fraction and ceil(min_samples_leaf * n_samples) are the minimum number of samples for each node.\n",
    "min_samples_leaf = [0.001, 0.01, 0.025, 0.05,]\n",
    "\n",
    "# Create the random grid\n",
    "random_grid = {\n",
    "   'regressor__regressor__n_estimators': n_estimators,\n",
    "   'regressor__regressor__max_features': max_features,\n",
    "   'regressor__regressor__max_depth': max_depth,\n",
    "   'regressor__regressor__criterion': criterion,\n",
    "   'regressor__regressor__min_samples_split': min_samples_split,\n",
    "   'regressor__regressor__min_samples_leaf': min_samples_leaf}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helpful articles:\n",
    " - custom scoring metric: https://stackoverflow.com/questions/48468115/how-to-create-a-customized-scoring-function-in-scikit-learn-for-scoring-a-set-of\n",
    " - random parameter search: https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74\n",
    "you can also use the traditional grid search in pyhton, but I prefer the randomizedSearch\n",
    " - Once the optimal parameters are found for the model, you don't need to run this again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum weights: 1.0\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'regressor__sample_weight' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\Documents\\KuLeuven\\Semester2\\AA\\AAForB_Assignment2\\code\\PieterJan\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;31m# run grid search and refit with best hyper parameters\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mweights_train_min_p\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0mweights_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mplot_weights\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[0mrandom_search\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mregressor__sample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrandom_search\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrandom_search\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_score_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'regressor__sample_weight' is not defined"
     ]
    }
   ],
   "source": [
    "# define random search (and narrow down time grid search)\n",
    "if RUN_GRID_SEARCH:\n",
    "    random_search = RandomizedSearchCV(\n",
    "       estimator = pipeline_update, \n",
    "       param_distributions = random_grid, n_iter = 10,\n",
    "       cv = 5, verbose=2, random_state=1, n_jobs = -1, refit=True,\n",
    "       scoring=make_scorer(custom_scoring_func, greater_is_better=False)\n",
    "    )\n",
    "\n",
    "\n",
    "    # run grid search and refit with best hyper parameters\n",
    "    weights_train_min_p =  weights_samples(df=y_train.iloc[:,0], order=0, plot_weights=False)\n",
    "    random_search.fit(X_train, y_train)  \n",
    "    print(random_search.best_params_)    \n",
    "    print(random_search.best_score_)\n",
    "    \n",
    "# have look at the best hyperparameters and their respective performance (maybe also look at the sd)\n",
    "pd.DataFrame(random_search.cv_results_).sort_values(\n",
    "    by=['mean_test_score'],ascending=False).head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Performance on validation data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2) Manual parameters tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum weights: 1.0\n",
      "[Pipeline] ...... (step 1 of 2) Processing preprocessor, total=   0.0s\n",
      "[Pipeline] ......... (step 2 of 2) Processing regressor, total=   4.6s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TransformedTargetRegressor(check_inverse=True, func=None, inverse_func=None,\n",
       "                           regressor=Pipeline(memory=None,\n",
       "                                              steps=[('preprocessor',\n",
       "                                                      ColumnTransformer(n_jobs=None,\n",
       "                                                                        remainder='drop',\n",
       "                                                                        sparse_threshold=0.3,\n",
       "                                                                        transformer_weights=None,\n",
       "                                                                        transformers=[('num',\n",
       "                                                                                       Pipeline(memory=None,\n",
       "                                                                                                steps=[('imputer',\n",
       "                                                                                                        KNNImputer(add_indicator=False,\n",
       "                                                                                                                   copy=True,\n",
       "                                                                                                                   metric='nan_euclidean',\n",
       "                                                                                                                   missi...\n",
       "                                                                            min_impurity_decrease=0.0,\n",
       "                                                                            min_impurity_split=None,\n",
       "                                                                            min_samples_leaf=0.001,\n",
       "                                                                            min_samples_split=0.001,\n",
       "                                                                            min_weight_fraction_leaf=0.0,\n",
       "                                                                            n_estimators=2710,\n",
       "                                                                            n_jobs=-1,\n",
       "                                                                            oob_score=False,\n",
       "                                                                            random_state=1,\n",
       "                                                                            verbose=0,\n",
       "                                                                            warm_start=False))],\n",
       "                                              verbose=True),\n",
       "                           transformer=Pipeline(memory=None,\n",
       "                                                steps=[('transformer',\n",
       "                                                        PowerTransformer(copy=True,\n",
       "                                                                         method='yeo-johnson',\n",
       "                                                                         standardize=False))],\n",
       "                                                verbose=False))"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train your final model on all data with best parameters \n",
    "model = RandomForestRegressor(\n",
    "     criterion='mse',\n",
    "     n_estimators=2710, \n",
    "     max_depth=105,\n",
    "     max_features='log2',\n",
    "     min_samples_split=0.001,\n",
    "     min_samples_leaf=0.001,\n",
    "     bootstrap=False,\n",
    "     n_jobs=-1,\n",
    "     random_state=1\n",
    ")\n",
    "# add to pipeline\n",
    "pipeline = Pipeline(memory=None,\n",
    "              steps=[('preprocessor', preprocessor),\n",
    "                     ('regressor', model)],\n",
    "              verbose=True)\n",
    "\n",
    "# again add transformer for target variable\n",
    "pipeline_update = TransformedTargetRegressor(regressor=pipeline, \n",
    "                                         transformer=pipeline_y)\n",
    "\n",
    "# fit final model on all training data we have at hand\n",
    "weights_train = weights_samples(y_train.iloc[:,1], order=2)\n",
    "pipeline_update.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'minimum price': 0.19519067453172378,\n",
       " 'maximum price': 0.31723201841345444,\n",
       " 'total error': 0.5124226929451783}"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# performance on data where the model was fit one (should be very low)\n",
    "# performance on data where the model was fit one (should be very low)\n",
    "pred_train = pipeline.predict(X_train)\n",
    "\n",
    "# calculate performance\n",
    "calculate_perf(y_train, pred_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Performance on validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'minimum price': 114.05916893459869,\n",
       " 'maximum price': 116.0190068324625,\n",
       " 'total error': 230.07817576706117}"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# performance on validation data\n",
    "pred_val = pipeline_update.predict(X_val)\n",
    "\n",
    "# calculate performance\n",
    "calculate_perf(y_val, pred_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save predictions on validation data\n",
    "# submission format\n",
    "submission_format_validation = pd.DataFrame.from_dict(\n",
    " {'ID':df_val['id'].values,\n",
    " 'MIN':pred_val[:,0],\n",
    " 'MAX':pred_val[:,1]}).set_index('ID')\n",
    "\n",
    "# write to csv\n",
    "submission_format_validation.to_csv('output/validation/random_forest.csv' ,\n",
    "                            header=True, index=True, sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C) Post processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a15fdcf293e49a69a0cfafbe131ec06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = plt.subplots(2, 2, figsize=(9, 7))\n",
    "axs = np.ravel(axs)\n",
    "\n",
    "# fitted against true predictions minimum price\n",
    "plot_predictions_results(ax=axs[0], \n",
    "                        y_true=y_val.iloc[:,0], \n",
    "                        y_pred=pred_val[:,0], \n",
    "                        title=\"RF Min. Price Test Set\", \n",
    "                        log_scale=True)\n",
    "\n",
    "# fitted against true predictions maximum price\n",
    "plot_predictions_results(ax=axs[1], \n",
    "                        y_true=y_val.iloc[:,1], \n",
    "                        y_pred=pred_val[:,1], \n",
    "                        title=\"RF Max. Price Test Set\", \n",
    "                        log_scale=True)\n",
    "\n",
    "# residuals minimum price\n",
    "plot_residuals(ax=axs[2], \n",
    "               y_true=y_val.iloc[:,0], \n",
    "               y_pred=pred_val[:,0], \n",
    "               title=\"\", \n",
    "               log_scale=False,\n",
    "               order=1)\n",
    "\n",
    "# residuals maximum price\n",
    "plot_residuals(ax=axs[3], \n",
    "               y_true=y_val.iloc[:,1], \n",
    "               y_pred=pred_val[:,1], \n",
    "               title=\"\", \n",
    "               log_scale=False,\n",
    "               order=1)\n",
    "fig.tight_layout()\n",
    "plt.savefig('output/figures/random_forest/fig1.png', dpi=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f20be47b662b4571aba3d6fdbe850ed9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(8,4))\n",
    "plt.plot(pred_val[:,1] - pred_val[:,0], label=\"Predictions Test\", linestyle=\"--\")\n",
    "plt.plot(y_val.iloc[:,1] - y_val.iloc[:,0], label=\"Truth Test\", linestyle=':')\n",
    "plt.xlabel(\"Observations\")\n",
    "plt.ylabel(\"Max. Price - Min. Price\")\n",
    "plt.legend(loc=\"upper right\");\n",
    "plt.savefig('output/figures/random_forest/fig2.png', dpi=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Pieter-Jan\\Anaconda3\\lib\\site-packages\\sklearn\\base.py:434: FutureWarning: The default value of multioutput (not exposed in score method) will change from 'variance_weighted' to 'uniform_average' in 0.23 to keep consistent with 'metrics.r2_score'. To specify the default value manually and avoid the warning, please either call 'metrics.r2_score' directly or make a custom scorer with 'metrics.make_scorer' (the built-in scorer 'r2' uses multioutput='uniform_average').\n",
      "  \"multioutput='uniform_average').\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "939d27e74a5045399f620d099a54fb96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vip_val = permutation_importance(pipeline_update, X_val, y_val, n_repeats=10,\n",
    "                                random_state=1, n_jobs=3)\n",
    "\n",
    "sorted_idx = vip_val.importances_mean.argsort()\n",
    "fig, ax = plt.subplots()\n",
    "ax.boxplot(vip_val.importances[sorted_idx].T,\n",
    "           vert=False, labels=X_val.columns[sorted_idx])\n",
    "ax.set_title(\"Permutation Importances (Test set)\")\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "plt.savefig('output/figures/random_forest/fig3.png', dpi=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D) Predictions test data\n",
    "\n",
    "Refit on all training data (using the parameters found on the random search) and submit prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum weights: 1.0\n",
      "[Pipeline] ...... (step 1 of 2) Processing preprocessor, total=   0.1s\n",
      "[Pipeline] ......... (step 2 of 2) Processing regressor, total=   5.7s\n"
     ]
    }
   ],
   "source": [
    "# train your final model on all data with best parameters \n",
    "model_final = RandomForestRegressor(\n",
    "     criterion='mse',\n",
    "     n_estimators=2710, \n",
    "     max_depth=105,\n",
    "     max_features='log2',\n",
    "     min_samples_split=0.001,\n",
    "     min_samples_leaf=0.001,\n",
    "     bootstrap=False,\n",
    "     n_jobs=-1,\n",
    "     random_state=1\n",
    ")\n",
    "# add to pipeline\n",
    "pipeline_final = Pipeline(memory=None,\n",
    "              steps=[('preprocessor', preprocessor),\n",
    "                     ('regressor', model_final)],\n",
    "              verbose=True)\n",
    "\n",
    "# again add transformer for target variable\n",
    "pipeline_final = TransformedTargetRegressor(regressor=pipeline_final, \n",
    "                                         transformer=pipeline_y)\n",
    "\n",
    "# fit final model on all training data we have at hand and give more weight to certain samples\n",
    "weights_all_train = weights_samples(y_all_train.iloc[:,1], order=4)\n",
    "pipeline_final = pipeline_final.fit(X_all_train, y_all_train, regressor__sample_weight=weights_all_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'minimum price': 0.891189468886219,\n",
       " 'maximum price': 0.9839949863788819,\n",
       " 'total error': 1.875184455265101}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# performance on all data where the model was fit one (should be very low)\n",
    "calculate_perf(pipeline_final.predict(X_all_train), y_all_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions on test data\n",
    "pred_test = pipeline_final.predict(X_test)\n",
    "\n",
    "# submission format\n",
    "submission_format = pd.DataFrame.from_dict(\n",
    " {'ID':df_test['id'].values,\n",
    " 'MIN':pred_test[:,0],\n",
    " 'MAX':pred_test[:,1]}).set_index('ID')\n",
    "\n",
    "# write to csv\n",
    "submission_format.to_csv('output/submission/random_forest.csv' ,\n",
    "                            header=True, index=True, sep=',')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
