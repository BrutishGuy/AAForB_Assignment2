{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# auto reload libraries (you do need to re-import libraries if you make changes)\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# base \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "import os\n",
    "\n",
    "# preprocessing \n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.experimental import enable_iterative_imputer \n",
    "from sklearn.impute import SimpleImputer, IterativeImputer,KNNImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, RobustScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "# variable importance\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# visualize\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# models\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "# own defined functions/classes \n",
    "from preprocessing.preprocess_pipe import MakeLowerCase\n",
    "from preprocessing.preprocess_pipe import HdResolutionCategorizer\n",
    "from preprocessing.preprocess_pipe import StorageCategorizer\n",
    "from preprocessing.preprocess_pipe import SsdCategorizer\n",
    "\n",
    "from preprocessing.preprocess_pipe import print_missing\n",
    "from preprocessing.preprocess_pipe import calculate_perf\n",
    "from preprocessing.preprocess_pipe import custom_scoring_func_single_p\n",
    "\n",
    "from modelling.weight_samples import weights_samples\n",
    "\n",
    "from postprocessing.postprocessing import plot_predictions_results\n",
    "from postprocessing.postprocessing import plot_residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Pieter-Jan\\Documents\\KuLeuven\\Semester2\\AA\\AAForB_Assignment2\n"
     ]
    }
   ],
   "source": [
    "# global parameters\n",
    "pd.set_option('display.max_columns', 500)\n",
    "# style for plotting\n",
    "plt.style.use('ggplot')\n",
    "# interactive plotting\n",
    "%matplotlib widget\n",
    "# run grid search\n",
    "RUN_GRID_SEARCH = True\n",
    "# set working directory\n",
    "uppath = lambda _path, n: os.sep.join(_path.split(os.sep)[:-n])\n",
    "__file__ = 'C:\\\\Users\\\\Pieter-Jan\\\\Documents\\\\KuLeuven\\\\Semester2\\\\AA\\\\AAForB_Assignment2\\\\code\\\\PieterJan'\n",
    "out = uppath(__file__, 2)\n",
    "os.chdir(out)\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: import data and split in train and validation set\n",
    "The validation set is more our own kind of test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions of all training data (510, 22)\n",
      "Dimension test data (222, 20)\n"
     ]
    }
   ],
   "source": [
    "# read in trainig and validation data\n",
    "# use the same data split as we did in R\n",
    "df_all_train = pd.read_csv(\"data\\\\train.csv\", sep=',')\n",
    "df_test = pd.read_csv(\"data\\\\test.csv\", sep=',')\n",
    "\n",
    "print(f'Dimensions of all training data {df_all_train.shape}')\n",
    "print(f'Dimension test data {df_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split in training and validation set\n",
    "df_train = df_all_train.sample(frac=0.75, random_state=0, replace=False)\n",
    "df_val = df_all_train.drop(df_train.index)\n",
    "\n",
    "# reset index, if you don't resit missing rows get inserted in the pipeline\n",
    "# see: https://stackoverflow.com/questions/31323499/sklearn-error-valueerror-input-contains-nan-infinity-or-a-value-too-large-for\n",
    "df_train = df_train.reset_index().drop('index',axis=1)\n",
    "df_val = df_val.reset_index().drop('index',axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: define pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Numerical features to pass down the numerical pipeline \n",
    "numerical_features = ['screen_size' ,'pixels_x','pixels_y',\n",
    "                      'ram', 'weight','ssd','storage']\n",
    "\n",
    "#Categrical features to pass down the categorical pipeline \n",
    "categorical_features = ['brand','base_name', 'screen_surface','touchscreen',\n",
    "                        'cpu','cpu_details','detachable_keyboard',\n",
    "                        'discrete_gpu','gpu', 'os','os_details']\n",
    "\n",
    "# define all unique features\n",
    "features = np.unique(numerical_features).tolist() + np.unique(categorical_features).tolist()\n",
    "\n",
    "# target variables\n",
    "target = ['min_price','max_price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "X_train = df_train[features]\n",
    "y_train = df_train[target]\n",
    "\n",
    "# validation (this is kind our own test set)\n",
    "X_val  = df_val[features]\n",
    "y_val = df_val[target]\n",
    "\n",
    "# train_validation (this is all training data we have) for fitting the model\n",
    "X_all_train = df_all_train[features]\n",
    "y_all_train = df_all_train[target]\n",
    "\n",
    "# test\n",
    "X_test = df_test[features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can add many more and \n",
    "# you can even define custom preprocessing steps like 'MakeLowerCase()'\n",
    "\n",
    "# pipeline  uses only numerical features,\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    #('imputer', SimpleImputer(missing_values=np.nan, strategy='mean')),\n",
    "    #('imputer', KNNImputer(n_neighbors=5, weights='uniform',metric='nan_euclidean')),\n",
    "    ('imputer', IterativeImputer(missing_values=np.nan, random_state=0, n_nearest_features=None, sample_posterior=True)),\n",
    "    #('transformation', PowerTransformer(method='yeo-johnson',standardize=False)),\n",
    "     #Scale features using statistics that are robust to outliers.\n",
    "    ('scaler', RobustScaler(with_centering=True, with_scaling=True, quantile_range=(10.0, 90.0)))]) \n",
    "\n",
    "# pipeline use only categorical features\n",
    "categorical_transformer = Pipeline(steps=[ \n",
    "    ('lowercase', MakeLowerCase()), # lower cases all columns containing strings\n",
    "    #('sd_category' ,SsdCategorizer(drop_original_feature=True)),\n",
    "    #('storage_category', StorageCategorizer(drop_original_feature=True)),\n",
    "    ('imputer', SimpleImputer(missing_values=np.nan, strategy=\"most_frequent\")),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "# add both preprocessing pipelines in one pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numerical_features),\n",
    "        ('cat', categorical_transformer, categorical_features)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(382, 1343)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is how the preporcessed pipeline looks like (just to have an idea)\n",
    "pd.DataFrame(preprocessor.fit_transform(X_train)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: add models to pipeline\n",
    "\n",
    "Possible to fit multiple target variabels, so you **don't** need to fit a different models for min. price and max. price\n",
    "\n",
    "### A) Training and parameter tuning\n",
    "\n",
    "##### 1) Automatic tuning via grid search\n",
    "\n",
    "I will only **do the tuning for the minimum price** and use the found parameters also for the maximum price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters currently in use:\n",
      "\n",
      "{'C': 1.0,\n",
      " 'cache_size': 200,\n",
      " 'coef0': 0.0,\n",
      " 'degree': 3,\n",
      " 'epsilon': 0.1,\n",
      " 'gamma': 'scale',\n",
      " 'kernel': 'rbf',\n",
      " 'max_iter': -1,\n",
      " 'shrinking': True,\n",
      " 'tol': 0.001,\n",
      " 'verbose': False}\n"
     ]
    }
   ],
   "source": [
    "# define model: I just add some default parameters but you could\n",
    "# also just write: RandomForestRegressor() since we will perform a grid search \n",
    "# to find good hyperparameter values\n",
    "model_min_p = SVR()\n",
    "model_max_p = SVR()\n",
    "\n",
    "# Look at parameters used by our current forest\n",
    "print('Parameters currently in use:\\n')\n",
    "pprint(model_min_p.get_params())\n",
    "\n",
    "# add to pipeline\n",
    "# 1) min price\n",
    "pipeline_min_p = Pipeline(memory=None,\n",
    "              steps=[('preprocessor', preprocessor),\n",
    "                     ('regressor', model_min_p)])\n",
    "\n",
    "\n",
    "# add transformation on the target variable, by default power transformation \n",
    "# also performs standardization after performing the power transformation\n",
    "# and back transform to the original space when outputting predictions \n",
    "\n",
    "# 1) min price\n",
    "transformer_target = PowerTransformer(method='yeo-johnson',standardize=False)\n",
    "scale_target = RobustScaler(with_centering=True, with_scaling=True, quantile_range=(10.0, 90.0))\n",
    "pipeline_y = Pipeline(memory=None,\n",
    "              steps=[('transformer', transformer_target)])\n",
    "\n",
    "pipeline_min_p_update = TransformedTargetRegressor(regressor=pipeline_min_p, \n",
    "                                         transformer=pipeline_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kernelstring, optional (default=’rbf’)\n",
    "# Specifies the kernel type to be used in the algorithm. It must be one of ‘linear’, \n",
    "# ‘poly’, ‘rbf’, ‘sigmoid’, ‘precomputed’ or a callable. \n",
    "# If none is given, ‘rbf’ will be used. If a callable is given it is used to precompute the kernel matrix.\n",
    "\n",
    "kernel = ['linear', 'poly', 'rbf', 'sigmoid']\n",
    "\n",
    "# degreeint, optional (default=3)\n",
    "# Degree of the polynomial kernel function (‘poly’). Ignored by all other kernels.\n",
    "degree = [1, 3, 5, 7, 9, 11, 13]\n",
    "\n",
    "# tol, optional (default=1e-3)\n",
    "# Tolerance for stopping criterion.\n",
    "tol = [0.0001, 0.001, 0.01, 0.05, 0.1, 0.2, 0.3 ,0.5]\n",
    "\n",
    "# gamma{‘scale’, ‘auto’} or float, optional (default=’scale’)\n",
    "# Kernel coefficient for ‘rbf’, ‘poly’ and ‘sigmoid’.\n",
    "#   - if gamma='scale' (default) is passed then it uses 1 / (n_features * X.var()) as value of gamma,\n",
    "#   - if ‘auto’, uses 1 / n_features.\n",
    "\n",
    "gamma = ['scale','auto']\n",
    "\n",
    "# coef0 float, optional (default=0.0)\n",
    "# Independent term in kernel function. It is only significant in ‘poly’ and ‘sigmoid’.\n",
    "\n",
    "coef0 = [0, 0.1, 0.25, 1, 1.25, 1.5, 2, 2.5]\n",
    "\n",
    "# C float, optional (default=1.0)\n",
    "# Regularization parameter. The strength of the regularization is inversely \n",
    "# proportional to C. Must be strictly positive. The penalty is a squared l2 penalty\n",
    "C = [0.001, 0.01, 0.1, 0.25, 0.5, 1, 2, 10]\n",
    "\n",
    "       \n",
    "# epsilon float, optional (default=0.1)\n",
    "# Epsilon in the epsilon-SVR model. It specifies the epsilon-tube within which no penalty \n",
    "# is associated in the training loss function with points predicted within a distance epsilon from the actual value.\n",
    "epsilon = [0.001, 0.05,0.025, 0.1, 0.2, 0.3]\n",
    "      \n",
    "# shrinking boolean, optional (default=True)\n",
    "# Whether to use the shrinking heuristic.    \n",
    "shrinking = [True, False]\n",
    "\n",
    "# Create the random grid\n",
    "random_grid = {\n",
    "   'regressor__regressor__kernel': kernel,\n",
    "   'regressor__regressor__degree': degree,\n",
    "   'regressor__regressor__tol': tol,\n",
    "   'regressor__regressor__gamma': gamma,\n",
    "   'regressor__regressor__coef0': coef0,\n",
    "   'regressor__regressor__C': C,\n",
    "   'regressor__regressor__epsilon': epsilon,\n",
    "   'regressor__regressor__shrinking': shrinking    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum weights: 1.0\n",
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:   11.8s\n",
      "[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed:   40.0s\n"
     ]
    }
   ],
   "source": [
    "# define random search (and narrow down time grid search)\n",
    "if RUN_GRID_SEARCH:\n",
    "    min_p_random_search = RandomizedSearchCV(\n",
    "       estimator = pipeline_min_p_update, \n",
    "       param_distributions = random_grid, n_iter = 50,\n",
    "       cv = 5, verbose=2, random_state=1, n_jobs = -1, refit=True,\n",
    "       scoring=make_scorer(custom_scoring_func_single_p, greater_is_better=False)\n",
    "    )\n",
    "\n",
    "    # run grid search and refit with best hyper parameters\n",
    "    weights_train_min_p =  weights_samples(df=y_train.iloc[:,0], order=0, plot_weights=False)\n",
    "    min_p_random_search.fit(X_train, y_train.iloc[:,0])  \n",
    "    print(min_p_random_search.best_params_)    \n",
    "    print(min_p_random_search.best_score_)\n",
    "    \n",
    "# have look at the best hyperparameters and their respective performance (maybe also look at the sd)\n",
    "pd.DataFrame(min_p_random_search.cv_results_).sort_values(\n",
    "    by=['mean_test_score'],ascending=False).head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2) Manual parameters tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum weights: 1.0\n",
      "[Pipeline] ...... (step 1 of 2) Processing preprocessor, total=   0.1s\n",
      "[Pipeline] ......... (step 2 of 2) Processing regressor, total=   0.2s\n",
      "Sum weights: 1.0\n",
      "[Pipeline] ...... (step 1 of 2) Processing preprocessor, total=   0.1s\n",
      "[Pipeline] ......... (step 2 of 2) Processing regressor, total=   0.3s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TransformedTargetRegressor(check_inverse=True, func=None, inverse_func=None,\n",
       "                           regressor=Pipeline(memory=None,\n",
       "                                              steps=[('preprocessor',\n",
       "                                                      ColumnTransformer(n_jobs=None,\n",
       "                                                                        remainder='drop',\n",
       "                                                                        sparse_threshold=0.3,\n",
       "                                                                        transformer_weights=None,\n",
       "                                                                        transformers=[('num',\n",
       "                                                                                       Pipeline(memory=None,\n",
       "                                                                                                steps=[('imputer',\n",
       "                                                                                                        IterativeImputer(add_indicator=False,\n",
       "                                                                                                                         estimator=None,\n",
       "                                                                                                                         imputation_order=...\n",
       "                                                                                        'discrete_gpu',\n",
       "                                                                                        'gpu',\n",
       "                                                                                        'os',\n",
       "                                                                                        'os_details'])],\n",
       "                                                                        verbose=False)),\n",
       "                                                     ('regressor',\n",
       "                                                      SVR(C=1, cache_size=200,\n",
       "                                                          coef0=0.1, degree=7,\n",
       "                                                          epsilon=0.001,\n",
       "                                                          gamma='scale',\n",
       "                                                          kernel='rbf',\n",
       "                                                          max_iter=-1,\n",
       "                                                          shrinking=True,\n",
       "                                                          tol=0.0001,\n",
       "                                                          verbose=False))],\n",
       "                                              verbose=True),\n",
       "                           transformer=Pipeline(memory=None,\n",
       "                                                steps=[('transformer',\n",
       "                                                        PowerTransformer(copy=True,\n",
       "                                                                         method='yeo-johnson',\n",
       "                                                                         standardize=False))],\n",
       "                                                verbose=False))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train your final model on all data with best parameters \n",
    "\n",
    "# 1) min price\n",
    "model_min_p = SVR(\n",
    "    tol = 0.0001,\n",
    "    shrinking = True,\n",
    "    kernel = 'rbf',\n",
    "    gamma = 'scale',\n",
    "    epsilon = 0.001,\n",
    "    degree = 7,\n",
    "    coef0 = 0.1,\n",
    "    C = 1\n",
    ")\n",
    "\n",
    "# 2)  min price\n",
    "model_max_p = SVR(   \n",
    "    tol = 0.0001,\n",
    "    shrinking = True,\n",
    "    kernel = 'rbf',\n",
    "    gamma = 'scale',\n",
    "    epsilon = 0.001,\n",
    "    degree = 7,\n",
    "    coef0 = 0.1,\n",
    "    C = 1\n",
    ")\n",
    "\n",
    "\n",
    "# add to pipeline\n",
    "\n",
    "# 1) min price\n",
    "pipeline_min_p = Pipeline(memory=None,\n",
    "              steps=[('preprocessor', preprocessor),\n",
    "                     ('regressor', model_min_p)],\n",
    "              verbose=True)\n",
    "\n",
    "# 2) min price\n",
    "pipeline_max_p = Pipeline(memory=None,\n",
    "              steps=[('preprocessor', preprocessor),\n",
    "                     ('regressor', model_max_p)],\n",
    "              verbose=True)\n",
    "\n",
    "# again add transformer for target variable\n",
    "\n",
    "# 1) min price\n",
    "pipeline_min_p_update = TransformedTargetRegressor(regressor=pipeline_min_p, \n",
    "                                         transformer=pipeline_y)\n",
    "# 2) max price\n",
    "pipeline_max_p_update = TransformedTargetRegressor(regressor=pipeline_max_p, \n",
    "                                         transformer=pipeline_y)\n",
    "\n",
    "# fit final model on all training data we have at hand\n",
    "\n",
    "# 1) min price\n",
    "weights_train_min_p = weights_samples(y_train.iloc[:,0], order=0)\n",
    "pipeline_min_p_update.fit(X_train, y_train.iloc[:,0])\n",
    "\n",
    "# 2) max price\n",
    "weights_train_max_p = weights_samples(y_train.iloc[:,1], order=0)\n",
    "pipeline_max_p_update.fit(X_train, y_train.iloc[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'minimum price': 418.9103259317542,\n",
       " 'maximum price': 435.94005039542657,\n",
       " 'total error': 854.8503763271808}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# performance on data where the model was fit one (should be very low)\n",
    "pred_train_min_p = pipeline_min_p_update.predict(X_train)\n",
    "pred_train_max_p = pipeline_max_p_update.predict(X_train)\n",
    "\n",
    "# calculate performance\n",
    "pred_train = pd.DataFrame([pred_train_min_p,pred_train_max_p]).T\n",
    "calculate_perf(y_train, pred_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Performance on validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'minimum price': 346.16028764762916,\n",
       " 'maximum price': 362.34067023146133,\n",
       " 'total error': 708.5009578790905}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# performance on validation data\n",
    "pred_val_min_p = pipeline_min_p_update.predict(X_val)\n",
    "pred_val_max_p = pipeline_max_p_update.predict(X_val)\n",
    "\n",
    "# calculate performance \n",
    "pred_val = pd.DataFrame([pred_val_min_p,pred_val_max_p]).T\n",
    "calculate_perf(y_val, pred_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save predictions on validation data\n",
    "# submission format\n",
    "submission_format_validation = pd.DataFrame.from_dict(\n",
    " {'ID':df_val['id'].values,\n",
    " 'MIN':pred_val_min_p,\n",
    " 'MAX':pred_val_max_p}).set_index('ID')\n",
    "\n",
    "# write to csv\n",
    "submission_format_validation.to_csv('code\\\\PieterJan\\\\python\\\\output\\\\validation\\\\suppor_vector_machine.csv' ,\n",
    "                            header=True, index=True, sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C) Post processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cc2906bb01547ea8ba2f187bff9e343",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = plt.subplots(2, 2, figsize=(9, 7))\n",
    "axs = np.ravel(axs)\n",
    "\n",
    "# fitted against true predictions minimum price\n",
    "plot_predictions_results(ax=axs[0], \n",
    "                        y_true=y_val.iloc[:,0], \n",
    "                        y_pred=pred_val_min_p, \n",
    "                        title=\"SVM Min. Price Test Set\", \n",
    "                        log_scale=True)\n",
    "\n",
    "# fitted against true predictions maximum price\n",
    "plot_predictions_results(ax=axs[1], \n",
    "                        y_true=y_val.iloc[:,1], \n",
    "                        y_pred=pred_val_max_p, \n",
    "                        title=\"SVM Max. Price Test Set\", \n",
    "                        log_scale=True)\n",
    "\n",
    "# residuals minimum price\n",
    "plot_residuals(ax=axs[2], \n",
    "               y_true=y_val.iloc[:,0], \n",
    "               y_pred=pred_val_min_p, \n",
    "               title=\"\", \n",
    "               log_scale=False,\n",
    "               order=1)\n",
    "\n",
    "# residuals maximum price\n",
    "plot_residuals(ax=axs[3], \n",
    "               y_true=y_val.iloc[:,1], \n",
    "               y_pred=pred_val_max_p, \n",
    "               title=\"\", \n",
    "               log_scale=False,\n",
    "               order=1)\n",
    "fig.tight_layout()\n",
    "plt.savefig('code\\\\PieterJan\\\\python\\\\output\\\\figures\\\\support_vector_machines\\\\fig1.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22e95068fd994ad1920d7df8e94c1529",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(8,4))\n",
    "plt.plot(pred_val_max_p - pred_val_min_p, label=\"Predictions Test\", linestyle=\"--\")\n",
    "plt.plot(y_val.iloc[:,1] - y_val.iloc[:,0], label=\"Truth Test\", linestyle=':')\n",
    "plt.xlabel(\"Observations\")\n",
    "plt.ylabel(\"Max. Price - Min. Price\")\n",
    "plt.legend(loc=\"upper right\");\n",
    "plt.savefig('code\\\\PieterJan\\\\python\\\\output\\\\figures\\\\support_vector_machines\\\\fig2.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "vip_val_min_p = permutation_importance(pipeline_min_p_update, X_val, y_val.iloc[:,0], n_repeats=10,\n",
    "                                random_state=1, n_jobs=3)\n",
    "\n",
    "vip_val_max_p = permutation_importance(pipeline_min_p_update, X_val, y_val.iloc[:,1], n_repeats=10,\n",
    "                                random_state=1, n_jobs=3)\n",
    "\n",
    "sorted_idx_min_p = vip_val_min_p.importances_mean.argsort()\n",
    "sorted_idx_max_p = vip_val_max_p.importances_mean.argsort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b204b7fbe28d464588a15c2abad91e38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(9, 4))\n",
    "ax = np.ravel(ax)\n",
    "# minimum price\n",
    "ax[0].boxplot(vip_val_min_p.importances[sorted_idx_min_p].T,\n",
    "           vert=False, labels=X_val.columns[sorted_idx_min_p])\n",
    "ax[0].set_title(\"Permutation Importances: Min. Price (Test set)\", fontsize=9)\n",
    "ax[0].xaxis.set_tick_params(labelsize=8)\n",
    "ax[0].yaxis.set_tick_params(labelsize=8)\n",
    "\n",
    "# maximum price\n",
    "ax[1].boxplot(vip_val_min_p.importances[sorted_idx_max_p].T,\n",
    "           vert=False, labels=X_val.columns[sorted_idx_max_p])\n",
    "ax[1].set_title(\"Permutation Importances: Max. Price (Test set)\", fontsize=9)\n",
    "ax[1].xaxis.set_tick_params(labelsize=8)\n",
    "ax[1].yaxis.set_tick_params(labelsize=8)\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "plt.savefig('code\\\\PieterJan\\\\python\\\\output\\\\figures\\\\support_vector_machines\\\\fig3.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D) Predictions test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum weights: 1.0\n",
      "[Pipeline] ...... (step 1 of 2) Processing preprocessor, total=   0.2s\n",
      "[Pipeline] ......... (step 2 of 2) Processing regressor, total=   0.0s\n",
      "Sum weights: 1.0\n",
      "[Pipeline] ...... (step 1 of 2) Processing preprocessor, total=   0.3s\n",
      "[Pipeline] ......... (step 2 of 2) Processing regressor, total=   0.0s\n"
     ]
    }
   ],
   "source": [
    "# train your final model on all data with best parameters \n",
    "\n",
    "# 1) min price\n",
    "model_min_p_final = SVR(\n",
    "    tol = 0.0001,\n",
    "    shrinking = False,\n",
    "    kernel = 'rbf',\n",
    "    gamma = 'scale',\n",
    "    epsilon = 0.001,\n",
    "    degree = 5,\n",
    "    coef0 = 0.25,\n",
    "    C = 1\n",
    ")\n",
    "\n",
    "# 2)  min price\n",
    "model_max_p_final = SVR(\n",
    "    tol = 0.0001,\n",
    "    shrinking = False,\n",
    "    kernel = 'rbf',\n",
    "    gamma = 'scale',\n",
    "    epsilon = 0.001,\n",
    "    degree = 5,\n",
    "    coef0 = 0.25,\n",
    "    C = 1\n",
    ")\n",
    "\n",
    "\n",
    "# add to pipeline\n",
    "\n",
    "# 1) min price\n",
    "pipeline_min_p_final = Pipeline(memory=None,\n",
    "              steps=[('preprocessor', preprocessor),\n",
    "                     ('regressor', model_min_p_final)],\n",
    "              verbose=True)\n",
    "\n",
    "# 2) min price\n",
    "pipeline_max_p_final = Pipeline(memory=None,\n",
    "              steps=[('preprocessor', preprocessor),\n",
    "                     ('regressor', model_max_p_final)],\n",
    "              verbose=True)\n",
    "\n",
    "# again add transformer for target variable\n",
    "\n",
    "# 1) min price\n",
    "pipeline_min_p_final = TransformedTargetRegressor(regressor=pipeline_min_p_final, \n",
    "                                         transformer=pipeline_y)\n",
    "# 2) max price\n",
    "pipeline_max_p_final = TransformedTargetRegressor(regressor=pipeline_max_p_final, \n",
    "                                         transformer=pipeline_y)\n",
    "\n",
    "# fit final model on all training data we have at hand\n",
    "\n",
    "# 1) min price\n",
    "weights_all_train_min_p = weights_samples(y_all_train.iloc[:,0], order=2)\n",
    "pipeline_min_p_final.fit(X_train, y_train.iloc[:,0])\n",
    "\n",
    "# 2) max price\n",
    "weights_all_train_max_p = weights_samples(y_train.iloc[:,1], order=2)\n",
    "pipeline_max_p_final.fit(X_train, y_train.iloc[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'minimum price': 62.55169329839375,\n",
       " 'maximum price': 63.131304433276924,\n",
       " 'total error': 125.68299773167067}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# performance on all data where the model was fit one (should be very low)\n",
    "pred_all_train_min_p = pipeline_min_p_final.predict(X_all_train)\n",
    "pred_all_train_max_p = pipeline_max_p_final.predict(X_all_train)\n",
    "\n",
    "# calculate performance \n",
    "pred_all_train = pd.DataFrame([pred_all_train_min_p, pred_all_train_max_p]).T\n",
    "calculate_perf(y_all_train, pred_all_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions on test data\n",
    "pred_test_min_p = pipeline_min_p_final.predict(X_test)\n",
    "pred_test_max_p = pipeline_max_p_final.predict(X_test)\n",
    "\n",
    "# submission format\n",
    "submission_format_test = pd.DataFrame.from_dict(\n",
    " {'ID':df_test['id'].values,\n",
    " 'MIN':pred_test_min_p,\n",
    " 'MAX':pred_test_max_p}).set_index('ID')\n",
    "\n",
    "# write to csv\n",
    "submission_format_test.to_csv('code\\\\PieterJan\\\\python\\\\output\\\\submission\\\\suppor_vector_machine.csv' ,\n",
    "                            header=True, index=True, sep=',')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
