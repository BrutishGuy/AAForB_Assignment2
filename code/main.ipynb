{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# auto reload libraries (you do need to re-import libraries if you make changes)\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# base \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "\n",
    "# preprocessing \n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.experimental import enable_iterative_imputer \n",
    "from sklearn.impute import SimpleImputer, IterativeImputer,KNNImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, RobustScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "\n",
    "# models\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# own defined functions/classes \n",
    "from preprocessing.preprocess_pipe import MakeLowerCase\n",
    "from preprocessing.preprocess_pipe import HdResolutionCategorizer\n",
    "from preprocessing.preprocess_pipe import StorageCategorizer\n",
    "from preprocessing.preprocess_pipe import SsdCategorizer\n",
    "\n",
    "from preprocessing.preprocess_pipe import print_missing\n",
    "from preprocessing.preprocess_pipe import calculate_perf\n",
    "from preprocessing.preprocess_pipe import custom_scoring_func\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global parameters\n",
    "pd.set_option('display.max_columns', 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO\n",
    "- Add custum scoring metric: **DONE**\n",
    "- further integrate preprocessing steps from Arnaud **DONE**\n",
    "- Integerate steps from Victor **Partly DONE**\n",
    "- Look at comments of Victor for next steps\n",
    "- Look at feature engineering ideas\n",
    "- Integrate steps from Bram \n",
    "- Integrate custum scoring metric (also when performing the cross validation/grid search) **DONE**\n",
    "- Make function that checkts whether prediction of the maximum price are >= predictions of the minimum price\n",
    "- Investigate why there is such a huge difference between the performance on 'our test set' and the performance of submission test set\n",
    "- Make function that saves trained models\n",
    "- Try out other models (Boosting, Support Vector Machines, Penalized Linear models like lasso, stacked models ...)\n",
    "- Implement better missing values imputations methods\n",
    "- Add some post processing visualizations:\n",
    "    - feature importance plots\n",
    "    - look at our predictions visually (do the make since?)\n",
    "    - look at the residuals\n",
    "    - Have a look at this especially section 5 for model interpretability ideas\n",
    "    https://christophm.github.io/interpretable-ml-book/pdp.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: import data and split in train and validation set\n",
    "The validation set is more our own kind of test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions of all training data (510, 22)\n",
      "Dimension test data (222, 20)\n"
     ]
    }
   ],
   "source": [
    "# read in trainig and validation data\n",
    "# use the same data split as we did in R\n",
    "df_all_train = pd.read_csv(\"../data/train.csv\", sep=',')\n",
    "df_test = pd.read_csv(\"../data/test.csv\", sep=',')\n",
    "\n",
    "print(f'Dimensions of all training data {df_all_train.shape}')\n",
    "print(f'Dimension test data {df_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split in training and validation set\n",
    "df_train = df_all_train.sample(frac=0.75, random_state=0, replace=False)\n",
    "df_val = df_all_train.drop(df_train.index)\n",
    "\n",
    "# reset index, if you don't resit missing rows get inserted in the pipeline\n",
    "# see: https://stackoverflow.com/questions/31323499/sklearn-error-valueerror-input-contains-nan-infinity-or-a-value-too-large-for\n",
    "df_train = df_train.reset_index().drop('index',axis=1)\n",
    "df_val = df_val.reset_index().drop('index',axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: define pipeline\n",
    "\n",
    "The pipeline should handle all steps performed on the data from data cleaning till the acutal model prediction. \n",
    "It should also be fairly easily to apply this to the test data. Therefore we make use of the scikit learn pipeline which allows all of this functionality. You can also implement your own `custom` transformations.\n",
    "\n",
    "For more information on how to use and why using pipelines see:\n",
    " - https://medium.com/vickdata/a-simple-guide-to-scikit-learn-pipelines-4ac0d974bdcf \n",
    " - scikit learn api on pipelines: https://scikit-learn.org/stable/modules/compose.html\n",
    " - Scikit learn api for more preprocessing steps: https://scikit-learn.org/stable/modules/preprocessing.html#non-linear-transformation\n",
    "\n",
    "We make a difference between the pre processing steps for\n",
    "- numerical features\n",
    "- categorical features\n",
    "\n",
    "It's important to note that if a feature is in the numerical feature pipeline it can't be in the categorical feature pipeline. Also if some preprocessing steps in the numerical feature pipeline need features from the categorical pipeline you will get an error. All preprocessing steps in each pipeline should be able to perform the transformation only with the categorical or numerical features. \n",
    "\n",
    "You can implement some custom transformation steps like 'MakeLowerCase' ==> see preprocessing.preprocess_pipe for more details or check this article  these two article (also gives some explanation on how to implement pipelines)\n",
    "\n",
    "- https://gist.github.com/amberjrivera/8c5c145516f5a2e894681e16a8095b5c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Numerical features to pass down the numerical pipeline \n",
    "numerical_features = ['screen_size' ,'pixels_x','pixels_y',\n",
    "                      'ram', 'weight','ssd','storage']\n",
    "\n",
    "#Categrical features to pass down the categorical pipeline \n",
    "categorical_features = ['brand','base_name', 'screen_surface','touchscreen',\n",
    "                        'cpu','cpu_details','detachable_keyboard',\n",
    "                        'discrete_gpu','gpu', 'os','os_details']\n",
    "\n",
    "# define all unique features\n",
    "features = np.unique(numerical_features).tolist() + np.unique(categorical_features).tolist()\n",
    "\n",
    "# target variables\n",
    "target = ['min_price','max_price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "X_train = df_train[features]\n",
    "y_train = df_train[target]\n",
    "\n",
    "# validation (this is kind our own test set)\n",
    "X_val  = df_val[features]\n",
    "y_val = df_val[target]\n",
    "\n",
    "# train_validation (this is all training data we have) for fitting the model\n",
    "X_all_train = df_all_train[features]\n",
    "y_all_train = df_all_train[target]\n",
    "\n",
    "# test\n",
    "X_test = df_test[features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can add many more and \n",
    "# you can even define custom preprocessing steps like 'MakeLowerCase()'\n",
    "\n",
    "# pipeline  uses only numerical features,\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    #('imputer', SimpleImputer(missing_values=np.nan, strategy='mean')),\n",
    "    #('imputer', KNNImputer(n_neighbors=5, weights='uniform',metric='nan_euclidean')),\n",
    "    ('imputer', IterativeImputer(missing_values=np.nan, random_state=0, n_nearest_features=None, sample_posterior=True)),\n",
    "    #('transformation', PowerTransformer(method='yeo-johnson',standardize=False)),\n",
    "     #Scale features using statistics that are robust to outliers.\n",
    "    ('scaler', RobustScaler(with_centering=True, with_scaling=True, quantile_range=(10.0, 90.0)))]) \n",
    "\n",
    "# pipeline use only categorical features\n",
    "categorical_transformer = Pipeline(steps=[ \n",
    "    ('lowercase', MakeLowerCase()), # lower cases all columns containing strings\n",
    "    #('sd_category' ,SsdCategorizer(drop_original_feature=True)),\n",
    "    #('storage_category', StorageCategorizer(drop_original_feature=True)),\n",
    "    ('imputer', SimpleImputer(missing_values=np.nan, strategy=\"most_frequent\")),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "# add both preprocessing pipelines in one pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numerical_features),\n",
    "        ('cat', categorical_transformer, categorical_features)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(382, 1)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is how the preporcessed pipeline looks like (just to have an idea)\n",
    "pd.DataFrame(preprocessor.fit_transform(X_train)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: add models to pipeline\n",
    "## 1) Random Forest\n",
    "Possible to fit multiple target variabels, so you **don't** need to fit a differenet models for min. price and max. price\n",
    "\n",
    "### A) Training and parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters currently in use:\n",
      "\n",
      "{'bootstrap': True,\n",
      " 'ccp_alpha': 0.0,\n",
      " 'criterion': 'mae',\n",
      " 'max_depth': None,\n",
      " 'max_features': 'auto',\n",
      " 'max_leaf_nodes': None,\n",
      " 'max_samples': None,\n",
      " 'min_impurity_decrease': 0.0,\n",
      " 'min_impurity_split': None,\n",
      " 'min_samples_leaf': 1,\n",
      " 'min_samples_split': 2,\n",
      " 'min_weight_fraction_leaf': 0.0,\n",
      " 'n_estimators': 100,\n",
      " 'n_jobs': None,\n",
      " 'oob_score': False,\n",
      " 'random_state': 1,\n",
      " 'verbose': 0,\n",
      " 'warm_start': False}\n"
     ]
    }
   ],
   "source": [
    "# define model: I just add some default parameters but you could\n",
    "# also just write: RandomForestRegressor() since we will perform a grid search \n",
    "# to find good hyperparameter values\n",
    "model_rf = RandomForestRegressor(random_state=1, criterion='mae', bootstrap=True)\n",
    "\n",
    "# Look at parameters used by our current forest\n",
    "print('Parameters currently in use:\\n')\n",
    "pprint(model_rf.get_params())\n",
    "\n",
    "# add to pipeline\n",
    "pipeline_rf = Pipeline(memory=None,\n",
    "              steps=[('preprocessor', preprocessor),\n",
    "                     ('regressor', model_rf)])\n",
    "\n",
    "\n",
    "# add transformation on the target variable, by default power transformation \n",
    "# also performs standardization after performing the power transformation\n",
    "# and back transform to the original space when outputting predictions \n",
    "transformer_target = PowerTransformer(method='yeo-johnson',standardize=True)\n",
    "pipeline_rf_update = TransformedTargetRegressor(regressor=pipeline_rf, \n",
    "                                         transformer=transformer_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 500, stop = 4000, num = 20)]\n",
    "\n",
    "# The number of features to consider when looking for the best split:\n",
    "# - If “auto”, then max_features=n_features.\n",
    "# - If “sqrt”, then max_features=sqrt(n_features).\n",
    "# - If “log2”, then max_features=log2(n_features).\n",
    "# - If None, then max_features=n_features\n",
    "max_features = ['auto', 'sqrt','log2',None]\n",
    "\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(20, 200, num = 20)]\n",
    "max_depth.append(None)\n",
    "\n",
    "# The function to measure the quality of a split\n",
    "criterion = ['mse','mae']\n",
    "\n",
    "# Minimum number of samples required to split a node\n",
    "# - If int, then consider min_samples_split as the minimum number.\n",
    "# - If float, then min_samples_split is a fraction and ceil(min_samples_split * n_samples) are the minimum number of samples for each split.\n",
    "min_samples_split = [0.001, 0.01, 0.025, 0.05]\n",
    "# Minimum number of samples required at each leaf node\n",
    "\n",
    "# - If int, then consider min_samples_leaf as the minimum number.\n",
    "# - If float, then min_samples_leaf is a fraction and ceil(min_samples_leaf * n_samples) are the minimum number of samples for each node.\n",
    "min_samples_leaf = [0.001, 0.01, 0.025, 0.05,]\n",
    "\n",
    "# Create the random grid\n",
    "random_grid_rf = {\n",
    "   'regressor__regressor__n_estimators': n_estimators,\n",
    "   'regressor__regressor__max_features': max_features,\n",
    "   'regressor__regressor__max_depth': max_depth,\n",
    "   'regressor__regressor__criterion': criterion,\n",
    "   'regressor__regressor__min_samples_split': min_samples_split,\n",
    "   'regressor__regressor__min_samples_leaf': min_samples_leaf}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helpful articles:\n",
    " - custom scoring metric: https://stackoverflow.com/questions/48468115/how-to-create-a-customized-scoring-function-in-scikit-learn-for-scoring-a-set-of\n",
    " - random parameter search: https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74\n",
    "you can also use the traditional grid search in pyhton, but I prefer the randomizedSearch\n",
    " - Once the optimal parameters are found for the model, you don't need to run this again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 20 candidates, totalling 200 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed: 12.9min\n",
      "[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed: 32.0min\n",
      "[Parallel(n_jobs=-1)]: Done 200 out of 200 | elapsed: 62.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'regressor__regressor__n_estimators': 2710, 'regressor__regressor__min_samples_split': 0.001, 'regressor__regressor__min_samples_leaf': 0.001, 'regressor__regressor__max_features': 'log2', 'regressor__regressor__max_depth': 105, 'regressor__regressor__criterion': 'mse'}\n",
      "-323.0841059233438\n"
     ]
    }
   ],
   "source": [
    "# define random search (and narrow down time grid search)\n",
    "rf_random_search = RandomizedSearchCV(\n",
    "   estimator = pipeline_rf_update, \n",
    "   param_distributions = random_grid_rf, n_iter = 20,  \n",
    "   cv = 10, verbose=2, random_state=1, n_jobs = -1, refit=True,\n",
    "   scoring=make_scorer(custom_scoring_func, greater_is_better=False)\n",
    ")\n",
    "\n",
    "\n",
    "# run grid search and refit with best hyper parameters\n",
    "rf_random_search.fit(X_train, y_train)  \n",
    "print(rf_random_search.best_params_)    \n",
    "print(rf_random_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **TODO make a plot that visualizes hyperparameters (maybe kind of heatmap)**\n",
    "- Once we have found good tuning parameters write them down so we don't need to redo this step over and over"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_regressor__regressor__n_estimators</th>\n",
       "      <th>param_regressor__regressor__min_samples_split</th>\n",
       "      <th>param_regressor__regressor__min_samples_leaf</th>\n",
       "      <th>param_regressor__regressor__max_features</th>\n",
       "      <th>param_regressor__regressor__max_depth</th>\n",
       "      <th>param_regressor__regressor__criterion</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>split5_test_score</th>\n",
       "      <th>split6_test_score</th>\n",
       "      <th>split7_test_score</th>\n",
       "      <th>split8_test_score</th>\n",
       "      <th>split9_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13.479068</td>\n",
       "      <td>1.150149</td>\n",
       "      <td>0.474687</td>\n",
       "      <td>0.138394</td>\n",
       "      <td>2710</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>log2</td>\n",
       "      <td>105</td>\n",
       "      <td>mse</td>\n",
       "      <td>{'regressor__regressor__n_estimators': 2710, '...</td>\n",
       "      <td>-292.351281</td>\n",
       "      <td>-462.415131</td>\n",
       "      <td>-303.559482</td>\n",
       "      <td>-266.786839</td>\n",
       "      <td>-365.295985</td>\n",
       "      <td>-370.527492</td>\n",
       "      <td>-294.776052</td>\n",
       "      <td>-381.197264</td>\n",
       "      <td>-281.152255</td>\n",
       "      <td>-212.779277</td>\n",
       "      <td>-323.084106</td>\n",
       "      <td>67.856372</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>6.767192</td>\n",
       "      <td>0.363616</td>\n",
       "      <td>0.087842</td>\n",
       "      <td>0.026452</td>\n",
       "      <td>500</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>None</td>\n",
       "      <td>171</td>\n",
       "      <td>mse</td>\n",
       "      <td>{'regressor__regressor__n_estimators': 500, 'r...</td>\n",
       "      <td>-263.938468</td>\n",
       "      <td>-511.664647</td>\n",
       "      <td>-263.040974</td>\n",
       "      <td>-279.961373</td>\n",
       "      <td>-301.749974</td>\n",
       "      <td>-416.063834</td>\n",
       "      <td>-311.028899</td>\n",
       "      <td>-401.571016</td>\n",
       "      <td>-324.348347</td>\n",
       "      <td>-225.038672</td>\n",
       "      <td>-329.840620</td>\n",
       "      <td>83.120078</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>564.831582</td>\n",
       "      <td>69.796700</td>\n",
       "      <td>0.411761</td>\n",
       "      <td>0.118170</td>\n",
       "      <td>1421</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.001</td>\n",
       "      <td>auto</td>\n",
       "      <td>57</td>\n",
       "      <td>mae</td>\n",
       "      <td>{'regressor__regressor__n_estimators': 1421, '...</td>\n",
       "      <td>-263.816019</td>\n",
       "      <td>-517.734659</td>\n",
       "      <td>-266.500882</td>\n",
       "      <td>-293.497694</td>\n",
       "      <td>-310.503175</td>\n",
       "      <td>-423.968593</td>\n",
       "      <td>-309.431664</td>\n",
       "      <td>-401.819253</td>\n",
       "      <td>-330.836935</td>\n",
       "      <td>-214.457371</td>\n",
       "      <td>-333.256625</td>\n",
       "      <td>85.454787</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>263.074534</td>\n",
       "      <td>48.241131</td>\n",
       "      <td>0.152257</td>\n",
       "      <td>0.033016</td>\n",
       "      <td>1052</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.001</td>\n",
       "      <td>None</td>\n",
       "      <td>57</td>\n",
       "      <td>mae</td>\n",
       "      <td>{'regressor__regressor__n_estimators': 1052, '...</td>\n",
       "      <td>-272.993354</td>\n",
       "      <td>-519.647564</td>\n",
       "      <td>-270.649291</td>\n",
       "      <td>-295.723758</td>\n",
       "      <td>-313.549730</td>\n",
       "      <td>-431.602505</td>\n",
       "      <td>-317.743510</td>\n",
       "      <td>-403.861203</td>\n",
       "      <td>-331.690432</td>\n",
       "      <td>-219.478645</td>\n",
       "      <td>-337.693999</td>\n",
       "      <td>84.698615</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>13.482061</td>\n",
       "      <td>0.838702</td>\n",
       "      <td>0.322748</td>\n",
       "      <td>0.046353</td>\n",
       "      <td>2157</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.01</td>\n",
       "      <td>auto</td>\n",
       "      <td>171</td>\n",
       "      <td>mse</td>\n",
       "      <td>{'regressor__regressor__n_estimators': 2157, '...</td>\n",
       "      <td>-272.770082</td>\n",
       "      <td>-495.134701</td>\n",
       "      <td>-299.700345</td>\n",
       "      <td>-260.181870</td>\n",
       "      <td>-346.592453</td>\n",
       "      <td>-439.149380</td>\n",
       "      <td>-312.676365</td>\n",
       "      <td>-406.732719</td>\n",
       "      <td>-326.541273</td>\n",
       "      <td>-226.526805</td>\n",
       "      <td>-338.600599</td>\n",
       "      <td>80.432115</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "1       13.479068      1.150149         0.474687        0.138394   \n",
       "8        6.767192      0.363616         0.087842        0.026452   \n",
       "16     564.831582     69.796700         0.411761        0.118170   \n",
       "0      263.074534     48.241131         0.152257        0.033016   \n",
       "10      13.482061      0.838702         0.322748        0.046353   \n",
       "\n",
       "   param_regressor__regressor__n_estimators  \\\n",
       "1                                      2710   \n",
       "8                                       500   \n",
       "16                                     1421   \n",
       "0                                      1052   \n",
       "10                                     2157   \n",
       "\n",
       "   param_regressor__regressor__min_samples_split  \\\n",
       "1                                          0.001   \n",
       "8                                          0.001   \n",
       "16                                          0.01   \n",
       "0                                          0.025   \n",
       "10                                         0.025   \n",
       "\n",
       "   param_regressor__regressor__min_samples_leaf  \\\n",
       "1                                         0.001   \n",
       "8                                         0.001   \n",
       "16                                        0.001   \n",
       "0                                         0.001   \n",
       "10                                         0.01   \n",
       "\n",
       "   param_regressor__regressor__max_features  \\\n",
       "1                                      log2   \n",
       "8                                      None   \n",
       "16                                     auto   \n",
       "0                                      None   \n",
       "10                                     auto   \n",
       "\n",
       "   param_regressor__regressor__max_depth  \\\n",
       "1                                    105   \n",
       "8                                    171   \n",
       "16                                    57   \n",
       "0                                     57   \n",
       "10                                   171   \n",
       "\n",
       "   param_regressor__regressor__criterion  \\\n",
       "1                                    mse   \n",
       "8                                    mse   \n",
       "16                                   mae   \n",
       "0                                    mae   \n",
       "10                                   mse   \n",
       "\n",
       "                                               params  split0_test_score  \\\n",
       "1   {'regressor__regressor__n_estimators': 2710, '...        -292.351281   \n",
       "8   {'regressor__regressor__n_estimators': 500, 'r...        -263.938468   \n",
       "16  {'regressor__regressor__n_estimators': 1421, '...        -263.816019   \n",
       "0   {'regressor__regressor__n_estimators': 1052, '...        -272.993354   \n",
       "10  {'regressor__regressor__n_estimators': 2157, '...        -272.770082   \n",
       "\n",
       "    split1_test_score  split2_test_score  split3_test_score  \\\n",
       "1         -462.415131        -303.559482        -266.786839   \n",
       "8         -511.664647        -263.040974        -279.961373   \n",
       "16        -517.734659        -266.500882        -293.497694   \n",
       "0         -519.647564        -270.649291        -295.723758   \n",
       "10        -495.134701        -299.700345        -260.181870   \n",
       "\n",
       "    split4_test_score  split5_test_score  split6_test_score  \\\n",
       "1         -365.295985        -370.527492        -294.776052   \n",
       "8         -301.749974        -416.063834        -311.028899   \n",
       "16        -310.503175        -423.968593        -309.431664   \n",
       "0         -313.549730        -431.602505        -317.743510   \n",
       "10        -346.592453        -439.149380        -312.676365   \n",
       "\n",
       "    split7_test_score  split8_test_score  split9_test_score  mean_test_score  \\\n",
       "1         -381.197264        -281.152255        -212.779277      -323.084106   \n",
       "8         -401.571016        -324.348347        -225.038672      -329.840620   \n",
       "16        -401.819253        -330.836935        -214.457371      -333.256625   \n",
       "0         -403.861203        -331.690432        -219.478645      -337.693999   \n",
       "10        -406.732719        -326.541273        -226.526805      -338.600599   \n",
       "\n",
       "    std_test_score  rank_test_score  \n",
       "1        67.856372                1  \n",
       "8        83.120078                2  \n",
       "16       85.454787                3  \n",
       "0        84.698615                4  \n",
       "10       80.432115                5  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# have look at the best hyperparameters and their respective performance (maybe also look at the sd)\n",
    "pd.DataFrame(rf_random_search.cv_results_).sort_values(\n",
    "    by=['mean_test_score'],ascending=False).head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B) Performance on validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'minimum price': 117.97146951503387,\n",
       " 'maximum price': 118.18884872124389,\n",
       " 'total error': 236.16031823627776}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_perf(y_val, rf_random_search.predict(X_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C) Post processing\n",
    "\n",
    " - inspect predictions/residuals (make visualisations) (See Bram)\n",
    " - feature importance (see Bram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D) Predictions test data\n",
    "\n",
    "Refit on all training data (using the parameters found on the random search) and submit prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ...... (step 1 of 2) Processing preprocessor, total=   0.4s\n",
      "[Pipeline] ......... (step 2 of 2) Processing regressor, total=  16.5s\n"
     ]
    }
   ],
   "source": [
    "# train your final model on all data with best parameters \n",
    "model_rf_final = RandomForestRegressor(\n",
    "     criterion='mse',\n",
    "     n_estimators=2710, \n",
    "     max_depth=105,\n",
    "     max_features='log2',\n",
    "     min_samples_split=0.001,\n",
    "     min_samples_leaf=0.001,\n",
    "     bootstrap=True,\n",
    "     n_jobs=-1\n",
    ")\n",
    "# add to pipeline\n",
    "pipeline_rf_final = Pipeline(memory=None,\n",
    "              steps=[('preprocessor', preprocessor),\n",
    "                     ('regressor', model_rf_final)],\n",
    "              verbose=True)\n",
    "\n",
    "# again add transformer for target variable\n",
    "pipeline_rf_final = TransformedTargetRegressor(regressor=pipeline_rf_final, \n",
    "                                         transformer=transformer_target)\n",
    "\n",
    "# fit final model on all training data we have at hand\n",
    "pipeline_rf_final = pipeline_rf_final.fit(X_all_train, y_all_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'minimum price': 55.224733303068184,\n",
       " 'maximum price': 56.84639523342007,\n",
       " 'total error': 112.07112853648826}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# performance on all data where the model was fit one (should be very low)\n",
    "calculate_perf(pipeline_rf_final.predict(X_all_train), y_all_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions on test data\n",
    "rf_pred_test = pipeline_rf_final.predict(X_test)\n",
    "\n",
    "# submission format\n",
    "rf_submission_format = pd.DataFrame.from_dict(\n",
    " {'ID':df_test['id'].values,\n",
    " 'MIN':rf_pred_test[:,0],\n",
    " 'MAX':rf_pred_test[:,1]}).set_index('ID')\n",
    "\n",
    "# write to csv\n",
    "rf_submission_format.to_csv('../output/predictions_test/rf_python.csv' ,\n",
    "                            header=True, index=True, sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Gradient Boosting\n",
    "Does not seem possible to fit multiple target variabels, so you do need to fit a differenet models for min. price and max. price"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Support Vector Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does not seem possible to fit multiple target variabels, so you do need to fit a differenet models for min. price and max. price"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Penalized Linear regression (Lasso, Ridge, Elastic net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does not seem possible to fit multiple target variabels, so you do need to fit a differenet models for min. price and max. price"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
