{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# auto reload libraries (you do need to re-import libraries if you make changes)\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# base \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "\n",
    "# preprocessing \n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.experimental import enable_iterative_imputer \n",
    "from sklearn.impute import SimpleImputer, IterativeImputer,KNNImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, RobustScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "\n",
    "# models\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# own defined functions/classes \n",
    "from preprocessing.preprocess_pipe import MakeLowerCase\n",
    "from preprocessing.preprocess_pipe import HdResolutionCategorizer\n",
    "from preprocessing.preprocess_pipe import StorageCategorizer\n",
    "from preprocessing.preprocess_pipe import SsdCategorizer\n",
    "\n",
    "from preprocessing.preprocess_pipe import print_missing\n",
    "from preprocessing.preprocess_pipe import calculate_perf\n",
    "from preprocessing.preprocess_pipe import custom_scoring_func\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global parameters\n",
    "pd.set_option('display.max_columns', 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO\n",
    "- Add custum scoring metric: **DONE**\n",
    "- further integrate preprocessing steps from Arnaud **DONE**\n",
    "- Integerate steps from Victor **Partly DONE**\n",
    "- Integrate steps from Bram \n",
    "- Integrate custum scoring metric (also when performing the cross validation/grid search)\n",
    "-  Make function that checkts whether prediction of the maximum price are >= predictions of the minimum price\n",
    "- Implement better missing values imputations methods\n",
    "- Add some post processing visualizations:\n",
    "    - feature importance plots\n",
    "    - look at our predictions visually (do the make since?)\n",
    "    - look at the residuals\n",
    "    - Have a look at this especially section 5 for model interpretability ideas\n",
    "    https://christophm.github.io/interpretable-ml-book/pdp.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: import data and split in train and validation set\n",
    "The validation set is more our own kind of test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions of all training data (510, 22)\n",
      "Dimension test data (222, 20)\n"
     ]
    }
   ],
   "source": [
    "# read in trainig and validation data\n",
    "# use the same data split as we did in R\n",
    "df_all_train = pd.read_csv(\"../data/train.csv\", sep=',')\n",
    "df_test = pd.read_csv(\"../data/test.csv\", sep=',')\n",
    "\n",
    "print(f'Dimensions of all training data {df_all_train.shape}')\n",
    "print(f'Dimension test data {df_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split in training and validation set\n",
    "\n",
    "df_train = df_all_train.sample(frac=0.75, random_state=0)\n",
    "df_val = df_all_train.drop(df_train.index)\n",
    "\n",
    "# reset index, if you don't resit missing rows get inserted in the pipeline\n",
    "# see: https://stackoverflow.com/questions/31323499/sklearn-error-valueerror-input-contains-nan-infinity-or-a-value-too-large-for\n",
    "df_train = df_train.reset_index()\n",
    "df_val = df_val.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: define pipeline\n",
    "\n",
    "The pipeline should handle all steps performed on the data from data cleaning till the acutall models. \n",
    "It should also be easily fairly easily to apply this to the test data. Therefore we make use of the scikit learn pipeline which allows all of this functionality. You also implement your own `custom` transformations.\n",
    "\n",
    "Make a difference between the pre processing steps for\n",
    "- numerical features\n",
    "- categorical features\n",
    "\n",
    "I still don't use all features, since some extra data cleaning is needed on certain features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Numerical features to pass down the numerical pipeline \n",
    "numerical_features = ['screen_size','detachable_keyboard' , \n",
    "                      'ram', 'weight','pixels_x','pixels_y']\n",
    "\n",
    "#Categrical features to pass down the categorical pipeline \n",
    "categorical_features = ['brand', 'screen_surface','touchscreen',\n",
    "                        'cpu', 'discrete_gpu','gpu', 'os','ssd','storage']\n",
    "\n",
    "# define all unique features\n",
    "features = np.unique(numerical_features).tolist() + np.unique(categorical_features).tolist()\n",
    "\n",
    "# target variables\n",
    "target = ['min_price','max_price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "X_train = df_train[features]\n",
    "y_train = df_train[target]\n",
    "\n",
    "# validation (this is kind our own test set)\n",
    "X_val  = df_val[features]\n",
    "y_val = df_val[target]\n",
    "\n",
    "# train_validation (this is all training data we have) for fitting the model\n",
    "X_all_train = df_all_train[features]\n",
    "y_all_train = df_all_train[target]\n",
    "\n",
    "# test\n",
    "X_test = df_test[features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can add many more and \n",
    "# you can even define custom preprocessing steps like 'MakeLowerCase()'\n",
    "\n",
    "# pipeline  uses only numerical features,\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    #('imputer', SimpleImputer(missing_values=np.nan, strategy='mean')),\n",
    "    #('imputer', KNNImputer(n_neighbors=5, weights='uniform',metric='nan_euclidean')),\n",
    "    ('imputer', IterativeImputer(missing_values=np.nan, random_state=0, n_nearest_features=None, sample_posterior=True)),\n",
    "    #('transformation', PowerTransformer(method='yeo-johnson',standardize=False)),\n",
    "     #Scale features using statistics that are robust to outliers.\n",
    "    ('scaler', RobustScaler(with_centering=True, with_scaling=True, quantile_range=(10.0, 90.0)))]) \n",
    "\n",
    "# pipeline use only categorical features\n",
    "categorical_transformer = Pipeline(steps=[ \n",
    "    ('lowercase', MakeLowerCase()), # lower cases all columns containing strings\n",
    "    ('sd_category' ,SsdCategorizer(drop_original_feature=True)),\n",
    "    ('storage_category', StorageCategorizer()),\n",
    "    ('imputer', SimpleImputer(missing_values=np.nan, strategy=\"most_frequent\")),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "# add both preprocessing pipelines in one pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numerical_features),\n",
    "        ('cat', categorical_transformer, categorical_features)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(382, 1)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is how the preporcessed pipeline looks like (just to have an idea)\n",
    "pd.DataFrame(preprocessor.fit_transform(X_train)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: add models to pipeline\n",
    "## 1) Random Forest\n",
    "\n",
    "### A) Training and parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters currently in use:\n",
      "\n",
      "{'bootstrap': True,\n",
      " 'ccp_alpha': 0.0,\n",
      " 'criterion': 'mse',\n",
      " 'max_depth': None,\n",
      " 'max_features': 'auto',\n",
      " 'max_leaf_nodes': None,\n",
      " 'max_samples': None,\n",
      " 'min_impurity_decrease': 0.0,\n",
      " 'min_impurity_split': None,\n",
      " 'min_samples_leaf': 1,\n",
      " 'min_samples_split': 2,\n",
      " 'min_weight_fraction_leaf': 0.0,\n",
      " 'n_estimators': 100,\n",
      " 'n_jobs': None,\n",
      " 'oob_score': False,\n",
      " 'random_state': 1,\n",
      " 'verbose': 0,\n",
      " 'warm_start': False}\n"
     ]
    }
   ],
   "source": [
    "# define model: I just add some default parameters but you could\n",
    "# also just write: RandomForestRegressor() since we will perform a grid search \n",
    "# to find good hyperparameter values\n",
    "model_rf = RandomForestRegressor(random_state=1)\n",
    "\n",
    "# Look at parameters used by our current forest\n",
    "print('Parameters currently in use:\\n')\n",
    "pprint(model_rf.get_params())\n",
    "\n",
    "# add to pipeline\n",
    "pipeline_rf = Pipeline(memory=None,\n",
    "              steps=[('preprocessor', preprocessor),\n",
    "                     ('regressor', model_rf)])\n",
    "\n",
    "\n",
    "# add transformation on the target variable, by default power transformation \n",
    "# also performs standardization after performing the power transformation\n",
    "# and back transform to the original space when outputting predictions \n",
    "transformer_target = PowerTransformer(method='yeo-johnson',standardize=True)\n",
    "pipeline_rf_update = TransformedTargetRegressor(regressor=pipeline_rf, \n",
    "                                         transformer=transformer_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 500, stop = 6000, num = 50)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(20, 200, num = 50)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [1, 2, 5, 10,20]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4, 8, 16]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "# Create the random grid\n",
    "random_grid_rf = {\n",
    "               'regressor__regressor__n_estimators': n_estimators,\n",
    "               'regressor__regressor__max_features': max_features,\n",
    "               'regressor__regressor__max_depth': max_depth,\n",
    "               'regressor__regressor__min_samples_split': min_samples_split,\n",
    "               'regressor__regressor__min_samples_leaf': min_samples_leaf,\n",
    "               'regressor__regressor__bootstrap': bootstrap}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helpful articles:\n",
    " - custom scoring metric: https://stackoverflow.com/questions/48468115/how-to-create-a-customized-scoring-function-in-scikit-learn-for-scoring-a-set-of\n",
    " - random parameter search: https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74\n",
    "you can also use the traditional grid search in pyhton, but I prefer the randomizedSearch\n",
    " - Once the optimal parameters are found for the model, you don't need to run this again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 100 candidates, totalling 1000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:   54.7s\n",
      "[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed:  7.7min\n",
      "[Parallel(n_jobs=-1)]: Done 357 tasks      | elapsed: 14.4min\n",
      "[Parallel(n_jobs=-1)]: Done 640 tasks      | elapsed: 26.8min\n",
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed: 44.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'regressor__regressor__n_estimators': 2857, 'regressor__regressor__min_samples_split': 5, 'regressor__regressor__min_samples_leaf': 1, 'regressor__regressor__max_features': 'sqrt', 'regressor__regressor__max_depth': 181, 'regressor__regressor__bootstrap': False}\n",
      "-315.5175356454738\n"
     ]
    }
   ],
   "source": [
    "# define random search (and narrow down time grid search)\n",
    "rf_random_search = RandomizedSearchCV(\n",
    "   estimator = pipeline_rf_update, \n",
    "   param_distributions = random_grid_rf, n_iter = 100,  \n",
    "   cv = 10, verbose=2, random_state=1, n_jobs = -1, refit=True,\n",
    "   scoring=make_scorer(custom_scoring_func, greater_is_better=False)\n",
    ")\n",
    "\n",
    "\n",
    "# run grid search and refit with best hyper parameters\n",
    "rf_random_search.fit(X_train, y_train)  \n",
    "print(rf_random_search.best_params_)    \n",
    "print(rf_random_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **TODO make a plot that visualizes hyperparameters (maybe kind of heatmap)**\n",
    "- Once we have found good tuning parameters write them down so we don't need to redo this step over and over"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_regressor__regressor__n_estimators</th>\n",
       "      <th>param_regressor__regressor__min_samples_split</th>\n",
       "      <th>param_regressor__regressor__min_samples_leaf</th>\n",
       "      <th>param_regressor__regressor__max_features</th>\n",
       "      <th>param_regressor__regressor__max_depth</th>\n",
       "      <th>param_regressor__regressor__bootstrap</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>split5_test_score</th>\n",
       "      <th>split6_test_score</th>\n",
       "      <th>split7_test_score</th>\n",
       "      <th>split8_test_score</th>\n",
       "      <th>split9_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>7.756778</td>\n",
       "      <td>0.147370</td>\n",
       "      <td>0.381489</td>\n",
       "      <td>0.016976</td>\n",
       "      <td>2857</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>181</td>\n",
       "      <td>False</td>\n",
       "      <td>{'regressor__regressor__n_estimators': 2857, '...</td>\n",
       "      <td>-272.750763</td>\n",
       "      <td>-434.221200</td>\n",
       "      <td>-278.790106</td>\n",
       "      <td>-308.786471</td>\n",
       "      <td>-280.774726</td>\n",
       "      <td>-385.856308</td>\n",
       "      <td>-311.530852</td>\n",
       "      <td>-349.243861</td>\n",
       "      <td>-293.190614</td>\n",
       "      <td>-240.030456</td>\n",
       "      <td>-315.517536</td>\n",
       "      <td>55.514433</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>16.502742</td>\n",
       "      <td>0.254536</td>\n",
       "      <td>0.845860</td>\n",
       "      <td>0.132707</td>\n",
       "      <td>5663</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>20</td>\n",
       "      <td>False</td>\n",
       "      <td>{'regressor__regressor__n_estimators': 5663, '...</td>\n",
       "      <td>-272.978500</td>\n",
       "      <td>-433.775705</td>\n",
       "      <td>-278.319974</td>\n",
       "      <td>-308.040762</td>\n",
       "      <td>-281.107885</td>\n",
       "      <td>-385.856782</td>\n",
       "      <td>-312.665655</td>\n",
       "      <td>-347.969186</td>\n",
       "      <td>-293.861860</td>\n",
       "      <td>-240.764269</td>\n",
       "      <td>-315.534058</td>\n",
       "      <td>55.212333</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>12.073451</td>\n",
       "      <td>0.278458</td>\n",
       "      <td>0.399036</td>\n",
       "      <td>0.026090</td>\n",
       "      <td>2969</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>192</td>\n",
       "      <td>False</td>\n",
       "      <td>{'regressor__regressor__n_estimators': 2969, '...</td>\n",
       "      <td>-261.299327</td>\n",
       "      <td>-439.731069</td>\n",
       "      <td>-273.029449</td>\n",
       "      <td>-337.622471</td>\n",
       "      <td>-281.089347</td>\n",
       "      <td>-376.183880</td>\n",
       "      <td>-319.388937</td>\n",
       "      <td>-352.818326</td>\n",
       "      <td>-289.229114</td>\n",
       "      <td>-248.908424</td>\n",
       "      <td>-317.930035</td>\n",
       "      <td>56.572235</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>15.297682</td>\n",
       "      <td>1.709751</td>\n",
       "      <td>0.465761</td>\n",
       "      <td>0.128846</td>\n",
       "      <td>3081</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>34</td>\n",
       "      <td>False</td>\n",
       "      <td>{'regressor__regressor__n_estimators': 3081, '...</td>\n",
       "      <td>-261.388976</td>\n",
       "      <td>-439.496700</td>\n",
       "      <td>-272.828151</td>\n",
       "      <td>-337.578325</td>\n",
       "      <td>-280.806674</td>\n",
       "      <td>-376.390918</td>\n",
       "      <td>-319.592834</td>\n",
       "      <td>-353.011912</td>\n",
       "      <td>-288.974698</td>\n",
       "      <td>-249.270393</td>\n",
       "      <td>-317.933958</td>\n",
       "      <td>56.548617</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>11.764419</td>\n",
       "      <td>0.870570</td>\n",
       "      <td>0.510842</td>\n",
       "      <td>0.053234</td>\n",
       "      <td>3642</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>67</td>\n",
       "      <td>True</td>\n",
       "      <td>{'regressor__regressor__n_estimators': 3642, '...</td>\n",
       "      <td>-275.675132</td>\n",
       "      <td>-459.393618</td>\n",
       "      <td>-287.871329</td>\n",
       "      <td>-299.898764</td>\n",
       "      <td>-302.369528</td>\n",
       "      <td>-389.045133</td>\n",
       "      <td>-305.570318</td>\n",
       "      <td>-353.990018</td>\n",
       "      <td>-303.489289</td>\n",
       "      <td>-227.347639</td>\n",
       "      <td>-320.465077</td>\n",
       "      <td>61.747945</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "93       7.756778      0.147370         0.381489        0.016976   \n",
       "47      16.502742      0.254536         0.845860        0.132707   \n",
       "29      12.073451      0.278458         0.399036        0.026090   \n",
       "13      15.297682      1.709751         0.465761        0.128846   \n",
       "38      11.764419      0.870570         0.510842        0.053234   \n",
       "\n",
       "   param_regressor__regressor__n_estimators  \\\n",
       "93                                     2857   \n",
       "47                                     5663   \n",
       "29                                     2969   \n",
       "13                                     3081   \n",
       "38                                     3642   \n",
       "\n",
       "   param_regressor__regressor__min_samples_split  \\\n",
       "93                                             5   \n",
       "47                                             5   \n",
       "29                                             2   \n",
       "13                                             2   \n",
       "38                                             5   \n",
       "\n",
       "   param_regressor__regressor__min_samples_leaf  \\\n",
       "93                                            1   \n",
       "47                                            1   \n",
       "29                                            1   \n",
       "13                                            1   \n",
       "38                                            1   \n",
       "\n",
       "   param_regressor__regressor__max_features  \\\n",
       "93                                     sqrt   \n",
       "47                                     sqrt   \n",
       "29                                     sqrt   \n",
       "13                                     sqrt   \n",
       "38                                     sqrt   \n",
       "\n",
       "   param_regressor__regressor__max_depth  \\\n",
       "93                                   181   \n",
       "47                                    20   \n",
       "29                                   192   \n",
       "13                                    34   \n",
       "38                                    67   \n",
       "\n",
       "   param_regressor__regressor__bootstrap  \\\n",
       "93                                 False   \n",
       "47                                 False   \n",
       "29                                 False   \n",
       "13                                 False   \n",
       "38                                  True   \n",
       "\n",
       "                                               params  split0_test_score  \\\n",
       "93  {'regressor__regressor__n_estimators': 2857, '...        -272.750763   \n",
       "47  {'regressor__regressor__n_estimators': 5663, '...        -272.978500   \n",
       "29  {'regressor__regressor__n_estimators': 2969, '...        -261.299327   \n",
       "13  {'regressor__regressor__n_estimators': 3081, '...        -261.388976   \n",
       "38  {'regressor__regressor__n_estimators': 3642, '...        -275.675132   \n",
       "\n",
       "    split1_test_score  split2_test_score  split3_test_score  \\\n",
       "93        -434.221200        -278.790106        -308.786471   \n",
       "47        -433.775705        -278.319974        -308.040762   \n",
       "29        -439.731069        -273.029449        -337.622471   \n",
       "13        -439.496700        -272.828151        -337.578325   \n",
       "38        -459.393618        -287.871329        -299.898764   \n",
       "\n",
       "    split4_test_score  split5_test_score  split6_test_score  \\\n",
       "93        -280.774726        -385.856308        -311.530852   \n",
       "47        -281.107885        -385.856782        -312.665655   \n",
       "29        -281.089347        -376.183880        -319.388937   \n",
       "13        -280.806674        -376.390918        -319.592834   \n",
       "38        -302.369528        -389.045133        -305.570318   \n",
       "\n",
       "    split7_test_score  split8_test_score  split9_test_score  mean_test_score  \\\n",
       "93        -349.243861        -293.190614        -240.030456      -315.517536   \n",
       "47        -347.969186        -293.861860        -240.764269      -315.534058   \n",
       "29        -352.818326        -289.229114        -248.908424      -317.930035   \n",
       "13        -353.011912        -288.974698        -249.270393      -317.933958   \n",
       "38        -353.990018        -303.489289        -227.347639      -320.465077   \n",
       "\n",
       "    std_test_score  rank_test_score  \n",
       "93       55.514433                1  \n",
       "47       55.212333                2  \n",
       "29       56.572235                3  \n",
       "13       56.548617                4  \n",
       "38       61.747945                5  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# have look at the best hyperparameters and their respective performance (maybe also look at the sd)\n",
    "pd.DataFrame(rf_random_search.cv_results_).sort_values(\n",
    "    by=['mean_test_score'],ascending=False).head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B) Performance on validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'minimum price': 118.96486417124208,\n",
       " 'maximum price': 118.1037856365381,\n",
       " 'total error': 237.0686498077802}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_perf(y_val, rf_random_search.predict(X_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C) Post processing\n",
    "\n",
    " - inspect predictions/residuals (make visualisations)\n",
    " - feature importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D) Predictions test data\n",
    "\n",
    "Refit on all training data (using the parameters found on the random search) and submit prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ...... (step 1 of 2) Processing preprocessor, total=   0.1s\n",
      "[Pipeline] ......... (step 2 of 2) Processing regressor, total=   3.8s\n"
     ]
    }
   ],
   "source": [
    "# train your final model on all data with best parameters \n",
    "model_rf_final = RandomForestRegressor(\n",
    "     n_estimators=2857, \n",
    "     max_depth=181,\n",
    "     max_features='sqrt',\n",
    "     min_samples_split=5,\n",
    "     min_samples_leaf=1,\n",
    "     bootstrap=False,\n",
    "     n_jobs=-1\n",
    ")\n",
    "# add to pipeline\n",
    "pipeline_rf_final = Pipeline(memory=None,\n",
    "              steps=[('preprocessor', preprocessor),\n",
    "                     ('regressor', model_rf_final)],\n",
    "              verbose=True)\n",
    "\n",
    "# again add transformer for target variable\n",
    "pipeline_rf_final = TransformedTargetRegressor(regressor=pipeline_rf_final, \n",
    "                                         transformer=transformer_target)\n",
    "\n",
    "# fit final model on all training data we have at hand\n",
    "pipeline_rf_final = pipeline_rf_final.fit(X_all_train, y_all_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions on test data\n",
    "rf_pred_test = pipeline_rf_final.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# submission format\n",
    "rf_submission_format = pd.DataFrame.from_dict(\n",
    " {'ID':df_test['id'].values,\n",
    " 'MIN':rf_pred_test[:,0],\n",
    " 'MAX':rf_pred_test[:,1]}).set_index('ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write to csv\n",
    "rf_submission_format.to_csv('../output/predictions_test/rf_python.csv' ,\n",
    "                            header=True, index=True, sep=',')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
